audio_path,duration,text
task2/super_cleaned_audios/lesson99.wav,338.0304,the next thing that we will see is how do you create images from embedding so let me  see what that means   so remember that each of these things can be thought of as an embedding of the image  right because you had this original image which was two hundred and twenty-seventwo hundred and twenty-seven dimensional and now you  have a four thousand and ninety-six representation for that or a two hundred and fifty-six  seven  seven representation for that so you could  just flatten it as an out as a vector and you could treat that as a embedding for the original  image right  now for any kind of embedding or hidden representation what do we always want from  that representation think auto encoders it should capture all the important  characteristics of the original image and in particular i should be able to dash the original  image from it  student refer time one hundred and three  we construct the original image from it right so thats what i would want from a good  embedding so let us see if we can do this right so find an image this is the  optimization problem that i am interested in find an image such that its embedding a  similar to a given embedding what do i mean by that is suppose i take a monkey  image and pass it through all these layers and compute all these embeddings right  now again i start with a blank image and my optimization problem is such that for this  blank image i want to modify it so again this blank image is my parameter matrix and i  want to modify it such that the embedding that it produces should be similar to the  embeddings that the monkey image produced so how can you set this as a optimization  problem what would your loss function be so lets call the original monkey images ione  and let us call this as embedding of ione   now can you tell me what the objective function would be for the new image that you  are trying to create this entry the first entry in its output that let me call that e i two ok so  e i two one and ei one one that means the first dimension of the embedding they should both be  student very  very close so in such cases what is the error function that we will choose  student refer time zerotwotwo9  refer time zerotwothreezero right so you have to get comfortable with designing these loss  functions right so you have seen you have seen this loss function before we just have to  be able to related to the problem that you are trying to work on     so let phi zero be the embedding of the image of interest let x be a random image and we  will report the repeat this forward pass using x and compute phi of x right that means  were computing the embedding of this random image that we have started with then we  compute this loss function and add appropriate regularization for that and that propagate  and update what what will you update  student refer time zerothreezerosix  image right you will update your x matrix right and you will keep doing this till  convergence    and let us see what happens so its suppose so now what i am trying to do is this is my  original image and i have the convolution one embedding of it so in this i am using  convolution one as the embedding and then i am trying to solve this optimization  problem to recreate x such that its very close to the original image  so let us see what are the different outputs that i get so this is the original image and  on the right hand side you have the reconstructed image such that the conv one embedding  of both the images is the same so you can see that when i am trying to do a  reconstruction from the conv one layer i get almost the same image back now if i keep  doing it from different layers what do you expect it to be if i do it from conv two conv three  conv four and so on  student refer time zerothree59  it wont be so accurate right so let us see what happens if i try to reconstruct it from  conv two    ah relu one    max pooling    norm one    conv two    relu two i am keep i am going deeper and deeper into the network so what i am trying to  do here is remember that i have different choices for these embeddings so the first  thing which i showed you was when i was trying to the first thing was when i was trying  to set my objective function such that i am trying to map this embedding the second  image that i showed you was when i was trying to map this embedding and the last  image that i will show is when i was trying to map these to embedding so my  objective function was to create an image such that this embedding of the created image  is the same as this embedding of the original monkey image right so thats what i am  progressively trying to do    as you can see as i keep going ahead i get more and more abstracter reconstructions and  i dont really get the monkey back    and once i go to the last fc six or f seven layers i get very weird looking reconstructions    and thats expected right because by that layer they have completely abstracted it out  right you have just probably captured there is something like a nose something like eyes  or some for here and there but you have loss the entire shape and other characteristics of  the original image right from the deeper layer the construction would not be that good  and thats kind of expected right    in spite of having the maximum no you could right a maximal operation is just another  embedding which is that the compression there is much more because you have ignored  the four entries and just taken the max value so it becomes harder and harder to do that  but mean you i wouldnt call this as a reconstruction right what you see here is not  except for the conv one and conv two layers the rest of the things were not really such  accurate reconstruction so just says that you are losing a lot of information in that  abstraction or maybe not i will do it next time   
task2/super_cleaned_audios/lesson66.wav,675.9441,i will do will do early stopping where again we will get into some of these eigenvector  analysis so let us see that     so the idea been early stopping is actually very simple in principle what needs to be  done so we know that this that this trend exists between the training error and the tester  right so in practice what you will do is you will continue to optimize the training error  the empirical training error which is the sum of the errors on the m training points   you will also continuously keep track of the validation error that means the same  quantity you will compute over the n validation or test points everyone get this you can  do this and you are actually doing this in your back propagation assignment keeping  track of the training error as well as the validation error and you keep plotting them ok i  will keep running for various epochs and keep something known as a patients parameter  p   so if you are at the twozeroth epoch and if your patients parameter p is equal to phi and just  do a check whether in the phi last phi epochs has my validation error ever gone down or  it has been staying the same or has it been increasing ok now i will give you a condition  that it was either staying the same or it was actually increasing is this good or bad   what does it tell you while your training error was of course decreasing may the more  you train your training error will keep going down so what does this tell it is just over  fitting you are fitting the training error you are just making it zero or as close to zero as  possible but that is not helping your validation error so the validation error is either  worst case increasing or remaining the same right  so this is a very commonly used trick which is known as early stopping you keep this  passions patients parameter and you make sure that if you have cross this patients right  and the patients here is that i was waiting for the validation error to go down but it is not  going down for some p epochs so no point in continuing training anymore i will just  stop it does not make sense    so and this can also be used in conjunction with other regularizers right so in the quiz  also we had this question sorry for bringing up the quiz but we also at this question  where you have the sparsity regularization and i was asking whether i can add the ltwo  regularization along with it so these regulations can be added or used in conjunction it  is not that you can only use one of them   so early stopping is a way of regularizing but you could also use it in conjunction with  ltwo regularization or any other regularization technique that you do not want right so but  how does this act as a regularizer from the picture it is probably clear and is the same as  the explanation i was trying to give to his question right that you are preventing  yourself from entering in these regions and trying to enter into more favorable stop at   more favorable regions  but can you think of slightly more in terms of what happens in gradient and what  would happen if you stopped it early and so on can you try it to connect it to the update  rule of gradient descent what happens as you keep doing it for more and more epoch  no gradient descent has nothing to do with validation error or backtracking error  gradient descent only works on the training data let us think in those terms  gradient star diminishing to zero so what happens how does gradient descent progress  where do you start i started a random point at every epoch which is a collection of  iterations right or you go or many training points what happens to this i start moving i  keep moving now if i fix the number of epochs or do not allow it to change any more  after a number of epochs what am i doing i am restricting the boundary around the  weight right i am not allowing it to grow beyond a certain boundary do you get that    let us see that so we will first see an intuitive explanation and then go to a more  mathematical analysis are update so the update rule for gradient descent is i always  make this mistake this has to be minus oh the t h have disappeared ok is there so sorry  other to have disappear   so now what would actually happen at the t h step is we have w naught three plus or minus  does not matter it just tells you that how much it is going to change this is what is  happening actually at the t h step right you have just subtracted all the previous  derivatives that you had so far right from where you started off now you are looking at  t steps so at every point you are computing a certain gradient but had a certain  magnitude  now let me say that across all these steps the maximum gradient that you had i will  just call it by tau right so that means in this summation there are t terms i am saying  the maximum of those was tau that was the maximum rate gradient that i got at any one  point  now what i am going to do after this i am going to replace this by something this  summation is always going to be less than or equal to this right because i am assuming  that each of my steps is less than tau there are t such steps so i could have at matched  moved t into tau right but i would have moved less than that because tau was the  maximum gradient that i had  so this is going to be less than equal to is that do you get the change from the equality to  less than equal to ok so now what am i restricting actually in early stopping what is  being restricted there are only so many symbols there i just speak one t tau is of course  not in your hands w naught is not in your hands w so t is the one right so i am only  allowing that many updates so that means from w naught you can only moves that  much this looks you see that analogy that this is something similar to you not allowing  the weights to really grow a lot    so now but will not end here you will of course do some more stuff on this right ok  so we now see a mathematical analysis of this so recall that a taylor series  approximation for l w is the following the same thing which i wrote a few slides back  or many slides back everyone remembers this right and now again i am going to do the  same thing that if i know the optimal w star then the gradient at that point is going to be  zero   so this term disappears and now if i take the derivative this is what will remain this is  exactly what we did earlier also right so we will have derivative of this and derivative  of this so the derivative of this quantity is just this and the derivative of this is zero  because that is exactly what we started off with right that w star is the optimal solution   now sgd update rule is the following ok which i can write as this i just replaced this  by this ok i am just rearranging some terms is that ok how many if you are fine with  this how many feels to tired to even care about this     so this is what w t would be this is again some simple steps leading to some conclusion  the conclusion is what matters the steps are very easy you can go back and look at them  right so again i will use the evd the same trick that i did earlier and it will give me  this instead of h ok again i will just do some rearrangements and actually i can show  that if i start with w naught equal to zero then w two is actually given by this quantity ok and  there is a proof of this in the appendix you can go and look at it   now what does this look similar to rotation diagonal rotation exactly similar to the  analysis that we did for ltwo regularization right and in fact if you can you can show  that if we compare this expression with the while we had for ltwo regularization and this  is the expression that we had for ltwo regularization right rotation some scaling and then  again rotation right then we can show that early stopping is actually equivalent to ltwo  regularization if the following condition is satisfied   this does not mean much because god knows how you will satisfy this condition  right but all it is saying is that there is some equivalence at under certain conditions  and that is what is the intuition was also telling us that it is somehow preventing the  weights from going large and it is doing this in this very convoluted way where this  condition holds for it to be equivalent to ltwo regularization   as i said for you and me is going to be very hard to create this condition right how do i  make sure that something like this is true right but that does not matter what matters is  that there is some equivalence between them    so when you are doing early stopping it is not just a heuristic or a blind thing that you  are doing you know that it is somehow related to ltwo regularization hence that you are  doing it and hence it also works in practice is it fine we will that work for all of you ok  right so the things to remember is that early stopping only allows t updates to the  parameters this is the important thing rights so now if a parameter w corresponds to  a dimension which is important for the loss then what would this quantity be the partial  derivative of the loss with respect to that parameter it is going to be if there is a  parameter  for example let us take the amir khan an example right that whatever weight you  gives to whether the actor was amir khan or not if that is very important because if  that feature is on you are lost completely changes and so on right if you do not learn the  weight correctly that feature is very sensitive  so for important features the loss would be very sensitive to the changes in the weights  of these features is that intuition correct right that means this gradient would be large  ok and if a parameter corresponds to a feature which is not important what would this  derivative be small now what is the net effect of this you have some parameter which  are important so the derivatives are large some parameters which are not important  so the derivatives are going to be small and you are going to only allow t updates so  what is going to happen the parameters which are important we will end up getting  effectively more updates right because each of these magnitudes was higher and you  did t of those the parameters which are not important we will end up getting effectively  lesser movement  because each of these gradients were small and you did only t of those right so you  again see this that it is a weird way of ensuring that your important parameters get more  updates than your non important parameters right so it is very important to see these  connections between these different regularization methods all of you are fine with this  fine   
task2/super_cleaned_audios/lesson72.wav,1548.9848,so in this module we will talk about better initialization strategies  so this is where we are in the story right we saw that deep learning has evolved and at  least these are the four things which have happened so we have by the way this slide is  incomplete what are the other things which have happened actually which you already  said in the beginning two more things which are not technical but which happened more  data right  and more compute but these are not really technical in the sense that i mean this just  happened we have large amounts of data that means more data means what if you are  more data for training you would have complex networks but not over fit right because  you have so many so much of data right and more compute of course it speeds up  some of these matrix computations which happen  so remember in a deep neural network most of the things which are doing are matrix  matrix operations right you are taking are that is what exactly you did in your back  propagation assignment you did a lot of matrix vector computations and so on and the  advent of gpu’s this became very very fast rate orders of magnitude fast  so this two which are here as nothing much to talk about that is just something we all  understand what has happened they and so now we will talk about better weight  initialization strategies    so let us start with this question right we will take this network and we will ask this  question what happens if we initialize all the weights to zero i like it when you all try to  visualize it and ok so you have to see what happens right so let us start with a one one  which is w one one x one plus w one two x two  so i always start small l i do not try to see what will happen everywhere just start with  one neuron and see what happens take a one two what would a one month’s value b if all  the weights are initialized to zero zero and a one two again zero right and same for a one three it is all the  way all the neurons in this layer are going to be zero till is it  so that means they will all get the same activation so if the as are same the h s are  also going to be same and that is obvious irrespective of what nonlinearity you use now  what will happen during back propagation what will delta w one one b this again i do not  know why you do this ok anyway that will be erase it into x one  so remember that the gradient is always proportional to the input and you have  somewhere along the lines along the chain rule you have this h one one and a one one just  remember that and what would delta at gradient of w two one b is that fine  now can you see some things on the left hand side and make some comments on the  gradients we have seen that a one one is same as a one two and h one one is same as h one two that means  these gradients are going to be equal right that means the weight started off at the same  value they are going to get the same updates and again remain at the same or different  value but this same right then of course move from where you started will not be zero  anymore but they will all be at the same value    both the weights will get updated with the same value and they will remain equal so but  fine as i keep training they will move away from each other right this is what i told  you when you feed in the first example take a both the weights remain the same but  now if you feed another example and you keep feeding batches there there is no dearth  of data that you have and eventually these weights will move away from each other  the update is the same again the weights are the same again the same situation will  hold right again your w one one x one plus w one two x two is going to be the same as w two and x one  plus w two two x two and the same argument repeats  how many if you get this ok so once you initialize the weights to zero in all subsequent  iterations the weights are going to remain the same i mean they will move away from  zero but they will all be equal ok and this symmetry will never break during training so  what actually is happening in terms of the capacity of the network this is same as  student single line tying the weights  the same as tying the weights so this symmetry will never break during training so  asking what is the net effect which is happening  so you have so many weights in your layer but all of them are moving together will so  in essence you do not have the same freedom as you have with n different weights right  here in some sense unintentionally tied them because it started off with the same value  now you are all moving at the same values rate you are all going to the same value so  you do not really have the amount of freedom that you would actually expect with n  different parameters all of you get this  and the same is true for w one two and w two two also which are the weights connected to the  second neuron and this is in fact true for all the weights in layer two you can actually  mathematically verify it that means whatever this small analysis that i did here just go  back and do it for all the weights in the network and you will see that all of them if you  are going to initialize them to zero all of them are going to remain equal  this is known as this symmetry breaking problem this is are known problem this is  existed much before twozerozero6 and so on if we initialize all the weights to zero you will have the  symmetry breaking problem is there anything sacrosanct about zero or would this happen  even if you initializer to same but non zero values and that should have been cleared from  the iteration right because after the first iteration we were at non zero weights and after  that the story repeated right  so even if you initialize it to non zero weights the same story is going to report repeat  so that means as long as you initialize all the weights to the same value you are going  to end up with this symmetry breaking problem ok which is not good so what is it that  we have learnt about initializing weights  student refer time 6threetwo  definitely do not initialize all weights to zero definitely do not realize them to the same  value ok this is the first thing that you have learned so we are seeing different ways of  not making the light bulb and then we will come to a way of making it so zero and  equalist no bad yes some weights will not get updates in that case right  so then that that should be fine so that is the other thing i wanted to make at some point  right these four things right initialization optimization regularization and activation  function these are not independent things they are all tied to each other   so as you said now if you use regularization then probably you could be a bit careless  with the initialization even if you had initialize the weights together drop out would  have ensured that some of these weights are not active at a particular training instance  that means they will not get weight updates that means they will move away from the  other weights  so that is this is not that only one of these things can be done right you are going to  use a combination of these things but while analyzing them we will just look at one of  these things assuming that the others are not being right so will assume that we are  not using drop order anything is that fine    so this at least this in practice you are not supposed to initialize the weights to zeros and  equal values that is what we have learned so far now for the rest of the to convince you  about some other weight initialization methods what i am going to do is i am going to  take a feed forward network where you have as input some thousand points each of  this point is fivezerozero dimensional and the input data is drawn from a unit gaussian what i  mean by that is you have this x one two x one fivezerozero rate for the data instance one  so all of these fivezerozero dimensions come from a unit gaussian is that fine so this comes  from a unit gaussian this comes from a unit gaussian and so on ok that is what i am  going to assume    and the network has five layers each layer has fivezerozero neurons the input is fivezerozero neurons each  of the five layers is also fivezerozero neurons and now we will run forward propagation no  backward propagation no loss nothing and i am not even giving you an objective this is  just some input and i just want to see what happens up to the last layer i am not even  bothered about the actual last layer that means i am not trying to minimize any cost  entropy squared error loss anything    so let us try a few initialization strategies so we realize zero is not good realize equal is  not good so let us try some random initializations but small weights ok and this is my  way of randomly initializing with small weights  so my w is a matrix of size fan in into fan out rate which is n cross n ok the number of  weights coming in and out rate so n cross n and i am drawing from a uniform  distribution and then multiplying it by point zero one which ensures that all the weights are  very small you get the setup now with this i am going to start with the input and then  keep doing these transformations  so i will do w transpose x plus b pass it through a sigmoid and do this five times  because i have five deep layers now this is what happens to the activations across the five  layers so the first layer remember that we had drawn from a unit gaussian right so  that is what the data input data looks like so this is the first layer which is the input data  basically and then this is what happens across the different layers  so what is actually happening and this is for the tanh activation function there is no  variance in the output of so this tells me so this basically tells me that for all the  neurons what is the average value that i am getting right and i should ideally get some  histogram that for some neurons i am getting the value minus one for some neurons minus  zero9 zero8 and so on but what this is telling me is as i keep progressing across the layers  all the neurons have very similar values and they are all close to zero  this is what actually happens in practice i have just actually run it and computed the  histogram    and if i use sigmoid activation functions again something similar all the values tend to  be close to the center which is zerofive so this is zerofive and although i had started with a nice  gaussian distribution    now what will happen during back propagation so do not try to think for now that  why this happens i am just telling you have actually run the code and this is what  happens now given that this has happened what will happen during back  propagation so all the activations in a layer are very close to zero all the gradients are  going to be close to zero that means no gradients are going to flow back that means  which problem are we dealing with vanishing gradient problem  so if you initialize your weights to very small values and this is easy to see in the case  of tan h so for tan h this is my function right and this is zero now remember that this is  wi summation wi xi if all my weights are close to zero or very small values what is  summation w ix i going to be it is going to lie somewhere here right  so all these inputs are actually going to be very close to zero now if my inputs are going  to be close to zero i know that during back propagation at some point my gradient is  proportional to the input that i have given and when i say input here i mean layer one  layer two layer three and so on  so that means all my inputs are very close to zero now my gradients are actually  proportional to the input so all my gradients are also going to be close to zero that means  my gradients are vanishing right because remember that across five layers you will have  these products of gradients right all of them are very close to zero so you will end up  with something very close to zero raise to five how many if you get this right so our  gradients are going to vanish    if you do this very small initialization of the weights and that is exactly what is  happening so this is the histogram for the gradients and i see that all my gradients are  actually very close to zero that means no effective training is happening my weights are  not receiving any updates this is what happens in practice if you initialize your weights  to very small values    now let us try to do the opposite of this very small values did not work so let me try  large values and for large values i just sample from the uniform distribution i will get  some numbers between zero to one now can you guess what will happen remember  summation wi xi all your weights are large so why am i saying that number between zero  to one is actually large it is not by all practical because this is going to give me this  function is actually going to give me numbers between zero to one why am i calling them  large weights  student refer time onetwofivefour  no i will i just talked about the weights assume there is no biases how many of you get  that answer remember there are fivezerozero neurons so if you have fivezerozero small values that  summation is going to be still large right if you all of these are zerofour or zerofive which still  looks small but if you have twofivezero of these or if you have fivezerozero of these the resultant sum  could be somewhere of the order of twofivezero right and that is very large because if you pass  that to a sigma and neuron what will happen saturation right so you get this why i am  calling these weights as large    so and this is actually what happens so when i have these tan h activations across all  the five layers i observe that my neurons saturate i either get minus one as the output or plus  one as the output and same thing happens if i use sigmoid activations i either get zero as the  output or i get one as the output right neurons saturated means what will happen gradients  will vanish right  so even if you initialize the weights to very large values all your gradients are going to  be close to zero because they are going to vanish and again you have a problem so what  have we seen so far zero is not good equal is not good small weights is not good large  weight is not good then what do we do     so let us see what to do so let us try to arrive at a more principled way of initializing  weights and this again do not should the messenger i am going to give you a proof under  certain assumptions ok so just bear with me i just tell you what those assumptions are  going to be as we go along so as i said right  so i mean you would argue that in practice these assumptions do not hold true but at  least they give us some insights into what is happening right what is the overall idea  behind what is being proposed so let us start with that so now consider this deep  neural network and i am just considering the first layer of it where i have this neuron s  one one and i am talking about things before the activation  so i know that  soneone is equal to this quantity right so all the incoming weights to the  first neuron which is w one i into x i now for some reason i am not telling you why i am  interested in the variance of this can you tell me why i am interested in the variance  what did you see in the previous examples there was no variance right there was  hardly any variance so let us see what happens if you compute the variance of this  so i am just taking the variance formula a variance of a sum is equal to the sum of the  variances right this is of the form variance of a into b where a is w one i and b is x i  what is the formula for this or if you know it or do not know it do not care so this is  the formula  so this is the generic formula for variance of a into b where you have to assume that a  is wonei and b is xi so this is just a formula there is no trick here no math i mean no  nothing fancy here just apply the formula for variance of a b and substitute a is equal to  w one i and b is equal to x i    now i will assume that all my inputs are zero mean fine we have been assuming that  forever and all my weights are also from zero mean ok what is the effect of that which  quantities will disappear this will disappear because mean as zero means the expected  value of the weight is zero so the square of that is zero an expected value of the input is zero the  square of that is zero  so what am i left with summation where i variance of xi into variance of wonei ok now  i am going to assume that the variance of xi is equal to the variance of x that means it is  the same for all the i’s so i had this remember i had these fivezerozero inputs  so i am assuming that for all the inputs the variance is the same they all come from a  similar variance distribution and i am also going to make the same assumption for the  weights fine and then i end up with this neat formula that the variance of soneone is equal to  n times the variance of w into variance of x right because i assumed that all these  terms are equal and there are n such terms everyone is fine with the maths so far with  the assumptions that we have    so in general for any of these neurons right instead of just soneone i could take any sonei and  this is what the variance is going to be variance would turn out to be because i have  assumed that all the weights and all the inputs come out from the same variance  distribution ok from a distribution having the same variance  now let us what would happen if this quantity is very greater than one the variance of  sonei would be very large right and what would happen if this variance tends to zero  variance would be very low so i am just giving you two extremes to build the intuition  and let us see what we are going to do with that intuition fine    now let me add one more layer and see so i have added one more layer and using the  same procedure as above he will arrive at variance of stwoone is actually given by this  formula and actually what has happened here is that this is si had xi earlier but now  instead of xi i have sonei because those are the inputs to this layer right  so this is exactly the formula that we had arrived at earlier assuming zero mean and the  same variance for all the weights and the inputs and i am arriving in the same formula  for the next layer where instead of x i have sonei  so this will result in this quantity ok but i already had a formula for a variance of sonei  what was that n into this quantity so i will just substituted it there so i can say that  variance of stwoi is actually equal to this i just substituted this value  so that turns out to be i have a square here when i have two here so you see where i am  headed with this what will be the variance of s k i this raised to k and is on everyone  gets this ok i can just continue the same analysis and i have assumed that these weights  and always are the same variance right    so in general i can say this ok now can you tell me something about when would this  variance vanish when n variance of w is  student less than one  less than one ok and which is the thing that we should aim for you would want this  quantity to be equal to one in which case it will neither blow up nor shrink fine so so it  to ensure that the variance is the output of any layer does not blow up or shrink we  should ensure that n into variance of w is equal to one right so what is this this just take  a minute to understand this i am saying that i am going to initialize my weights  so i should initialize them in such a way that the weights are coming from some  distribution like we saw that the distribution was a uniform distribution from where i  was drawing the weights  so they are coming from some distribution i should try to draw them from a distribution  such that this condition holds if this condition holds then across layers my activations  will not blow up or shrink though that is exactly what was happening in the earlier case  when i was doing those bad initializations with small values and large values  so let us see how to do that so what i am going to do is i am going to consider a  random variable z ok where is z comes from a normal distribution ok and i am going to  scale it is value i will draw from there and then i am going to scale it by one by square root  of n what is n number neurons in each layer right here it is the same across all layers  but it could also be different so i am considering a particular layer and n is the number  of neurons in that layer  and now if w is actually equal to z by square root of n then i can write that n into  variance of w is actually equal to this quantity everyone is fine with this there is no  trickery here i am just saying that why i am doing this is not clear that will become  clear but at least what i am doing is clear i am drawing the weights i am taking a  random variable z which comes from a normal distribution and then i am setting my  weights to whatever values i have drawn  i just divide them by the square root of n    now let us see what is variance of a z a square into variance of z hey that is a basic  formula all of us know this so now what is variance of z by one in z into one by square  root of n one by n into variance of z right so the n and n cancel and what is variance of z  what did i assume about z it came from a normal distribution zero mean and unit variance  so variance of z is one that means this quantity n variance of w is going to be one if i have  initialized my weights such that they are equal to this right and now do you see  whether the weights are very small very large or what are they some now they made the  weights dependent on the number of neurons  so if you have very large number of neurons you are drawing drawing weights such  that or you are initializing weights such that it is some normal variable divided by the  square root of n right so now when you do this summation w i x i your summation  cannot blow up because you have already divided it by n  how many if you get this so this is a standard way used for initializing weights how  many if you tried this for your back propagation assignment why did you try this ah  student refer time twothreeonetwo  because you are having some problems with saturation i guess right so this is how  you should initialize your weights this is more or less the standard technique and some  variant of this right because instead of n you would have this fan in and fan in out it  how many weights are coming in and how many weights are going out  so you make it proportional to the square root of n into k or something like that right  so but in general this idea right of course this proof we arrived at it with lot of  assumptions but we at least got to some principle way of initializing weights and this is  a largely used standard this and some variants of it    so now let us see if i actually take the same network that means five layers fivezerozero neurons at  every layer and then initialize it using this so this exactly what i had told you right that  take it from a unit distribution sorry a normal distribution and then divided by the  square root of the number of neurons in that layer and now let us see what happens  across the five layers  you see what happens we get this good variance in the activation functions they are not  all going to zero or one or point five right so this solves the purpose for tan h activation and  also for the sigmoid activations    you see a good spread in the weights and remember actually for sigmoid although  these values look close to each other but this is the zero to one range this is actually minus one  to zero which will not happen for sigmoid so within the zero to one range you get a good spread  if you initialize the weights this way    but it turns out that this initialization does not work for the relu function in the relu  function you still see this effect that you started off with a good spread but as you keep  going across depths this spread disappears why would that happen to someone gave an  intuition for this and is again one of those heuristic things that in the case of relu you  need to account for this divided by half because half of the relu is not active right  half of the relu is zero    so you need to account for that fact and do this simple trick that instead of taking the  square root of the fan in you take the square root of fan in by two because you know that  half the times it is not going to produce any output right so that is a very simple  heuristic that someone tried and that leads to better activation functions better  activations across all these layers right so as you see across all the layers the spread is  good now so the same idea ok so now you have a good way of initializing neurons  so this should help you in your future assignments fine so this is how what you have  learned about how to initialize your weights and it makes a lot of difference to how  your network will behave right and that is what the i was trying to show that by  computing these activations across different layers and i showed that as you change  these initializations strategies you get better activations  
task2/super_cleaned_audios/lesson73.wav,902.1759,now we will end with something known as batch normalization which is again almost  a defacto standard at least in convolutional neural networks so if you are dealing with  convolutional neural networks you will use something known as batch normalization  so let us see what it is so this is again something which is some method which allows  us to be less careful about initialization so let us see why that happens    so to understand the intuition behind this let us consider a deep neural network ok and  let us focus on the last two layers h four and h three now typically will use some minibatch  algorithm for training right so we will use minibatch version of gradient descent or  minibatch version of adam or any of these algorithms right  now what would happen if there is a constant change in the distribution of h three no just  think about the question that i am trying to ask you so as far as these two layers are  concerned h three is the input and h four is the output it does not matter what has happened so  far or in particular does not matter what x was whether it came from a normal  distribution or whatever distribution right  at this point my input is h three and my output is h four now i am training it in mini batches  what if across batches my distribution of h three looks very different what would happen is  it a good thing or a bad thing it is a bad thing right so if you have training data right  just think of this as i said just focus on this layer if you have an input which is not  following a fixed distribution is constantly changing during your training then that is  always a bad thing right because you try to adjust to one distribution and now again the  distribution is completely changing so that always makes our training very very  difficult right so if you have a very fluctuant distribution then a training is going to be  hard ok so that is the intuition that i want to build    so now this could actually happen so it would help if the pre activations at every layer  are you need gaussians because for the input we made a case that will make the input as  unit gaussian right  so that things are very nice they are all coming all the inputs are coming from the same  distribution but we now realize that at every layer we have an input right it is not that the  original input the only input even h three is an input even h four is an input and so on so why  not ensure that at every layer your inputs or your h one h two h three also is something which  looks like a gaussian distribution which comes from a gaussian distribution why not  ensure that that is the basic idea behind batch normalization and how do you do that is  the following that you had computed this s i k just as we had done in the derivation  earlier right so s i k is one of these guys   now if you do this what are you actually doing you just normalizing it right you are  subtracting the mean and dividing by the variance so that means you are making it zero  mean unit variance and that is the intuition which i was trying to build that why not at  every layer have this good distribution which is zero mean unit variance by even if you  are feeding it multiple batches for that batch you will ensure that by this subtraction and  division or the normalization process the data will become unit variance and zero mean  ok so now at every batch the data is coming from the same distribution even if it was  originally from a distant different distribution but how do we compute this mean and  variance  so did you understand the question that i am asking i am focusing on this s i k i want to  subtract the mean of that s i k how do i do that so the name gives it away batch  normalization it cannot be more explicit than that so compute the mean for the current  batch and the variance for the current batch and normalize your inputs or normalize the s  i k according to that you get this so now end up with a situation where all your inputs at  every layer across different minibatches seem to come from the same distribution is it  fine the current batch so you take the average value from the current batch  so then it will become zero mean for that batch and unit variance for that batch and this  you are ensuring for every batch so every independently every batch you are ensuring  that it comes from a zero mean unit variance distribution right so overall the effect is  that all the batches are coming from the same distribution no so at validation time you  will compute the mean and variance from your entire data entire training data once after  the training is done right  so now we will computed from a minibatch and this is ensure that across minibatches  now your input always comes from a zero mean unit variance distribution across all the  layers    this is what a deep network will look like with batch normalization right so what will  happen is you passed an input you computed this tan h then you will have this batch  normalization layer watch is what is the operation that the batch normalization is going  to do this is the operation that it is going to talk ok everyone gets that and now it gives  me a unit normalized distribution sorry it gives me a input coming from a zero mean unit  variance distribution and then i pass it to the next layer again at a batch normalization  layer  so after every layer you will actually add a batch normalization here now my question  is is this legal what is legal in this course anything that is differentiable right so you  have to make sure that if we have added this operation it should be a differentiable  operation so that you can come so now the gradients have to flow all the way here  right so that means i should be able to compute the gradients with respect to this so  now this is one of my a i and i should be able to compute dou a i with respect to  something or rather the loss dou of the loss function with respect to a i by turns out that  the operation that you have done is actually differentiable    you can actually work that out and it is not important i am not going to derive it because  it is just yet another derivative that you will take but it is a you should get the intuition  from here right what you are doing is this simple operation and this just looks  differentiable right  so the operation that you are doing is differentiable so that is why you can add these  batch normalization layers and you can back propagate through this layer but now what  is the catch here it somehow ties to the question that he was trying to ask you are  actually enforcing that all your are zero mean and unit variance right so this is again  some sort of a constraint that you are enforcing right what if that is not the best situation  in which the network can learn what if to distinguish between some classes it was ok if  the distribution was not same across all the batches they get this they are enforcing a  certain consider they are enforcing a certain condition on all the layers and all of them  have to be zero mean and unit variance but that may not always be good    so they do something which is counterproductive let us see what that is why not let the  network decide what is best for it so after the batch normalization layer so this is what a  normalized s i k was after you have done that you compute a y k and this is not the final  output this is the output at the k’th layer this is equal to this why do they do this and  remember that gamma and beta are going to be learnable parameters what are you doing  actually you are again scaling it and shifting it this is the same as adjusting the variance  and the mean right  so now what happens if the network learns the following you get back the s i k so you  had taken s i k and you had normalized it but now if you allow these gammas and betas  to be there in the network then the network can decide that maybe at this layer i do not  want this normalization i just want to stick to whatever output i was getting  so it could learn the gammas and betas in this way and ensure that you get back the  unnormalized s how many of you get this fine lot of you do not seem to get this but i  am pretty sure if you go back and look at it you will get it right so what is happening  here is that is why i said it is counterproductive that you first forced it to make at unit  mean and zero variance and now you added no zero mean and unit variance and now you  added this operation which is again a scaling and shifting operation so remember that  when you make the data zero mean and unit variance that is exactly what you do you  shift it so that it become zero mean and you scale it so that it becomes unit variance  so you are again introducing parameters which again introduce the same flexibility that  you could learn gamma and beta in such a way that you could get back the original data  which was not normalized ok so if the network wants to learn that and if the network  fees that is the right thing to do then it has the flexibility to learn those parameters and  you can recover si  i think the rationale is that your first making is something which is more standard right  and then from there trying to learn it instead of just trying to let it learn in the way do  you get the difference between the two the first bringing it to all of these things to some  standard value which is between i mean which is the normal distribution and then from  there allowing it to learn wherever it has to learn right that is the idea but it could be the  case that the other thing also works here    so now what we will do is we will compare the performance with and without batch  normalization on mnist data using two layers    so here in this figure what i am going to draw is the validation loss am i no the training  loss as i keep increasing the number of epochs and here i am showing you the histogram  of the activation functions at layer one so i have trained a deep feed forward neural  network and i am showing you what do the activations look like at layer one with and  without batch normalization so remember that we started with this intuition that  without batch normalization there would be this constant fluctuation and the data would  seem to come from different distribution at every training instance  whereas with batch normalization you are ensuring at your data comes from zero mean  unit variance distribution right and so that is one thing which i want to see another thing  i want to see is that how does it affect training right so that is the animation that i am  going to show you so focus on all these three things i do not know how you will do it but  focus on this focus on this and focus on this with two eyes    so let us see to see what happened right so this so now look at the focus on the leftmost  figure so that does not seem to change much with respect to it is mean and variance  right but if you look at the middle figure that is constantly changing it is mean and  variance right and you see the effect on the training loss that the first one which was  with batch normalization that converges faster as compared to the second one right  again an empirical result i am not really proving that this will always happen this is what  empirically we observed  so this was the story that we covered from one986 to twozerozero6 where back propagation was  already it was already discovered but was not working well and there was this spark in  twozerozero6 that showed that we could do some things to make training really work for deep  neural networks but maybe that something is not sacrosanct we could try different  things what we tried at that time was unsupervised free training which is almost  nonexistent now  but that lead that led to these thoughts that maybe this is because of optimization  generalization regularization activation functions and so on right so there was a lot of  research in these different areas and that led to a lot of developments which was better  optimization algorithms better regularization better activation functions better  initializations and batch normalization right  so these a few concepts that you have seen in the past few lectures one being dropout  and the other being weight initialization using this xavier initialization or he  initialization and this batch normalization right this is something which is all prevalent  right so this is something that you will see in all deep neural networks that get trained  definitely in convolutional neural networks and more often than not even in recurrent  neural networks so these are the two most popular types of neural networks so in both  of these you will see that these ideas are regularly applied and they always lead to more  stable training or better generalization    so now this was all which happened till twozeroone6 or one7 what has happened still since then  so there is still continuous research in designing better optimization methods so as i  said after adam there was this eve which did not become very popular but there is still  people looking at better optimization methods and there is something which has been  developed on adam and came out in december last year  now people have also started looking at data driven initialization methods right so  instead of having this fixed initialization which is drawn from a unit or just which is  drawn from a normal distribution and then just divided by the square root of n why not  think of data driven initialization methods that so there are some works on that again not  very popular because most of the shelf things that you will try will not really do any data  driven initialization  but if you really think that you are stuck at some point then you could look at some of  these works and see how they try to come up with initializations based on the data that  you are dealing with and now after batch normalization there have been some other types  of normalizations which have been proposed which seem to work better than batch  normalization but largely the stable configuration which has kind of prevalent is adam  in terms of optimization xavier or he initialization in terms of initialization relu in  terms of activation functions what else is there batch normalization in terms of again  regularization plus initialization and dropouts in terms of regularization right  so these are roughly the key terms that you will almost see in all the deeply living deep  neural network people that you see right you will always see when they describe the  hyper parameters they will say that this is how we initialized is this is the drop out that  we use this is the batch normalization and the training algorithm more often than not is  going to be adam  so they have seen some very crucial elements of training deep neural networks over the  past two to three lectures right and now we will build on these and we will assume that this is  what you are going to do so now when i talk about neural networks like convolutional  neural networks and so on i not go back and tell you use adam or use batch  normalization or assume that you already know these things and you will try to train your  networks using these tricks that we have your learned  the last couple of lectures have been about tips and tricks for deep neural networks and  from here on in the next lecture will move on to what wordtwovec because that is what  you need for your assignment so in the next lecture we will do a word representations  so that is essentially seeing an application of feed forward neural networks and from  there on we will move on to convolutional neural network  
task2/super_cleaned_audios/lesson67.wav,431.6677,so next we look at on ensemble methods and this is just to build the intuitions for  something known as dropout which is very popular technique in deep neural networks  and convolution neural networks and even recurrent neural networks    so how many you have seen ensembles before seen it in machine learning ensemble  was not done in machinery done with ok ravi did it so as a combine so the ensemble is  essentially just the combining the output of different models to reduce the generalization  error right why does that make sense have these different models all of these would  have different biases and variances right  so now you are combining them so i will end up with a better thing on the test error  right so that is the idea behind ensemble now the models could correspond to different  classifiers right for example here i have a logistic regression and svm and a naive  bayes i have trained them independently using the same data or different subsets of the  data and a test time i am taking a prediction from all of them and then taking an  ensemble of those predictions that is the basic idea  now it could be different instances of the same classifier trained with different hyper  parameters i could have the same neural network a threelayer neural network but trained  with different hyper parameters so the hyper parameters could be learning rate it could  be batch size it could be the number of neurons in each layer and so on right so it  could be same classifier but different hyper parameters different features right so  instead of looking at all the one hundred features that i have given i could train these classifiers  with different subsets of the features ok or different samples of the training data    so bagging is one such ensemble method where you have different instances of the same  classifier which are trained on different samples of the training data ok so i have one  classifiers trained on a subset t one of the training data another classifier trained on a  subset t two of the training data and so on right  and so each of these model is trained with  a different sample of the data    now when would bagging actually work what would you want these classifiers to be  so each classifier is going to make certain errors  what do you want these errors across classifiers to be dependent independent   student independent  independent right so if one classifier makes the errors on certain test instances other  classifier makes errors on a different set of test instances and the third classifier makes  errors on a very different set of instances that is the condition that you are looking for  right there is errors if all of them make error on the same instance then all of them are  collectively going to make an error on the final prediction also right  because it is like i asked three guys all of them gave me the wrong answer so my final  answer is going to be wrong but at least two of these three guys gave me the correct answer  then my final answer is going to be correct right so that means the errors that these  models make i want these errors to be independent if i treat error as a random variable i  want these errors to be independent  so so consider a set of k such logistic regression models suppose that each model  makes an error epsilon i on the test example now let epsilon i be drawn from a zero mean  multivariate normal distribution so the variance is equal to v and how many such  epsilons do i have how many such distributions i am considering  student k  k right because for each classifier there is a distribution so then i can compute the  covariance between these random variables ok i will add that let that covariance be c is  that fineok now the true the error made by the average prediction of all the models is  going to be given by this model one made an error of epsilon one model two made an error of  epsilon two  so the average error is going to be given by this now what is this expected squared  error this is the error this is the expectation this is the square that is the expected  squared error is that fine again this is a square of a sum so it will lead to a lot of terms  of the form epsilon i squares and what will happen now which terms will go to zero    the terms having epsilon i epsilon j again the same thing they are independent so i can  write the expectation of a product as the product of expectations and those expectations  are zero so this is what it is going to look like what is this oh sorry actually we had not  assumed that the covalence  what is this right and what is this covariance i am sorry i have not we had assumed  that there is some covariance said wed not assume they are independent right we would  want it to be independent but in the general case we will assume some covariance and  then i will show you the special case where they are independent  so then how many vs do i have here k right and how many cs do i have here    this summation is k into k minus one right or i equal to one to k and j equal to i plus one to k  fine and so this is what it looks like now can you make some inferences from this  equation this is what the expected mean square error is going to be now think in terms  of variance covariance and tell me when would this be beneficial i have already told  you the answer if the errors are independent what would covariance be zero right  so then what is the mean square error one by k one by k into v right so that means bagging  would work when your classifiers the k classifiers that you are combining    if the errors are independent then the mean square error should actually have been v  right for a single classifier it was v right because mean square error is nothing but the  expectation of the error expectation of epsilon i square which is nothing but v  but if you are if you are combining k classifiers and if these classifiers are independent  in terms of their errors then your mean square error is going to be one by k into v because  this term is going to disappear ok now if your classifiers are perfectly correlated then  what would happen and basically c is equal to v right is that fine so now what would  happen what is the net result if i substitute this as v going to be v right  so if you are all your classifiers are perfectly correlated that is the other case we had  tried taken and all of them are making errors on the same test instances and the same  errors right then you will not get any benefit of doing bagging but if you look at the  other extreme where all your errors are independent or all your classifiers are making  independent errors then you will get a benefit your expected mean square error would go  down from v to one by k into v everyone gets that    so this was just to develop an intuition that taking an ensemble helps right and using  this intuition now we are going to see at how to do this ensemble in the case of deep  neural networks  
task2/super_cleaned_audios/lesson98.wav,585.9378,ok the next thing that we are going to do is optimization over images so this is again  interesting and it eventually led to this whole field of adversarial deep learning or  adversarial machine learning in general right so we will see what this is   suppose i have a trained convolutional neural network ok and now i want to figure out  what kind of image should i pass through this so that it gets recognized as a dumbbell  why we want to do i would not want to have such a weird objective can you think of a  reason why would want such a weird objective i know there is a convolutional neural  network which can distinguish k classes these classes could be anything   now i want to deliberately create images which get passed as the dumbbell class why  would i want to do this ok you are going into your details so i will give you a  application right suppose this network is supposed to do face detection and the k classes  which are there are k people right now you want to see what kind of image should i  feed to this so that i get recognized as amitabh bacchan right so now that could have  certain benefits and various high places and so on its i would want to do that right   so thats the whole idea behind adversarial learning so now i am asking this question  that i want and here its in of course a toy setup there is no reason i why i would want  to generate dumbbells but say if i am going to if its an automatic verification whether  my product looks like a dumbbell or not i might want to do this right so you could  think of all sorts of reasons why you want to do this so what we will do is the question  that we are interested in is that i have a blank slate with me it just contains some pixels  i want to be able to modify this pixel so that my class dumbbell class gets fired   now we have done enough gradients enough back propagation everything in this class  so i will ask you to give me a solution for this and the hint is treat the image itself as a  parameter matrix the second hint is assume that all of this is going to remain constant  you are not going to change any of this and you have initialized your parameters which  is the image pixels to zeros that means you are started with a gray image   now i will change the question a bit only a bit and all of you will be able to answer this  ok suppose my network is strained and now i want to change the weights in this layer  so that my accuracy improves so that when its a dumbbell class it predicts dumbbell  how will you do that it will pass the same image what will you do how will you  change the weights in this layer back propagation what is the update rule say the  gradient descent update rule say that the gradient descent update rule  student refer time three hundred and one  w is equal to ok you guys actually unanimously said gradient is an update rule ok so w  is equal to w minus oops oops oops ok minus eta into ok  student refer time three hundred and sixteen  thats what you will do now if i ask you the question for this you can answer it but if i  ask you the same question here why cannot you answer it   student refer time three hundred and twenty-seven  so here what were you doing computing the gradients of the loss with respect to the  weights what will you do here  student refer time three hundred and thirty-three  it put respect to each of these pixels and then update this pixel by using what formula  student refer time three hundred and thirty-nine  ione thats the first pixel is equal to ione minus eta gradient ione where what is gradient ione  actually everyone gets the intuition right you can do it now     so we could pose this as an optimization problem where what we want to do is given an  image we want to maximize the score of the output class and i also want some  regularization because whatever i get i want it to look like an image right  so we will see different types of regularization for doing this some very simple  regularizations but this is the overall idea right so any generic loss function is always  the training loss plus the regularization so i have just kept both the training loss as well  as the regularization whats my training loss the score for the class that i am interested  in and what are the parameters of this object of this optimization problem the input  pixels right   so far we had already be always been doing w b but instead of w b you now have i as  the parameters of your optimization problem is that fine    and now we can just think of the entire image as a collection of parameters and we can  now update the weights of this matrix which is the image matrix ok    so let us see how we will do it so we start with a zero image as i said set the score vector  to all zeros and one for the class that i am interested in ok   now compute the gradient of this score vector with respect to ik its i want this quantity  to be maximized everything else to be zero so thats what my loss function is so i am  going to compute the gradient of each of the pixels with this now i am going to update  the pixel using my gradient descent rule which i just explained brief previously  now i again do a forward so now instead of this zero image i have a modified image  slightly modified image because the pixels i have moved away from zero update based on  the gradients now this image i will pass back through the network and what will i do  now again change so this is the same as the weight matrix right so you should be able  to visualize it exactly the same way as you would have visualized this you had certain  weights here you change them a bit again did the forward pass again did the backward  pass change them a bit and keep doing this till   student refer time zerofivefive9  till convergence right whatever is your definition for convergence till you are satisfied  and instead of score of one you are at least getting a score of zero9 or zero9five or something like  that right so we will keep doing this right till convergence at the end you will you all  of you can imagine that this image will keep getting modified ok     so now let us see if we learn run this score or the run this code for certain classes so i  mean interested in the dumbbell class and i have ran that algorithm starting with the zero  image and this is the kind of image that i end up with you see a dumbbell here without  me drawing it right if you go back and look at it you will see that there are a lot of these  dumbbell like shapes which have actually appeared here the colour is of course very  much different i dont think dumbbells are of these colours ever but you can see that its  actually trying to produce that shapes which will cause the dumbbell output to fire   now what is interesting is that its being very redundant so its not trying to generate a  single dumbbell a generating a lot of dumbbells of different orientations so i just  keeping its basis covered so that some of this should actually fire and cause a dumbbell  output to be maximized ok     now let us see if we take a cup and this is like the trophy cup i believe so this is what  is appearing here there is one more cup here and there is one more cup here its a  generating these cups so that you cant be you would not be able to see it its different oh  it really looks like i am manipulating it but i am not you can go back in check it those  cups are there ok     and then for dalmatian actually this at least you can see some white and black spots  right at least thats fine  so dalmatians are these dog which have these white and black spots so and you can  also see some kind of a shape here right which with my drawing so it is actually  producing that doglike shape and its producing multiple of those so its being redundant  i am trying to compute that right     and now you see right with these very arbitrary images which to you and me do not  know nowhere close to we will fire will classify this as dalmatian but for the machine  and is classifying this as a dalmatian and this is bad right this is not good there is  nothing to be impressed about this is actually bad because i can give it these horrible  images and still get away by something called as a dalmatian   so if i want to sell some a dalmatian on olx this is what i can do right i can upload  this image and a machine would trigger it and some one would buy it ok so and this is a  bell paper so you can go back and see you see a lot of bell papers here and similar for  lemon and so on right      so various classes you can see that its actually trying to produce those shapes but its  nowhere actually producing a clear image which is undoubtedly of that object right is  generating something which can later on be used to fool the network right which is not a  good thing ok and we can actually do this for any arbitrary neuron so i was trying to  actually fire this neuron which was the output layer but maybe i want something else to  fire here so i want to actually see what is it that causes this neuron to fire so i could  repeat the same algorithm by setting something here as high and then again back  propagating the gradients only from here and reconstructing the image every time so  that this neuron then five is right     so these are what the updated images look like which excite certain neurons and some  layers so what does this look like its actually like a pirates ship if its not very clear  you have these multiple layers of things and something like this ok so its some neurons  are actually firing for this kind of a pattern there are some other neurons which are firing  for different kinds of patterns and so on right   so you can just create images which cause certain neurons to fire and all these are lot of  fun to do so you should i would encourage you to do this i will get more insights into  what your network is   
task2/super_cleaned_audios/lesson71.wav,1646.6613,let us start with better activation functions    so before i get into activation functions right let me first tell you why i care about  activation functions why do i actually want to come up with better activation functions  so will start with the following question what makes deep neural networks powerful  among other things what is this one thing which makes it powerful so let me give you  this intuition    this i have a deep network ok and do not worry it is a thin network but i could have  had a wide network also but just for illustration i have taking a deep network thin  network   now imagine that each of these neurons that you have if i replace the sigmoid in each  layer by a simple linear transformation a by the way this is technically incorrect so  orange is always input so this should not be a sigmoid there right either add one more  layer there or let us change the figure  so suppose i replace all these sigmoids by linear transformations what would y be can  you write y as a function of x what would it be give me the function will we just be  this right so first we will do w one of x which is this right then will take w two of that then  w three of that and w four of that  so i could actually have written this just as y equal to w x where w is equal to w four w three  w two w one so there is no depth here there is actually only one weight which i could have  learned you get that right if you just have all linear transformations then essentially you  do not have so many weights you just have one weight throughout you get that make  sense    so what you are learning eventually we will just y as a linear function of x and initially  at some point we started off with such linear functions right w transpose x in the case of  perceptron and mp neurons  so what does that lead to what kind of decision boundaries does that lead to linear  decision boundary so if you do not have these nonlinearities we cannot have these  arbitrary decision boundaries will only be left with linear decision boundaries    in particular will not be able to solve this problem that we had right we were given some  red and blue points and there was no way to draw a line such that the red points are  separated from the blue points what we needed is some kind of circles or ellipses to  separate the red points from the blue points that cannot be done with linear decision  boundaries that can happen only if you use a deep neural network with nonlinear  decision boundaries and we actually have a proof for that what that proof the universal  approximation theorem actually towards right    so that is why nonlinearities or the activation functions clear a very important role in the  success of deep neural networks right hence you want to examine them very closely  and see what are the newer kinds of nonlinearities that have been proposed so we  always start with the basics so will start with sigmoid see what are the problems with  sigmoid and then see what we can do to solve some of these problems    so this is what the sigmoid function looks like you have seen it a million times and it  actually constrains the input to zero to one right so it takes some input and it constrains it two  values between zero to one now since we are always interested in gradients right because  the entire training and that is why i did that precursor in the first module the training  always depends on gradients  so it is always important to look at what does the gradient look like so we know what  the gradient looks like we have computed this is just sigmoid of x into one minus sigmoid  of x so now let us see what happens if you use such a sigmoid neuron in a deep neural  network    this is a deep neural network and without loss of generality i am going to use a thin deep  network but the same holds for a deep for a wide deep network also so suppose you  are interested in computing the gradient with respect to w two right at some point in the  chain rule you will have this term how many of you are convinced about this ok and  that will lead to this could that cause a problem  so at some one of the terms in your chain rule is going to be this dou h three by dou a three i  am assuming all of you are convinced about that and i have given you the exact formula  for dou h three by dou a three will that lead to a problem  student refer time fourthreetwo  good so what is the consequence of this to answer this we need to understand the  concept of saturation right    so a sigmoid neuron is said to have saturated if it is output is one or zero or rather close to one  or close to zero ok what would happen in that case to the gradient  student refer time fourfour8  it will vanish right because sigmoid of x into one minus sigmoid of x so it either  extremes is going to vanish and you do not even need the formula for that you can just  see it from the diagram right because the gradient here is going to be zero that is obvious  right it just a what horizontal line  so this gradient would be zero so fine why does it bother us what is our entire training  premise based on gradients right what does our update rule what happens if this guy  is zero no update e the weights just stay where they are right that means the training  gets stalled right  so think about this right if all the neurons in your network have saturated that means  all the weights the gradients will be zero that means all the weights will remain where they  are you pass another input nothing is going to change right it still be zero so if this  neurons have saturated your training will just stalled ok so that was one of the reasons  which is to cause problem in training deep neural networks earlier right    so that is one of the reason why it was not converging because these weights used to  these neurons is to saturate so this is one problem with sigmoid neurons a saturated  sigmoid neuron can cause the gradient to vanish    but why would the neurons saturated i mean what would cause them to saturate ok  this saturate find their gradients will vanish but why would they saturate we should be  able to get some hints from the figure that has been drawn so this is actually that x  needs to be changed  so on the x axis we have x quite obviously but that has to be something else  so what  it is what is happening is what does the sigmoid neuron do it takes this aggregate it  or someone just disappear refer time 7two6 so is it very boring today no right so  you have this aggregated sum of the inputs once you have that aggregation you applied  the sigmoid now tell me when would it saturated  student refer time 7three9  when the aggregation is very large that means one of the two things could happen either  the x’s are very large or the w’s are very large would the x is x is be large i see a lot of  you saying no why  student refer time 7four5  good we normalize them right we make sure they are between zero to one so we do not  allow those arbitrary large values of pressure density and so on right we make sure  they are between zero to one so then the weights can be a problem right now why would  the weights be lies move later first  student refer time seven hundred and fifty-eight  if i initialize the weights to a large value if i initialize all my weights in my infinite  wisdom to a large value what would happen right from the first training example itself  w i x i would take on a very large value and your neurons will start saturating so  imagine if all the weights throughout my network are initialized to large values  then right from training instance zero my neurons will start saturating and i will not be  able to train anything how many of you experienced this while doing back propagation  and the others did not do the assignment they copied it please raise your hands how  many of you experienced it now many more hands will be raised still now ok honest  people that is a paradox    consider what would happen if you use sigmoid neurons and initialize the weights to a  very high value they will start saturating and hence you will have this problem of  vanishing gradients ok everyone gets this so this is a problem at this sigmoid neurons    the other problem with sigmoid neurons which is very interesting is that they are not zero  centered what do i mean by that they are not zero center that is what it ok so sigmoid  is are not zero centered what do i mean with that mean by that they are not zero centered  the value is between zero to one right so the average cannot be zero it is always going to be  above zero ok sigmoid neurons are always going to take on positive values between zero to one  so why is that a problem so that is an interesting explanation oh did i say that did i  put the acknowledgements somewhere so all of this material that i have been talking  about it is taken from andrej karpathys lecture notes so here is this interesting  explanation for this  so now consider this particular network ok and i am going to focus only on this part  that means the output layer and just the layer before that and the layer before that has  these two weights wone and w two i am going to focus on that  so to update these weights i need to compute so what do we need to compute  gradient ok now you will answer so we need to compute the gradient with respect to  wone and w two and this is what it is going to look like what is the red part and blue part  why red and blue the red part is dash for both common for both right  so this is going to be common i do not know why i did that ok refer time 9three9 so  this red part is common for both and what is the blue part actually what is dou a three by  dou w one h two one and dou a three by dou w two   so dou a three by dou w one is just h two one and dou a three by dou w two is just h two two ok so let me  just plug in those values and note that h two one and h three are between zero to one so can you make  some interesting commentary on this interesting but useful not just philosophical stuff  that these two derivatives are for the weights at a given layer i have just taken two weights  but i could have taken n weights and the same thing would have hold  because i know that the derivative is proportional to the input that it gets and the rest of  the part is going to be constant because that is coming from the chain rule up to the  previous layer right  so now what is happening because of that just to make fun of you guys i mean if you  get that sorry good yeah it is not very straightforward but let us see so if the first  common term in red is positive right then what would happen to these two guys they  would both be positive right because h two one and h two two are positive  now the first common term in red is negative then what would happen to these two guys  both negative so that means the gradients of the weights at a particular layer where  either all be positive or they will all be negative you get that that is because of this  common part and the blue part the blue part we know is positive  so what matters is the common part and that common part can either be positive or  negative for all of them together right that means for a given layer all the gradients at a  layer are either positive or they are all negative so let us see what is the implication of  that right    so this actually restricts the possible update directions  so which is the quadrant which has all positive first ok sorry for embarrassing yeah  and all the negative is the third quadrant that means your movements can only happen  in the first quadrant and the third quadrant so do you see a problem with this right so  you are going to actually try to move that your theta  which is a collection of w one and w two is theta minus eta into the gradient right and you  know that this vector which is the gradient vector can either be positive that means can  lie in the first quadrant or it can lie in the third quadrant these movements are not  possible that means there are certain turns or certain movements or certain directions  that i am not allowed to take so what would this mean it would take a dash time to  converge  student longer time  longer time to converge right because i am restricting my movement so imagine you  have to go from destination to destination b and i say that you can never take a right turn  right and there is some going to be some problem it will take longer to reach there  unless the directions are to our left right unless your destination is refer time onethreetwotwo  but that will not happen    so suppose this is the optimal w star    and we start with some random initialization because that is why we are going to start  then the only way i can reach it is i may by making a series of this kind of movements  right as the exact pattern is what will have to take because these are the only  movements which are allowed or some movements which are allowed and it will lead to  a certain cryptic pattern and i will not be able to have the complete freedom of moving in  the direction which would have directly taken me to the optimal  so that is a problem with something not being zero center and lastly sigmoids are  expensive to compute because you have to do this exp right it is not something as  easier as something else that we will see in the lecture today ok so these are some  problems with sigmoid functions   student refer time onefourone5  so this is some issues that were they with sigmoid functions so this pointed that ok  maybe we should try better activation functions    that is why tanh become very popular but tanh is not something which happened post  twozerozero6 right so this was like 9two or 9three when i think yan lacunae had started moving to  tanh from sigmoid functions right now again here other inputs are compressed between  minus point to one ok where inputs are now zero centered which takes care of this problem  which i mentioned at the end  that these directions of movements are constrained and was the derivative of this  function one minus tanh square right what happens at saturation even without looking at  the formula the gradient would vanish to zero right so the vanishing gradient problem is  still there  what you have solved is a problem of zero centering and that itself used to give better  results than just using a sigmoid function but it is still computationally expensive  because you still have to do these e raise two components right the you still have to  compute these exponential powers so it is still computationally expensive    so then in around twozeroonetwo i guess is when this relu was introduced in the context of  convolutional neural networks right and this is what the relu function actually looks  like is this a nonlinear function it just looks like a line right why is it a nonlinear  function it is a nonlinear function right because x is you cannot write x the output as  a function of i mean as a linear transformation right so you have this zero in fact if you  take two relu functions smartly    you can actually get the sigmoid i mean you can get an approximate for the sigmoid  function so you can go back and check this right so if you take these two functions and  subtract one from the other what is this this is a relu function this is also a relu  function right  so i define relu as max of zero comma x so both of these are relu functions some  variant of that and now if you subtract one from the other you will actually get a  approximation of the sigmoid function right and this cannot happen if you have two linear  functions take any two linear functions you will not be able to get this kind of an  approximation    so relu is a nonlinear function what are the advantages of relu one is it does not  saturate in the positive region right it is computationally very efficient the output is  either zero or x there is no powers nothing like that right and it practice it converges much  faster than sigmoid and tanh so that is what this twozeroonetwo paper show and now relu has  actually become more or less the standard in all convolutional neural networks    but there is still a caveat while using relu ok so the derivative of relu we can see  that if x is less than zero then the derivative is going to be zero right and if x is greater than  zero then the derivative is going to be one and that straight away follows from the definition  of relu which is zero or x  so when it is zero the derivative will be zero and when it is x the derivative will be one so now  consider this given network and let us assume and this is not a very far faced assumed  assumption it can happen in practice that at some point a large gradient causes the bias  b to be updated to a large negative value so what i am saying is that something happens  and b gets updated to a large negative value    now what would happen to this quantity remember this quantity which i have circled is  actually the input to the blue colored relu neuron that i have so i am asking you what  would happen to that input that input would become negative  so the neuron would output zero and i am calling it a dead neuron why if the input is zero i  mean is a input is negative then the relu functions output would be zero what would  happen to the gradients during back propagation zero that means what would happen to  the weights  student refer time one8zero6  would not be updated right now but that is fine right if you give some other input this  will recover why am i calling a dead means permanent right unless you are in some  fantasy world but dead is dead right so why am i saying that it is dead i could might  as well i would give it a next input and then probably things would be ok bias is still  very negative because nothing is getting updated right or bias is still very negative you  know that x one and x two are constrained because you have normalized them right and w one  and w two have not been updated  so still the situation does not change so what happens is that once a relu neuron dies  because somewhere in the chain rule you got a zero it will stay dead forever ok it will  never be able to come out of that it will always produce a negative output that means  that output will be clamped to zero that means no gradients will flow back and that means  all the weights will not get updated connected that neuron    so in practice when you train a network with relu you will observe that a large  fraction of the units can die if the learning rate is set too high why this if condition  student refer time one9threezero  what was the assumption that i made that the bias receives a large negative update and  that is possible if your learning rate is very high because you got some small negative  gradient but your learning rate blew it up  now what is the practical implication of this if a training a network and a large number  of your relu neurons have died what does it mean most parts of your network are  dash useless they are not learning any feature nothing right is all zero that means you have  this large number of parameters versus getting wasted because they feed into a relu  you function and the relu function just keeps outputting zero   so if you have n neurons in the particular layer and most of them are zero that means you  are not really learning an n dimensional feature representation you are just learning a  much smaller feature representation right so can you give me a simple way of one  simple way of avoiding this among many other ways  student refer time twoonezero7  no dropout is statistical right it is probabilistic this is like always dead one thing is to  update the weight to a large to a positive value and zerozeroone mind you is a large positive  value right later on we will see y but zerozeroone is reasonably large ok so were going to  initialize the bias to a positive value  so that even if this large negative gradient flows through there is still a chance that it  will not become very negative and hence it will not mess up the things the way it does  that is one solution to that right but still you will find that even after that the relu  neuron a lot of those can die but still in practice they work better for a deep  convolutional neural network ok and we can also use other variants of relu    so there have been to avoid this dead neuron problem there are other variants of  relu which have been proposed and that is what we look at next so there is  something known as a leaky relu is it obvious from the equation what it does right  so instead of producing zero it will just produce a very small value proportional to the  input now what would happen to the gradients they will not saturate right will have  the gradient would be if the input is negative what would the gradient be  student refer time twoone56  zerozeroone right so that means some gradient will still flow through how many if you get  this right so that means if you use a leaky relu neuron some gradient would still  flows through so just understand this trend right that ah and this is i mean all this stuff  is simple there is nothing great in this but just put it in context right  so in twozerozero6 to twozerozero9 people realized ok now we can trained networks and maybe  whatever we have done with unsupervised pre training actually corresponds to better  initializations or better optimizations or better activations and so on  so now let us try doing research in that so that led to the discovery of relu now  people started observing problems with relu and then proposed a variant of it which is  leaky relu right so that is how this area has now become very prolific and grow right  so we started off with this seed idea that it is possible to train these deep neural  networks and now we are trying to make arrive at better and better ways of doing it  making it more and more easier to train them and take care of some of these  irregularities which existed earlier so one of them being sigmoid not being a very neat  function to optimize with right so that is what all this is about individually all of these  are probably easy for you to understand once you go back and look at the slides you all  this is nothing great in this  but what i want you to really understand is this bigger picture of what is happening here  as long as you get that time frame with and of course leaky relu is again  computationally very efficient there is no exponents no squares nothing like that and it  is close to zero centered and it is still not zero centered but close to zero centers because you  have outputs on both side and then someone came up with a generalization of this which  is parametric relu so y zerozeroone make it alpha x and alpha will also be a  student parameter  parameter it is a trainable parameter it is not a hyper parameter ok how many of you  know the difference between parameter and hyper parameter ok you have used this in  the back propagation as i am right so it is a trainable hyper parameter it will get  optimized along with your other parameters in the network    so then someone said leaky relu fine parametric relu is fine let us try to do  exponential relu ok so it has all the benefits of relu it ensures that at least a small  gradient will flow through even when your inputs are negative that means it avoids this  dead neuron problem again close to zero centered outputs but it is expensive because now  we have added this exponential right  so these are all ideas which came out during this period and all of them were shown to  work better than the other and so on and of course at the end i have to tell you a final  conclusion right whenever i give you so many possibilities  so i have given you sigmoid tanh relu parametric leaky exponential now what do  you use right this the idea is not to confuse you but to give you one solution which  would largely work yeah what regularization  student refer time twofourfour5  yeah you could have done yeah that refer time twofourfour8 there is exactly so a lot of  this research right which has happened in this period it is not a lot of it is juristic right  you solve one problem with relu ok the neurons and saturated ok just make it  something which does not saturated    so that is there it is possible that the other solutions would also go there is not that this  is the only solution which works now then someone came out with max out neuron  which is a generalization of relu and leaky relu why do i say it is a generalization  what was relu that means w one equal to b one equal to zero w two equal to one  so it is a special case of the max out neuron what about leaky relu this was  parametric value but again what about so now what is happening w one equal to alpha b  one equal to zero w two equal to one b two equal to zero so you see how it generalizes right so this is  how these variants keep kept coming up     now the problem of course is doubles the number of parameters right because you  earlier had only w transpose x plus b now you have w one transpose b one w two transpose b  two and so on right so it is actually doubling the number of parameters that you have    so now coming to the final conclusion of all of this right what you need to remember is  that sigmoids are bad  so no one uses sigmoids in convolutional neural networks they still use somewhere i  am i am sorry about this relu is more or less the standard unit for convolutional neural  networks  so any standard cnn that you will pick up it will use relu as the activation function  if you want you can explore leaky relu max out elu and so on but it will require a lot  of careful tuning say if you want to use something out of the bulk box relu is just fine  relu just works fine in practice despite all this dead neuron and other problems  student refer time two7twozero  yeah so then the argument for that is that how often when you reach the point x equal  to zero right so the chance of that having is happening is very very low and if you get  there you can always approximate it by some epsilon or something and for that training  instance just go on right any ways you are making so many approximations with  stochastic and mini batch and so on  so this is one more approximation that is how people typically deal with it but in most  cases it will not come in that point appearing is very low but the question is valid and  tanh sigmoids are still used in lstm’s and rnn’s which you will see at some later  point in the course ok so there are a couple of more modules that i need to do so we  just take a break here  
task2/super_cleaned_audios/lesson65.wav,249.0232,so now going on to the next module which is adding noise to the outputs  so here when you are given some training data this is the label vector that you have  been given right where one of these elements is one so these are like zero to nine eight where  which digit it is and in this case it happens to be digit two so that element is one right that is  the true training data given to you    so what you could do is actually and actually what you try to do is minimize this  quantity p i log q i where what is p i p i is the vector which was given and what is q i  the predicted probabilities ok so now when you try to add noise to the output what  you actually do is you see that i do not trust the true labels they may be noisy   whatever data you have given to me that is one way of looking at it that i do not trust  it i will just say that it is noisy the other way of looking at it is that in some way i am  ensuring that i do not try to over fit to this label right because now my true whatever i  am trying to optimize let me just go to that and let us see so instead what we will do is  we will use soft targets    so this is what i mean by soft target assume that there was some epsilon noise in your  labels so instead of treating this as one and all zeros treat the true label as one minus epsilon  and divide that among the remaining nine entities right that probability mass divided among  the remaining nine entities  so now when you are trying to minimize this what is p i this soft distribution right and  q i is the predicted distribution so you see why this acts as a regularization why does it  act as a regularization what is the aim of regularization do not over fit on the training  data right to over fit on the training data what should it have done it should have  treated only the correct label now if i am giving it this information then i am not  allowing it to over fit on the training data right  because now with this distribution this quantity will not get minimized when q i is  equal to the onehour distribution where all the masses on two do you get that so in some  sense we are making sure that now if it tries to over fit on the training data it will not  get the minimized error right so you have this corrupted the outputs of it everyone gets  this is ok the trainer no that is the whole point   student refer time zerotwo4zero  no  so that is thing right so some of these are heuristics based so now we have started  with this whole derivation where we try to show the relations between trainer error  tested or not but things that we have seen some of these things right even whatever  unfortunately i tried to prove on the previous slide the weight decay thing even that is  only for these neat networks where you do not have any hidden layer and so on right  so most of these are just heuristics you are just saying that the principle is that you will  not allow the true training error as computed from the training data to go to zero if you do  that you know that you are going to over fit so try whatever you can to avoid that ok  that is the idea do you agree that doing this is going in that direction   student refer time zero3two5  training data the hope is that if you do not do that then it will not under fit on the test it  right  there is no i mean i have you are you looking for a proof where i say that doing this we  will ensure that a training error does not go to zero but the test error comes close to the  training error there is no such proof right just a heuristic it is going by the principle  that if i do not allow the training error to go to zero then hopefully i will over fit i will not  over it as much as i would have otherwise right   so that you can think of it as this way right so this is the curve that you are seeing it  this was a training curve this was your test curve you are preventing from entering this  region where the error is zero that means you will end up somewhere here right and you  know that that is a more preferred point as compared to this that is the intuition that you  are going right is that  
task2/super_cleaned_audios/lesson59.wav,1038.8282,so we spoke about bias and variance and we saw that simple models have a high bias but low variance and complex models have a low bias high variance and so on and we saw it some illustrative examples that what that is what that means and then the important thing to note was these two formal definitions of bias and formal definition of variance which you all know anyways and then the important concept that we spoke about was the train error versus test error right so this was the curve that we were interested in and one corner of this curve was related to high bias low variance and the other corner was related to low bias high variance right so i am looking for something in the middle that is what our quest is in this lecture right and we want to find ways of falling somewhere in middle and this led to the definition of two quantities of interest or training error and test errors so training error is computed from the training points these are the points that you actually look at while you are solving this optimization problem so the training always involves solving an optimization problem which is the objective that you want to optimize or maximize and the test error is something that you want to use it for at eventually so you all have these two quantities of interest that we design and we realize that the training error is more optimistic whether the test errors actually gives us the real picture of what we do and we tied those back to things that you have done previously in the machine learning or other courses that we always split the data into training valid and test training it on the training data do some validations on the validation data but never look at the test data that is for the final evaluation so that’s the this is this intuition which i have been trying to build with these two curves is the explanation for why we do things that way now we are interested in doing a more mathematically rigorous analysis of this intuition right so that is where we left off so what we are interested in so now i will just start from this point is that we are given some data which is m n m training points and n testing points and we know that there is a true function between the outputs and the inputs and we are also expecting or accepting some noise in this relation just as in any other relation so which means that y is related to xi but by some true function but there is also this noise and for simplicity we assumed as this noise comes from a normal distribution with zero mean and some small variance and as usual we never know f right but we are trying to approximate this f hat and we come up with some parametric form for f hat and then try to learn the parameters of f hat from the training subset of the data that is given to us so this is what we always do and we have already seen different variations of f hat one of them being the deep neural network and what we are actually interested in is this quantity the expected difference or square difference between the predictions made by our model and the true value of the output with respect to the true function right then we asked i asked you whether we can actually estimate this quantity and all of you said no why it is because you do not know what f of xi is right so we will see how to estimate this empirically so then we started off with this information that we have we know what y i hat is because that is the prediction that we make and we know yi what yi is we do not know the function but we see the output of the function in the form of the training data points given to us or any data points given to us so we wrote this by making this particular substitution where we notice that yi that we see is actually the true function plus some noise and then we did some trickery and try to simplify this and then we just realize that this is the term that we are interested in so we moved it to the other side of the equation and came up with this neat left hand side or neat right hand side that we need to analyze now so far everything is clear this is where we ended the last class right you just went to it very quickly but i assume everything is clear at this point ok fine so we are left with a bunch of expectations right and we have i am assuming we have no clue how to estimate this right i mean so and remember that when you are dealing with expectations as always this true expectation and then there is this empirical estimation right so what we are going to move towards so these all equations when i write e here capital e here i am talking about the true expectation now we will see how to approximate the true expectation with an empirical expectation and then based on that we will make some observations so that is what we will do now so we will just take a small d two and i will just tell you what expectations are or what empirically expectation is how to compute them so suppose we have observed the goals scored in k matches there is some k football matches that we have seen and we have seen that the goals scored were the following now if i asked q what is the expected value of the goal now the number of goals for what will you do take the average of this this is what you will do so what is it that you are doing here you are taking a dash estimate of the expectation empirical estimate you are making some observations these are the observations given to you these are the k matches watch as much as many football matches as you want after the semester ends and then notice the number of goals that were scored in them and then you can compute this expectation right and this is how you do empirically so there is something that we do on a regular basis but i just want you to realize that what you are doing is actually an implicit estimate of the true expectation now can you relate this to the quantity that we are interested in we are interested in computing a certain expectation which is this can you take an analogy and tell me how you would do this the hint is we have done this a million times in the course already fine so this is how we will do it and have actually done this a million times in the course so when you compute this we are actually doing an empirical estimate of the data so let us just take a minute to understand this we are given some data we are interested in this to expectation which we cannot compute so we will take this data we will assume there is enough of this we are given m samples which are enough and from that we will make an empirical estimate and just as in the case of these goal scored right as you see more and more matches you will have a better understanding of how many goals can be scored when two particular teams are playing in the same analogy goes here as you see more and more data your estimate would become better but that is how you will do the estimation so now we will come back to so now do not get surprised when i am going to replace all these e’s by this all the e’s that we had in our original equation i am going to replace them by these summations ok fine so this was our original equation that we had derived and we were interested in this left hand side quantity which is a sum of some terms on the right hand side so now this expectation i told you that we can estimate it from data but which data training data or test data both so we will try to estimate it from both and see if there is any difference which arises when you estimate it from one data and the other data ok so the first thing that i am going to do is i am going to use test observations to estimate this so can you tell me what are my summations going to look like it is summation over n plus one to n plus m right we assume that the first endpoints are training points and the remaining points are test points so the quantity on the left hand side is true error remember that because that has f x which we do not know quantity on the right side the first thing is empirical estimation of the error ok the second thing is a small constant however the epsilon i square and we assume that comes from a normal distribution with a small variance what is the third quantity actually i have given you the answer already but i want you to think about it i am saying it is the covariance between two things when i say it is the co variance between two things what is the first thing that i need to prove is that the two things are dash random variables i mean first thing we need to see is that the two things are random variables epsilon i clear it is a random variable what about this other thing or rather epsilon is a random variable what about the other thing and depending on the training instance that you have sampled this ongoing difference is going to differ right you are having your training or test instance whatever is this x i this is going to differ because these x’s are different they are all random variables so there is difference between these two quantities also going to be a random variable is that fine ok but still is this the so then i have told you this is x and this is y and what i am saying is that the co variance between x and y is just e of x x into y is that correct that is how you define co variance what is the definition of co variance if you have bothered to look at the prerequisites no expectation in the form of e so co variance is e of x minus mu of x into y minus mu of phi what is our x epsilon and what is our y what is mu of x zero so i will just simplify this a bit ok i will open up the product what is mu of y into e of x what is e of x what is the expected value of the noise zero so then this turns out to be as that is that fine that is why we are writing the co variance is just the product of the two things so let us just take a minute to again understand this the true error is the empirical estimation of the error plus i mean plus or minus a small constant ok and then this nasty quantity that we do not know what to do with it so let us look at this quantity and see what we can say about it now what is the co variance between these two i am trying to compute this expectation from the test data just remember that so each i here is a test instance are these two random variables dependent or independent is the question that i am trying to ask it is independent so let us look at it piece wise so remember that we had said that y is equal to f of x i plus epsilon i right this epsilon i had no relation to f of x i i mean i could choose any x i but this noise is going to be random so there is no relation between these two now is there a relation between f hat of x i and epsilon i we are doing tests so how did we come up with f hat of x i how did when i say how did we come up with f hat is i mean how did we learn the parameters of a f hat using the training data and what are we computing expectation with respect to now test data these these epsilon improve influence the parameters that we had learned further from the training data no since there is no dependence between these two guys so that is why epsilon i is independent of the other random variable that you see in this expectation is that clear do you get the intuition f hat x i further no but this is the mean this noise is what is present in the test data and you have not seen this add training time when you are training the parameters you did not look at this noise you are looking at the noise in the training data so this is not participated in the estimation of the parameters of f hat but that was for the training data right but this now i am doing the expectation from a test data so these two random variables are independent that means i can write this as is this fine what will happen to this zerook so what did we eventually conclude that the true error is equal to empirical test error plus a small constant right so what does this tell you now tell me forget the math tell me in english right what does this take what does this mean can you relate it to now why you do this training error validation error test error so what does this tell me this tells me that if i have trained a model and now if i take an estimate of the error on some data which i had not used for the training then that error which i see is actually very close to the true error it only differs by this small constant how many of you get that that is why when i look at the validation error it is not being overly optimistic it is giving me a true picture of what the actual error is right so there are two things that you need to understand here one this is the quantity that we are interested in which we cannot estimate we are trying to estimate it by using this we are trying to make an approximation so we are trying to see how good this approximation is what this derivation is telling us is that if you are approximated it using the test error or the test data then this approximation is actually very close to the true error and how close it is actually it just differs by this small constant so you get the importance of what we are seeing here right ok now to truly appreciate this i need to tell you what would have happened if you had used the training data for this estimation right it is largely dependent but that is again a normal assumption that you make so this is ok good that you asked at this point i will be doing a couple of things today where we will be deriving some things we will try to prove some things mathematically but all of these would have underlying some assumptions so if you remember the adam derivation with this we did there also we had made this funny assumption that the gradients are actually coming from a stationary distribution which will not happen in practice so this reminds me of this joke from big bang theory which says that i have a solution but it only works for squared eggs in a vacuum right so it is basically all these things always have some assumptions underlying them but the idea is to kind of ignore those assumptions and see what happens in a neat setting and at least see whether in a neat setting everything works fine or not so that is what is happening here so is a valid point that you are assuming that the noise comes from a zero mean distribution now if the noise did not come from a zero mean distribution then this would have not gone down to zero and the mean would have been higher than this is no longer a small constant and so on so those things are there so this is going to happen in some of the other derivations that i do today it is not that i am teaching you something wrong it is just that you have to take it with a pinch of salt in the sense that these assumptions are there and the original derivations these are not my assumptions and they work only under those assumptions so you have to be careful about that but the idea is that still with these assumptions can we at least make something meaningful out of it right is that fine with everyone can we all work with that basic premise so what i have done so far is told you that if you are estimating the errors from the validation data you are doing a good job now let us see if i would estimate the error from the training data take a guess what would happen what would my argument for this be now this will not disappear right because these two are not independent now i cannot write it as a product of two expectations that means it will not go down to zero so that is the argument which i am going to make so hence actually the true error if you see right it is equal to the empirical estimation plus some quantity that means the true error is dash as compared to the empirical error that means the empirical error that we see is pessimistic or optimistic optimistic that is what i started with that you gave a very optimistic estimation of your error if you are looking at this empirical estimation from the training data because you have ignored this quantity is it fine so what is missing in the story let us see now what was this quantity so far all our discussions l theta right but now suddenly i have realized that my true error is actually l theta plus something else right you see where i am headed with this ok so that is what we need to see now ok now think it would be we should but i am pretty sure it is positive i cannot work it out right now but i am pretty sure it is positive and you can see and if you find it is not then let me know so how is all this related to model complexity we started off with this idea that model complexity tells you how much is the bias how much is the variance and because of that you get these two curves that you are not happy with one curve being very optimistic and the other curve being a bit pessimistic now how does this discussion tie up to model complexity 
task2/super_cleaned_audios/lesson58.wav,642.4244,so we would start the next module where we will talk about training error versus test error and before that we will see this bias variance tradeoff so now what have we done so far in these complex models and the simple models we have trained them using the dash data training data and what are we interested in always a test data right i already know what was the oil amount of oil mined from the training data locations that i was given and i am not interested in predicting those i am less interested in learning those so that if you give me a new location i should be able to do the right prediction so i am always interested in the test data so now consider a new point which is not seen during the test data and there are several such points that you could see now if you use the model f hat x to predict the value of y then the mean square error is given by you get this it is just the expected value of this squared error that i will get so what is the randomness here y expected value because the x that i am going to feed at test time is going to vary for each of these different xs i will get a different error so hence that is a random variable do you get that so please focus on these things right i mean just do not take a formula for granted just see what is it trying to see so whenever you see an expectation over something always question what is a random variable here so what is the random variable here it is the squared error loss why is it random it is because it changed the input x you are going to try it over a multitude of test examples you will take one thousand text examples one thousandzero text examples and so on right for each of this you will get a different squared error that is the randomness so you want to see what is the expected value of this or very loosely speaking the average value of this now it turns out that this now just try to remember that this is also some expectation and you had the terms f x and f x hat here this also had some expectation and term f x and f x hat and so on right if you do not remember the exact formula it is ok but you do remember there were some expectations inside and the terms f x and f x hat whether they are so this is just simple you are dealing with a minus b the whole square on the left hand side if you if you open it up rearrange some terms you will get this right so you can show that the mean average or the expected square error on the test data is actually the bias square plus the variance that is a small amount of irreducible error you can go back and work this out and actually the proof is given here on the link ok but i hope you get the intuition you have this a minus b the whole square if you open it up and rearrange the terms you should be able to get this now what does this tell you what happens if the bias is high the squared error is going to be high what happens is if the variance is high it is going to be high so that is why you do not want a very high bias you do not want a very high variance also you want this sweet spot in between where the bias and variance are just about optimal you get that that is why there is a tradeoff between bias and variance you cannot rely on simple models which have high bias you cannot rely on complex models which have high variance you want something in between now the parameters of f hat x remember that they are trained using the training data which consists of these end points that you have at test time we are interested in evaluating the model on a validation set which was different from the training data this gives rise to the following two quantities one is the training error which you deal with at dash time training time that is the error that you are trying to minimize right but a test time you have a different error which is the test error and that is the error that you care about typically these two errors exhibit a certain trend do you know what the trend is now on the x axis i have model complexity and on the y axis i have error as a model complexity increases what would happen to the training error it will go to almost zero that is exactly what happened from the linear function to the polynomial function this is how it will behave as the model complexity increases as the model complexity increases what would happen to the validation error it will decrease up to a certain point right because you are still not over fitting on the training data your answers are still generalized so you had this degree one polynomial degree twenty-five polynomial if i take in something in between then probably this is where i would have ended up with the training error and that would not have been too bad for the test error you see this ok now you see i will mark two points two regions rather one of this corresponds to high bias the other one corresponds to high variance tell me which one is which do this i cannot understand so let me ask this is this is ok good so you see that there are these two extreme and we want somewhere to be in between ok at least you get the intuition behind this fine ok and you are looking for this sweets spot which is the perfect tradeoff between the bias and the variance right so now everyone gets why there is a tradeoff and how this relates to model complexity and therefore we are looking for the ideal model complexity how do we achieve the ideal model complexity well we cannot really ideal is ideal but we try to do this using dash what is the title of this lecture student regularization regularization i will try to use regularization to achieve this ok so let us formalize this a bit more and remember that this curve is actually because of this equation that you see right high bias you will be in this region i am actually inserting it ok fine ok so the intuitions that we have developed so far is that if there are n training points and m test points then we have a train error which goes over the training points and we have a test error which goes over the m test points ok so i am just taking a total of n plus m points the first n is training the next last m is test now as the model complexity increases what happens to the training error it becomes very optimistic and gives you a very wrong picture of how close the predicted function is to the true function whether it makes you feel that you have done a perfect job this you have actually discovered the true function but that is not correct it is giving you a false picture of that therefore we should always look at the dash error student validation error validation error so now you see that why you always do this train validation and test split test is unseen you try to optimize on the training error ok but you should always tune for the validation error your optimization algorithm is going to take in the training error it is going to be very optimistic it is going to try to drive to zero but you should look at the validation error and try to see that you are not over fitting on the training data everyone gets this intuition so now this is all intuition we will have to concretize this mathematically so that is what we will do now so that d be these training test points that we have we know that this relationship holds we do not know what f is but we know that this relationship holds so what am i trying to say here that we know that there is a true relation between y and x which is given by the function f but i am also willing to admit some noise that may not be a very neat function but a small noise might exist that is the epsilon i ok and i am going to assume that epsilon comes from a normal distribution with zero means so on average the noise is going to be zero but there is a small variance everyone gets this this is a true relation but i am willing to admit some noise in the relation ok fine and of course we do not know f we never know f right now going by our paradigm where we have these five components we use f hat to approximate f f hat will have some dash which will i try to learn from the training data what is this dash parameters right which will try to learn from the training data the training data t is a subset of your total data which is thus those endpoints right and we are interested in knowing this quantity this is what we are actually interested in can we compute this quantity how many of you say yes how many of you say no we cannot why cannot we compute it we do not know f so why cannot you raise your hands if you all can answer in chorus so we do not know what f is then how do we compute this quantity right but what do we actually know so now we are going to see something which is true expectation and something which is empirical estimate expectation how many of you know this what is the difference between the two most of you should but it is not confident about it ok so we do not know what fxi is the true thing but what do we know we are given some training data right we know these yi’s for was training data and we know these yi hats for those training data so this is something that we can estimate yes or no this is given to us so this expectation is going to be an empirical estimate right because we are going to look at some one thousand one thousandzero twozerozerozerozero training points and estimate this right it is an empirical estimate how many of you get that now i am just going to rewrite some of this so what i have done is i just defined that yi is equal to fxi plus epsilon i so i have just replaced yi by that ok is that fine now this is of the form a minus b the whole square so i am going to treat it as that and just open up the bracket so i will have a square minus two a b plus b square and now this is a sum or difference of expectation so i can push the expectation inside so this is what i get this is this fine ok now i am just going to rearrange the term so remember this was the quantity that we were actually interested in but this is the quantity that we had a handle over because these were the data points given to us so i will just rearrange the terms and i can write this which was my quantity of interest as this can you estimate everything on lhs on rhs this this what is this variance sigma square we assumed it came from zero sigma square distribution and this can estimate the answer is no for the same reason we do not know what f of x is ok 
task2/super_cleaned_audios/lesson70.wav,1455.0627,so with that we go on to the next module in which we will talk about unsupervised pre  training  so this work which i am going to talk about they trying to understand what has changed  since the late 9zeros or the early two thousand how did deep learning become so popular despite  this problem with training them right this problem was there  so what happened to them solve it right and this field actually got revived by this  seminal work by hinton and others in two thousand and six    so let us see what that idea was so this is the idea of unsupervised pre training in the  original paper they introduce idea in the context of something known as r b m’s which  we will do in the last thirty-three percent of the course but we could do the same with auto  encoders which we have already done so in this lecture i am going to talk about this  idea in the context of auto encoders    so consider the deep neural network shown in this figure so the a module name and the  idea was unsupervised pre training so that itself is a giveaway of what is going to  happen ok so suppose this is the deep neural network that i have designed for a  particular classification task so what it is doing is this taking an input which is the red  colored neurons that you see at the input it has four hidden layers that means it is four layer  deep and then you have the output layer which tells you whether positive or negative  right that is the network that i have and i know that this is hard to train such a network  the loss will not converge and i will not get anything meaningful  so what these guys suggested is that forget about the supervised criteria that you have  that means you are trying to minimize a classification loss just forget about that just take  the first two layers of this network ok which is x and h one right so you take the original  input x you feed it to some transformations and you get the hidden representation h one  and now try to reconstruct x from h one what is this  student auto encoder  auto encoder ok what is the objective of the auto encoder  student refer time zerotwoone9  it is exactly this for each of the m training examples look at each of the dimensions of  your input and minimize the square difference between the actual input and the predicted  input right is that fine that is what an auto encoder does so this is what they suggested  ok so right now i am not telling you why this makes sense and all that that is what we  will do later right now i am just telling you the trick then we will analyze by that trick  works and why is this objective unsupervised  student refer time zerotwo5two  because we are not using any labels we just giving an input and we just reconstructing  the input we only have x’s we do not have y’s of course eventually we will use the y but  at this stage when i am calling it unsupervised pre training i am not using the y    now at the end of this what would happen yeah what would h one learn  student refer time zerothreeone9  it will learn an abstract representation of x was that our original task what were we  interested in  student refer time zerothreetwo9  in the classification task but we are doing something very different why we will see ok  now guess what would the next step be does this make sense    now at the end of the first unsupervised pre training i have ensured that h one which is  this layer has learned some abstract representation of the input right and that i know  from the auto encoder i mean the auto encoder which we have learned earlier right that at  learns an abstract representation of the input  now i have this so that means given an input i know how to compute an extract  representation and i am also sure that it captures the important characteristics of the data  i will just repeat this process i know that i have four layers in my original network so i  will now take h one try to compute h two and then reconstruct h one from it so the in effect  what am i doing in plain english learning and even more  student refer time zerofourone9  abstract representation of the input h one was already one abstract representation now  from this i am learning an even more abstract representation and does the objective  function makes sense right all i have done is replaced x by h one right in both these  places the rest of it is the same for all the training examples for all the dimensions and  throughout i am assuming that we are n layers i mean sorry n neurons and every layer  including the input layer  now what would the next step be fix the weights in h one layer fix the weights in it is two  layer and now try to reconstruct h two from this h two right and in this way we will continue  and learn all the hidden representations does that look ok right so at least this much  we believe it because we know that auto encoder works and you are just using an auto  encoder and we are using it incrementally from every abstract representation learn an  even more abstract representation  now at the end of this what will i do what was my original task  student classification  classification so what will i do  student refer time zero5twothree  what is a network that i have when i finish this unsupervised pre training  student refer time zero5two9  no tell me of the diagrams that you see on the slide how much of the network would i  have right everything except the green output layer right because the last step would be  take h four or sorry take h three and reconstruct h three from it and in the process learn h four right is  that clear  so i would have learnt till that point and now what i am going to do is something very  simple    i will after this layerwise pre training is done i will add my output layer now all the  weights in my network for every layer have been initialized  and they have been initialized in a way that that layer learns a good abstract  representation of the input right that is the thing that we have achieved at the end of  unsupervised pre training that every layer has learned and more and more abstract  representation of the input right now i will keep all these weights initialized to whatever  i learned in the pre training setup does that make sense  so that means instead of taking this big network with the output layer and initializing  the way it is randomly i am just going to use whatever weights i learned using the  unsupervised pre training ok so can you tell me what has happened in terms of the error  surface and so on or my movement in the w b plane or in this case this very high  dimensional w plane  i have reached some configuration for the w’s where i know that each of these layers is  a good meaningful representation of my original input right is that a fair statement in  english how many of you agree with this ah anyone has any questions at this point  one layer weights that is what you do in answer because if you train all the four then you  are again entering the same problem which you had earlier right  you cannot back you cannot back propagate through all the four layers because now it is a  deep network and we know that does not work so at every layer you fix whatever you  have learned so far and at a time you are training only one layer so that is one  interesting way of looking at it right you know that the deep neural network with four layers  was not trainable  so now we have reduced it to one layer at a time i knew that one layer at a time works  right is that fine now i will add the output layer and what will i do train the weights of  the  student output layer refer time zero75three   i will not just do that i will fine tune the entire network that means i will train the  weights of the output layer and i will also fine tune the entire network but now i am  contradicting myself i just gave an answer to him that again i am doing this deep  training and i know that deep training does not work  but this actually works do you get the difference right one is that when i start from i  take this big network i start from random weight initialization and try to train it that is  the story from one986 to two thousand and six that in most cases these networks did not converge  so now in two thousand and six we came up or someone came up with this idea of unsupervised pre  training where you train the layer network one layer at a time you do up till the last layer  now you add the output layer and then fine tune the entire network that means back  propagate over the entire network is a set up clear to everyone how many you understand  the setup  now again when i am doing the last step which is known as fine tuning i have to back  propagate over the entire network because i am saying i will adjust all the weights but  suddenly this works as compared to starting from scratch you see the problem and you  see why this is important then because this has now given you a way of training deep  neural network i still not told you why it works  we will delve into it but not really give any concrete answers because concrete answers  do not exist but we will at least try to get some intuitions behind why it works so you  get the setup now that this is what was happening till one986 to two thousand and six and now with this  idea suddenly deep neural networks were being able to train well  so in effect what we have done is we have initialized the weights of the network using  the unsupervised objective right so now initial starting with random weights we have  some weights which cater to the unsupervised objective that we had and the  unsupervised objective was us layer wise reconstruction so that is what has happened in  plain english is that fine everyone gets that    now the question is why does this work better and i give you two options and i want to  think about both these options ok is it because of better optimization or is it because of  better generalization no that is not an option but i of course we will relate it to that but  given these two i want you to think whether there is any difference between these two  statements or not that is the first thing i want you to see how many if you get the  difference between these two statements not many why is it so what is optimization deal  with dash data or dash data  student refer time onezerothree5  the answer you can give dash right dash one data or dash two data what is optimization deal  with  student training data  training data optimization remains on training data what does generalization depend on  student it as zero  it as zero so you get the difference between these two questions fine so let us try to answer  this again here right this is two thousand and six to twozerozero9 period that i am going to talk about there are  some answers and just bear with me i will give you those answers some of them will not  look very convincing but what happened after that or as a result of these investigations  that is more important right whether these answers make sense or not they will make  sense to an extent i am not saying that we will just be bluffing  but it will not be very convincing because there is no theory behind it right so what is  convincing if i give you a proof that this less this is equal to that right then if we give  you a proof and everything you do not have any other questions that is not what i am  going to give you i am going to give you some intuitions because that is all these  existing works from two thousand and six to twozerozero9 had and then i will make a commentary on that which  will lead us to some other things so just bear with me for a few minutes right  student refer time oneonefour6  that is the optimization problem if that was the case the i will just come to that that is  what i want to talk about ok so it is so these are the two questions that we are dealing  with right and the answer is depends so we will see what it is    so let us first examine the case when it is because of better optimization    so let us first understand what is the meaning of this question when i ask is it because  of better optimization then the question that i am asking you is that the first set up where  i was trying to train everything from scratch compared to the second set up where i had  this unsupervised pre training is it that the optimization problem becomes easier in the  second set up now if the optimization problem becomes easier what do i actually mean  by that that i was able to drive the dash to dash  student loss to zero  loss to zero right so is it that this is the optimization problem that we were interested in  so is it the case that in the absence of unsupervised pre training we are not able to drive  the loss to zero for the training data and hence poor optimization right that if you do not do  this unsupervised pre training even for the training data we cannot drive at loss to zero that  means our optimization problem itself is not working properly right i mean the problem  is fine  but the solution is not good you get that do you understand what is the subtle meaning  of this how many if you get this so let us see this in more detail right    so the error surface of the supervised objective of a deep neural network is highly non  convex it looks something like this or even nastier than this and in particular it has  many hills and plateaus and valleys we saw this even in the toy examples that we were  dealing with right and given the large capacity of deep neural networks it is still easy to  land in one of these zero error regions on what basis am i making the statement which  theorem  student refer time onethirty-threetwo  universal approximation theorem that is what the universal approximation theorem told  us in fact there is a study the paper which has been cited it showed that if the last year  has a very large capacity then you can drive the loss to zero even without pre training do  you get the meaning of this what does is mean so i have the input i have a series of  hidden layers what do i mean by the last layer has a lot of capacity what do i mean by  that it has a lot of dash  student parameters  parameters now how do i create these parameters i will just grow the size of the last  hidden layer right and using that then i will predict this one y  so so that is how i could increase so that is exactly what they did they took a very deep  neural network and made sure that the last layer was given a very high capacity and then  they showered that even if you do not do an unsupervised pre training you can still drive  the training loss to zero right  so this was hinting that maybe this is not an optimization problem this is something it is  still not very conclusion but we will just go with these studies we will just all i am  saying is that do not shoot the messenger this is what the study says i am just relaying it  back to you right and they will have questions on these which will try to address but if  the capacity of the network is small then the unsupervised pre training helps  so if you do not have these large capacity networks but you have very deep networks  in that case unsupervised pre training helps and this is all empirical observation right  there is no proof which says that given a capacity k with so much error bound i can  guarantee that the loss would be epsilon within the zero loss and so on it nothing like that  that is what it should have been ideally the case in which case life is much easier for me  but that is not the case this is just an empirical study as are most of the studies done in  the period of two thousand and six to twozerozero9    so that tells us something about what optimization means and whether this was an  optimization problem or not    so let us look at the other question is it because of better regularization so what does  regularization do or you gave the exact answer it constrains the weights to lie between lie  in some regions so it does not allow the weights a lot of freedom right and so you  know what l one regulation does it constrains the weights to this box and l two  regularization constrains us to this circle why no why this i know this but why  student refer time one6zeroone  in why the circle i am pretty sure most of you do not know what you are saying but you  are saying the right answers but anyways i will test this in the quiz so i have given you  another quiz question on camera so yeah so a prevents a loss from taking large values    so indeed pre training also constrains to the way to lie in certain regions of the  parameter space why am i making this statement what is the meaning of the statement  so i told you that what regularization does and from there i am making this jump and  saying that even with pre training the same thing happens that your weights are actually  constrained to certain regions of the parameter space why am i making this statement   and what are these regions that the weight is constrained to think l theta think omega  theta any regulation is of the form l theta plus omega theta  let us see so it constrains the way to lie in regions where the characteristic of the data  are captured well that is what unsupervised pre training does it is trying to train the  network in a way that each layer actually captures the important characteristics of the  data and this is based on our understanding and belief in auto encoders so you could  actually think of this that the unsupervised objective that you had for all these layers that  was actually omega theta you are first trying to optimize omega theta  so in a normal regulation problem you put l theta and omega theta together and then you  try to balance them but here you have done it slightly differently you first gave it omega  theta which is the lost of reconstruction and you asked it to minimize this loss across for  every layer  student refer time one7four8  no is this fine tuning so now what that means is that see remember that this is a very  high dimensional region where you initialize makes a lot of difference so with this  unsupervised pre training you are at least ending up in reason so you could think of it  as a constraint that ok move wherever you want to but start from here which  automatically means that i have i mean i have how to it is some other regions in that  parameter space you get that  student refer time one8one6  as you typically that would be one thing and it would also mean that you are starting  from there so with this early stopping and other criteria you will not be able to grow  much out from here right so just if that makes sense geometrically from here you  would not be able to move all the way there you get that everyone gets this question and  the answer  so you see what the answer per is object was and you also see the difference between a  normal regularization and this regularization in the normal case you had l theta plus  omega theta put together and then you are trying to minimize the sum of these two it was a  joint optimization here you have first done omega theta ensured that the weights that you  learn minimize this objective and now you add in the supervise objective which is l  theta right  so now this makes sure that your network cannot be too greedy with respect to l theta  because it has been constrained that has to first honor the omega theta because that is  where you started and now from there on it has to decide how to do l theta does that  make sense you see how this is acting as a regularizer is that ok and that links back to  your weight initialization thing right fine    so some other experiments have also shown that pre training is more robust to random  initializations  now what do i mean that mean by that so in these two graphs that you see here so this  on the x axis you have the number of layers that you add to your deep neural network  and on the y axis you have the error that your network gives when you try different  initializations right so this box actually tells you the variance in the error  so that means i tried training a network with four layers and i tried different initializations  and the error varied in this range ok is that good or bad what would we want typically  something which looks like the plot below right where all these variances are little that  means even once you do unsupervised pre training right it is more robust to random  initializations random initializations of what  student refer time twozeroone5  the original random initializations from which point you started the unsupervised pre  training ok because once you have done the unsupervised pre training that is your  initialization everyone gets that    so these are some let us see ok so these are some empirical studies and let me just  make a comment on these  so what happened from two thousand and six to twozerozero9 is people showed that see this is possible you can  actually train a deep neural network using some of these tricks we do not have a very  clear answer for why this works and you could argue different way so this is  optimization this is regularization and so on but i do not have any theory supporting it  there is no proof for why unsupervised pre training works all of these are empirical  observations   but what it at least established was that it is possible to do this so now if it is possible  to do this let me see if there are better ways of doing this do we actually need to do  unsupervised pre training oh i think it is better regularization then why not i try better  regularization techniques and see whether that helps so that led to the evolution of  which thing that you have already seen yeah which regularization technique that you saw  in the last class  student drop out  drop out right so drop out was something specific to neural networks which was  introduced in the context of neural networks so this is because people started believing  it is possible so let us try even better ways of doing that so that is how dropout came  out right then people said maybe optimization is the problem maybe these earlier  algorithms which up till that point was which algorithm  student refer time twoonefour8  gradient refer time twoonefour9 maybe that was not good so let us try to decide and  design better optimization methods and that led to the evolution of adam adagard  rmsprop so on right so although these studies were not so theoretical in what they  were trying to prove they created this hope which then led to a lot of prolific work in  that field right so at least you get the context now right the some of these might look  oh this is one data set people did experiments on m l s but i could have taken a different  data set and showed that these results do not hold and so on you could always ask those  questions  but at least what happened is people started believing these and people started  questioning that ok unsupervised pre training is one thing what else can i do and now  what has eventually happened is today no one uses unsupervised pre training right that  method which led to the revival of this field and you would have hoped that that would  actually survive for many years that is out  now hardly anyone uses unsupervised pre training it is only used in the context of  transfer learning so what i mean by that is that if you have a model trained for one  classification say classification of images on one data set right  now you have a very small amount of data in some other domain so instead of training  a network from scratch for this domain you will just initialize it with the weights for  whatever you have trained on data set one so that is more of transfer learning rather  than unsupervised pre training so that is still very prevalent but this reliance on  unsupervised training to make sure that the network actually trains that is largely phased  out    because what has happened since two thousand and six and twozerozero9 is that we have better optimization  algorithms which are rmsprop ada grad adam even so on right many various and  even now that research area is active as i was saying just in december there was a paper  which pointed out some flaws in adam and how to improve it and so on we are better  regularization methods the most prominent among those being  student dropout  dropout so these two are things which you have already seen today we are going to talk  about better activation functions this is again something which evolved that maybe  sigmoid tanh are not good so maybe something else is needed and then better weight  initialization strategies so then people took this inference oh one way of looking at  unsupervised pre training is that it actually initializes the weights in a better way from  where on it becomes easier for me to reach convergence  so why do not i come up with better weight initialization methods itself instead of  relying on this indirect way of initializing the weights so you get this so get the whole  picture now what we have been doing in the past few lectures and how it connects to the  history and these studies which were done from the period two thousand to twozerozero9 how many if  you get the whole picture ok so that is where we are now so today we are going to  talk about better activation functions and better weight initialization methods  
task2/super_cleaned_audios/lesson48.wav,89.1838,a quick summary we have seen three different interpretations of pca and eigenvectors  played a very crucial role in that and the other thing which played a crucial role was the  covariance matrix of the original data and with these three different interpretations what  we realize is that the solution that we get or the transform data that we get projecting the  original data on to the on to a basis consisting of eigenvectors ensures that there is high  variance across the new dimensions  and we can ignore of the bottom top n sorry bottom n minus k dimensions along with  these variance is not high this also ensures that the error in reconstructing the data by  ignoring this dimensions is minimized right it is a lowest possible error and it also  ensures that the covariance between your retained dimensions is zero because we are able  to diagonalize the covariance matrix of the transformed data so that is what we had  so now if you think of it right just to connect it two things that we need later on for auto  encoder right we are trying to learn a new representation for the data right and we are  trying to also compress the data and we want this compression to be such that it is as  lossless as possible right we are going from n dimensions to k dimensions and still we  want to retain the essence of the data and do not want to lose out much of the  information in the data ok so that is essentially what pca is doing now let us see this  in practice  
task2/super_cleaned_audios/lesson74.wav,524.0262, so today we are going to talk about learning vectorial representations for words  so these are the acknowledgement slash references for where are the things that i have  referred to by preparing for this lecture so you can just go this some of these are also  available as video lectures on youtube can take a look at them also  in the first module we are going to look at one hot representations of words    so as usual we will start with this motivation or motivation motivating question why do  we need to learn representations for words vectorial representations for words when  words are there right you can write them using alphabets and characters and so on so  why do we need vectorial representations mention whatever you have seen so far in the  course i have seen anything like this let us see  so suppose you are given an input stream of words and it could be a sentence or  documented if i say documented pretty much covers almost all the text that you see  right you can always abstract everything as a document and email is also a document a  manuals are also documents and so on  and we are interested in learning some function of it and so i am given a document  and i am interested in the function y hat say y hat is equal to sentiments of the words in  the document or the sentiment of the document itself  this is imaginable this is not something that i am cooking up this is something that you  would want to do you would log on to for example if you are a movie maker you would  want to know once the movie is released people have written reviews about it what is a  sentiment is positive or negative similarly if apple has released a new product or a new  feature you would want to know what are the reviews written about this product and  what is the feature what is the sentiment coming out of it is positive or negative  now sentiment is a binary thing or it could be rated also right it could be on a scale of  one to ten also but let us consider it is binary that either people liked it or did not like it  so now i am trying to learn this function which gives me which takes as input words  but as output gives me real numbers either zero to one or on a scale or one to ten or whatever  right and this is not something that we have dealt within the course so far let how do we  take as input word so all inputs have already always been numbers right they were  either coming from rn or they are coming from zero to one raise to n or something of that  sort  we never had the situation when we have words as written so right so now how do we  deal with the situation and also i have made a case for that learning this function is a  valid thing to do you have several news cases where you will need this    so now if we employ a machine learning algorithm that some mathematical model so  we saw that we could have several such models logistic regression svm and neural  network and feed forward neural networks and so on right and at the end we are trying  to learn this function y hat is equal to f of x but in our case the x instead of have be  instead of x being numbers it turns out that x is actually a collection of words  so now how do we reconcile with the situation where we have suddenly have words  instead of numbers so the way to do that would be we need a way of converting these  words or documents into some number into some vectorial representation and once we  have this vectorial representation right so now we have r r raised to n and we know  how to deal with r raise to n given r raise to n how to predict r or even r square or rm  in general that we know right we can design neural networks or any other machine  learning algorithm should do that but how do we go from here to here that is the  question right and that is why we need to learn vectorial representations of words  this is a motivation clear to everyone okay now let us start getting with a refer  time zerothree5two how to do that right    so now we will start hearing this word corpus have you heard this word before that is  exactly what you are collecting for the word to like assignment right you are collecting a  corpus in specific languages and you have taken a very toyish corpus for the purpose  of illustration so here is a corpus it just contains four sentences right so think of it that i  have a very restricted domain i have very small set of documents and i just have these four  sentences with me this is the valid corpus  the corpus that you have constructing is probably much larger scale you are trying to  collect tenzero thousand sentences or 5zero thousand sentences or something of that order  right ah but we will take this toy example  now consider set v of all unique words across all these input streams so i just call  them input streams by input streams i mean sentences or documents or whatever right  you could take it as any sequence of words and v is set of all unique words across all  this input sentences that you have  so can you tell me what v is here what would can you tell me some elements of v of  the set v human machine interface and so on so that is why in fact this is the entire  set v which is written on the left hand side right and v is called the vocabulary of the  corpus so that means everything in the corpus comes from this vocabulary all sentences  are constructed by arranging words from this vocabulary  some of you might always i mean find this very trivial but i am just going over the  basics so that at least the terminology is clear to everyone and what we want is a  representation for every word in v so that is the title of the lecture learning vectorial  representations of words so for every word in our vocabulary whatever corpus we are  dealing with the vocabulary would change and for every word there you want to learn a  representation for that word so that is what our quest is today ok  and now one very simple way of doing this is right you tell me you want a vector and  that is all you care about here is a vector i will give you one hot representations so if a  total number of words in my vocabulary is v i just construct a vector of size v ok and i  have assigned a number to every word in my vocabulary right so i will say human is  equal to zero machine is equal to one interface is equal to two and so on  and if you ask me for a vectorial representation of that word i will just say take this the  or vector of size v and switch on the corresponding bit and anything else would be zero  hence one hot right at any given point of time only one of the elements in the vector  would be on so that is a simple one hot representation as this is a very simple recipe to  get a vectorial representation of words and for every word in your vocabulary    now what is the drawback of this v tends to be very larger right so for example  there is a standard corpus known as the penn treebank corpus which is used in various  nlp applications for various reasons and that corpus has a vocabulary of 5zero k  google of course operates at it is own scale so they have a word one t corpus which has  onethree million words so this is like all the most of the web pages that they have drawn  constructed a vocabulary from that  so now i am talking about for every word representing it by a vector of size onethree million  theory does not good work right there is too much of storage required for that and if  you look at that information in it is so redundant that is all zero except for that one bit which  is on  and the other important problem is that these representations do not capture any notion  of similarity other three words that i have shown you which do you want to have similar  representations cat and dog why because both of them are domestic animals right  both of them are mammals so there are some things that you would want at least at the  minimum that the similarity between a cat and dog is more than the similarly between cat  and truck  or alternately the distance between cat and dog is less than the distance between cat and  truck so now once i start talking about vectors i can talk about similarities like cosine  similarities or i can start talking about euclidian distance so once anything i convert it  to a vector i can start asking these questions other two questions which i am asking are  valid right what would you expect to be the euclidian distance between cat and dog as  compared to cat and truck  now what happens with the one hot representations take any two words in your corpus  any two what will be the euclidian distance what will it be square root of two right for  all the words take any two words in your corpus what will that cosine similarity mean zero  because all these vectors are orthogonal right so the cosine similarity is going to be zero  but this is that means these vectors are not really capturing any information about the  essence of the word right  so remember always we are interested even like that has been our philosophy right from  auto encodes and so right or even principle component analysis they are always  interested in learning meaningful representations which capture something fundamental  about the entity that we are trying to represent right but here something like that is  clearly not happened ok so that is not acceptable  
task2/super_cleaned_audios/lesson60.wav,486.1885,so now we will try to see that how does this true error that we see depend on the model  complexity    so using steins lemma and some trickery we can show the following what is steins  lemma so i had this deal with my students last year you do not ask me what steins  lemma is i will not ask you what steins lemma is ok so it is some lemma which tells  us that this quantity what was this quantity the last term which was troublesome rate  that covariance term which was troublesome that is this quantity this quantity is  actually equal to this quantity   so let us buy that let us all of us agree that steins lemma is correct and it tells us that  this is the case ok and you saw the quiz one paper ok fine from last year i mean ok so  now we will work with this premise and we will see what it actually tells us now when  will this quantity be high so what this is telling us i mean jokes apart let us try to  focus again that this quantity is actually equal to the summation of this quantity  now let us take one term in this summation when would dou f hat x i by dou y i be  large what does it actually tell you if i change one of these yi’s a bit when the  prediction for it is going to change by a lot do you get that how many of you get this  some of you do not get this just think about it when would this be high what does the  derivative capture if the derivative is high that means a small change in the  denominator is going to lead to a large change in the numerator   what is the denominator actually the true y that we have observed what is the  numerator that is the predicted y so what you are saying is that if there is a small  change in y i then there is going to be a large change in the prediction ok when would  this happen would this happen for simple models or complex models complex  models how many of you say complex models so this is the link to model complexity  rate and i will make a more intuitive case for this but at least some of you get this that if  your model is very complex that means it is even one of your data points changes and  the prediction of the model is going to change largely  so now relate this back to that sinusoidal model that we had and we had this complex  model every model that i was training which was strained on a different set of twenty-five  examples the model was vastly different and that is exactly what was happening when  you were changing even one data point your predictions were changing largely that  means your model was changing largely do you get that intuition so indeed a  complex model will be more sensitive to the changes in the observation whereas a  simple model will be less sensitive to it and hence we can say that the true error is  actually equal to the empirical train error plus something which relates to the model  complexity    now let us first verify that indeed a complex model is more sensitive to minor changes  in the data so this is some data that i had sampled from the same distribution and i  trained one simple model which is the green line which you see that was a linear model  and i trained one complex model which was a twenty-five degree polynomial which you see ok  now what i am going to do is i am going to take one of these points and change it a bit  and i retrain the model   what happens to the simple model it does not change much but what happens to the  complex model it is more sensitive to these observations that i have and that is exactly  the quantity that we were interested in that means a complex for a complex model  which is more sensitive that summation that we care about is going to be high that  means that difference between the true error and the estimated error is going to be high    so that is why instead of minimizing the train error we should always minimize the  train error plus some quantity which is linked to the model complexity this is the basis  for all dash methods regularization method  so now you see where this comes from so ok where omega theta would be high for  complex models and simple for simple models ok you get the intuition for this and the  rest of the lecture we will spend in taking various cases where we will actually show that  omega theta would be high and we are trying to control for omega theta this quantity  for the rest of this lecture and for the rest of this course i will assume that we all know  how to deal with   we have done enough of this we have done a lot of back propagation we have done  enough derivations of the laws with respect to the output layer and so on everything  right so all of us understand how to deal with l train theta where l train theta is this l  equal to one to m squared error loss or your log likelihood or any of these losses right so  we all know how to deal with this today we are going to focus on this other term which  brings in the regularization    so what omega theta does is actually acts as an approximation for this so what i should  have actually tried to minimize is not just l train theta but l train theta plus this other  quantity which was there in my equation you get this my true equation was that my loss  is equal to l trained theta plus this term right which we approximated using steins  lemma so i should have tried to minimize this quantity but i do not know how to  really compute this quantity   so i am going to just substitute it by omega theta and ensure that omega theta is such  that it is high for complex models and low for simple models do you get the recipe  everyone gets this how many of you understand this fine so we can show that lone  regulation l2 regularization early stopping all of these are actually special cases of this  particular formulation that we have    and remember that this is the sweet spot that we were aiming for ok and this gap is  actually this quantity because we are making a very optimistic estimation of the error  whereas there is actually this quantity which we have been ignoring and that is why we  see that the validation error is high ok so is the full picture in terms of the diagram and  all the equations that we have seen  so we should ensure using omega theta that this gap is also minimized therefore our  function should be minimized l theta plus omega theta so essentially what we are  trying to do is minimize this gap and hence the model would generalize better on the test  data is this intuition clear to everyone    why do we care about this bias variance tradeoff model complexity this is not a course  on machine learning they are highly complex models they have many parameters  many nonlinearities in fact now can you relate this back to the universal approximation  theorem what is the universal approximation theorem say give me any data i will give  you a deep neural network which will exactly over fit the data right and that is exactly  what we want to avoid that is why regularization is important in the context of deep  neural networks fine it is very easy for them to over fit the data and derive training error  equal to zero and that is why we need some regularization    so today we are going to look at different forms of regularization starting with l2  regularization some simple tricks so some of these are going to be mathematically  motivated some of these are just going to be heuristics or empirical stuff so data set  augmentation is one such empirical stuff how many of you tried data set augmentation  for the immunized assignment or the back propagation as parameter sharing and tying is  something that no i am not   please do not give me that look yeah i am not suggesting that adding noise the inputs  adding noise to the outputs early stopping ensemble methods and drop off right so  these are the things that we are going to talk about this and all of this is in the context of  regularization where you want to avoid some kind of model complexity   
task2/super_cleaned_audios/lesson61.wav,1411.2696,so let us start with ltwo regularization so i have seen this before  so all of you see that this is ltwo regularization right what does ltwo regularization does  now tell me in the context of things that we have discussed today what is this empirical  estimate of the train error ok and what is this is that fine right so everything that we  are going to write is l because of its w but fine right ok now why does this relate to  model complexity what am i doing here actually by adding this  so they are going to see a very detailed analysis of this but i just want to see first  whether you get an intuition behind this so by doing that what you are trying to do not  allow the model to become very complex right you do not want a model where your  weights can take any possible value you just want the weights to be small so you are  reducing the freedom on the model right less freedom less complex you get the intuition  at least we will see this in more detail but at least you get the intuition why we are  doing this  so we are using omega remember that we are using this omega theta as a surrogate for  model complexity so if you add something in all omega theta just make sure you  understand that this relates to model complexity ok fine and now for sgd what would i  need for gradient descent just in case you have forgotten what sgd is what do we  need nothing we have done it fl gradient of this which is a sum of the derivatives  of the two quantities of which you know one right you know this already and what is  the other guy alpha w right    so you see this ltwo regularization right one reason why it is preferred is now imagine  you have already written code for gradient descent all you need to do is change it at one  place add this to your update rule that is all you need and you can think of the vector  form of this where you have a vector of parameters you can think of the matrix form of  this variable vector matrix of parameters all you need to do is add one term to your  update rule so it can be done with very minimalistic change and this would be your  update rule now let us see geometric interpretation of this    now from here onwards some of you will start getting a bit uncomfortable with some of  the math because of these assumptions that it only works for squared eggs in a vacuum  right so you will see those kind of things i will not tell you upfront what is the  assumption i am making because that will just spoil the analysis you will just not enjoy  it as much as you would ignorance is bliss right so if you do not know what the  assumptions are you will probably enjoy it more  but for some of you will pick it up just keep it to yourself at the end i will tell you what  are the assumptions i had made ok there are some tricky assumptions that i want to  make but just live with it and just try to enjoy it while those assumptions last right ok  so now let us assume that w star is the optimal solution for l w what is l w the train  error not our regularized error just the train error  and so if w star is the optimal solution what can you take tell about the derivative with  respect to w star or derivative at w star sorry it is going to be zero from basic calculus  right so which i say minimize x square the minima is where derivative of x squared  with respect to x is equal to zero right  so now consider one point which is ok so what i actually want to consider is that let  me just see how to see this so let us see my w star ok and i want to consider some point  in the neighborhood of w star ok that is what i want do so one way of saying it is that h  is equal to w minus w star is that fine ok so that is what i am going to use in the next  few steps    so suppose i have such an h which is equal to w minus w star that means i can move  from w star to some point in its neighborhood by using h and what does taylor series  tell us this is what taylor series tells us right that the value of the function at this  neighborhood point is equal to this all of you know taylor series well now it is that  fine i do not need to really go over this right  this is approximation up to the second term second order derivative now what was h  actually w minus w star so i will just substitute that and this is what i get is that fine  what is this quantity one minus zero infinity minus infinity zero right we just did that ok so  that term will disappear what am i left with this quantity ok and i have forgotten what  is next  now again i am interested in the derivative of this ok so what will happen if i take the  derivative what would i get i am interested in computing grad l w what will the r h s  be how many of you fine with this remember this is a quadratic form right so this is  of the form x square that is i mean that is roughly how i remember it is not correct  because of the form x square so when you take the derivative one of the x is will  disappear and this quantity will remain ok so everyone gets this ok  so now what do i have is i have the formula for the gradient with respect to l w and it  is in terms of the gradient with respect to or rather the gradient at l w star that is what i  have achieved so far but what am i actually interested in the regularized loss i am  what i am still dealing with is the non regularized loss this is just the empirical  estimate of the training error that is not what i am interested in i am interested in the  regularized loss  how many of you lost at this point  h is the second order derivative oh so these are  brackets just for clarity but i see it is making it more unclear yeah actually we should  have used u and then call it u transpose h u so it is the brackets here are not indicating  function ok this is just h transpose h now let us say it i realize how bad it is so last  step what are we taking gradients with respect to is w right is it fine  so we have a so i mean do not get too confused right so up till this point we have a  formula for l w right and i am just interested in the derivative of that ok and all i have  achieved by this is that i have ok in fact i have one more step right    what is this quantity zero ok so we now know that the derivative of the loss function  with respect to w can be written as this quantity is it ok and i have just derived it step  by step there is nothing great about it anyone is can why i am doing this is not clear  that will become clear hopefully but what i am doing is clear right is that fine can i  move ahead   now what we are actually interested in is this quantity because this is the true loss that  we are going to deal with right and we just saw in the previous slide that this quantity  which is on the l h s is equal to this thing on the r h s this is what we saw on the  previous slide can i just go back to the previous slide because the derivative of this was  just alpha w now let us start with this so on the next slide let me just see if there is  anything else that i need to see here ok  so far everyone is clear what i have derived so far why is not clear but what is clear  what is being derived so far so i have said that the derivative of the loss function or the  regular is loss function can be written as this quantity ok is that fine where w star is the  optimal solution for with respect to the un regularized loss function ok and now i have  what i am interested in this solution with respect to the regularized loss function ok    now let w tilde be that solution for the regularized loss function so that means the  derivative of the loss the regularized loss function at w tilde is going to be zero nothing  great about this but i just told you on the previous slide that i can write this quantity as  this quantity that is what we derived on the previous slide ok just take my word that is  what we derived on the previous slide ok let just no confidence in me ok that is fine  now can you are you if i write it as this just rearranging some terms oh sorry  so i am just grouping all the w tilde some terms and this is a matrix is needed here  right because i need to i can only add two matrices so what i am just doing is putting  the elements across the diagonal everyone understands this everyone gets this step ok    so now i have a formula for w tilde in terms of w star ok i am going to go a bit further  and be a bit bold and compute the inverse also so now i have a exact formula for w  tilde in terms of w star so what is this actually what is this relation that i am trying to  establish suppose i know the solution with respect to the un regularized loss and now i  have added regularization what happens to the new solution  so i am telling you the new solution would be smaller weights and so on that is what ltwo  regularization tells you now you are just trying to make an interpretation for that so i  have given you a closed form solution that w tilde is actually equal to this quantity that  you see on the right hand side ok why you are doing this is still not clear but right now i  just focus on the what part of it this is just some mathematical steps that i am doing  anyone who is not comfortable with this  now notice what would happen if alpha tends to zero what would be w tilde be w star what  do you mean by alpha equal to zero no regularization right so that is just one corner case  that i want to do but that is not what we care about anything what that is stupid to do all  this and tell you that if you do not use regularization you will get the same solution but  that is not what i am going to tell you right we are interested in the case when alpha is  not equal to zero ok so let us look at that case    now i am going to assume that h is a symmetric positive semi definite matrix squared  egg in a vacuum ok so if that is the case then i can write h as this i have just done the  dash of h eigenvalue decomposition all right ok and i know that since it is a squared  symmetric matrix the eigenvalues are going to be eigenvalues are going to be orthogonal  yes eigenvalue vectors are going to be orthogonal and that is why i can write this that q  transpose is the inverse of q  now let us start with whatever we had on the previous slide and substitute what what i  am going to substitute instead of h i am going to use q lambda q transpose ok so i  am doing that so is that ok i will just go over the steps and let me know at any point if  you have a problem what i have done is i have replaced this i by this and its valid  because q q transpose is just equal to i i have just taken q and q transpose as common  right so this is a c b plus some a z b so i have taken a and b out right is that fine ok  now what is the next thing i am going to do this is of the form a b c inverse so i am  going to write it as and the inverses are neat right    this is fine what will happen to this quantity i what is this quantity q and this is what i  am left with but there is still something more i can do i guess let us see ok so i can  write this entire thing as a diagonal matrix how many of you see that it is a diagonal  matrix because lambda is a diagonal matrix i of course is a diagonal matrix i is  multiplied by a scalar which is also going to be a diagonal matrix and the whole thing is  again multiplied by some diagonal matrix ok what is the inverse of a diagonal matrix  the reciprocal of the diagonal elements  so i its fine so i have a very neat formula for what w tilde looks like in terms of w star  ok again why am i doing all this and god knows but and here d is equal to this  quantity    so what exactly is happening here in terms of linear algebra or in terms of geometric  interpretations so let me just see if i have to do something first ok so what is  happening to w star is getting  student refer time onethreeone5  rotated remember what happens when a matrix where hits a vector it gets rotated and  scaled also and then what is this diagonal matrix going to do scale it element wise  scaling actually everyone gets this operation ok and then i am again rotating it by q  again the same stupid question if alpha is equal to zero what would happen q transpose  would rotated by something and then q would rotate it back way that means you will  end up getting the same solution ok if alpha is equal to zero we understand  now if alpha is not equal to zero first let us see what does this matrix look like so what is  this matrix actually it is a diagonal matrix what are the diagonal elements the what is  the first element in the diagonal one by everyone agrees with this what is the second  element ok fine and what is the other matrix that i have lambda so d is equal to the  product of these two things right so what is d going to be what is the first element of  this matrix is going to be how many if you say lambda one by one lambda one plus alpha this  much is clear everyone gets this  so this is a diagonal matrix of the form a b c let us consider a three by three matrix now i am  going to multiply it by another matrix x y z which is also a diagonal matrix right  because this is also it so this matrix i have already told you what it looks like the other  matrix is also a diagonal matrix now what is this product actually a x b y c z and  everything else has zero now everyone gets it now can you say what would this product  look like if you can actually make out it would be a diagonal matrix and what would the  diagonal elements be       so now what is happening so first this rotation is happening that no one is denying  after rotating what is happening this is a this product is actually a vector that is fine ok  what are we doing to every element of the vector scaling it scaling it by what quantity  these quantities that every element is getting scaled by the corresponding entry in the  diagonal in this diagonal right  so the first entry is getting scaled by this the second entry is getting scaled by this and  so on ok i just want you to take some threezero seconds and try to figure out where i am  headed from here    let us see if i can yeah maybe look at this sentence and see first of all everyone agrees  with this sentence right is there anyone who does not agree with the sentence i am just  trying you to figure out the implication of the sentence you get it some people are  nodding their heads just in because if you scale it right then there is no guarantee that  what the vector has changed ok what happens in the following case that means that  dimension will be left as it is ok but if the eigen if this condition holds what would  happen that dimension is almost getting multiplied by a zero right  so see these two extremes when the eigen value is very large you will end up staying  where you were so those dimensions will not be affected if the eigen value is very small  then you are almost getting scaled down to zero so now what will happen is actually only  the significant directions larger eigen values will be retained so what is the effective  number of parameters in your model now  see remember that this w vector is a vector of all the parameters what am i telling you  that some of these are going to disappear when which condition holds the third can the  third bullet holds some of these are going to disappear that means the effective  number of parameters which remain in your model is going to be less right and you see  that it is going to be given by this quantity right  so that is sometimes known as the effective number of parameters in a neural network  if the effective number of parameters in your neural network is decreasing that means  what you are doing making the model less complex right so that is what we have  achieved you see that ok    now let me end with a pictorial interpretation of this you see two figures here and there  is only one figure but you see two different things here can you tell me what this is and  what this is that is the first question i want to ask you the hint is that in this lecture we  care about the other hint is what was w star the solution for the  student refer time one8twothree  unregulated loss which means which loss l theta you need any more hints sorry this  box is the contours of l theta this box contours of omega theta so this thing just ignore  this part of the figure for now ok this i have marked as w star w star was the solution  when i only had the un regularized loss ok there is the solution when i had the un  regularized loss ok  so remember the contour maps that we had seen so this is the minimum of that  particular function so this is the contour map for l theta that is clear now what  probably is not clear is why is this the contour map of omega theta let me just go ahead  actually    please do not read this this is the prestige ok so do not read that so this is the contour  map of omega theta right because omega theta in the what is the minima for the omega  theta it is a function of the form w square what is the minima zero and what does that  function look like and what is this point zero the origin right so that is why this is the  contour for omega theta ok  now what is happening this was the solution when you had without regularization and  now this is w tilde which is a solution with regularization so can you make some  commentary on this with respect to not just general commentary with respect to the  things that we saw in the derivation we talked about rotation scaling dimension  specific scaling so what is happening this was my original solution vector this was my  original solution vector when i did not have the regularization term now what has  happened the rotation has happened and we saw that there is a rotation operation  happening more importantly what has happened scaling has happened  more importantly what has happened dimension specific scaling is happening right one  dimension has not this dimension has scaled down this dimension has not scaled down  enough that is exactly what we wanted right we wanted the less important weights to  go down and the more important weights to stay there we did not want a uniform  scaling down we wanted a dimension specific scaling down  so the weight vector has been rotated yes each dimension after rotation has been scaled  some dimensions have been scaled down more the other dimensions have been scaled  down less how many of you can make this interpretation from the figure now that i  have told you this interpretation    now still if you do not how mean if you can still have a doubt with this you still have a  doubt what is doubt fine so so this was the original solution vector right the map told  us that what actually happens is when you add this omega theta the solution vector gets  rotated ok at the same time there is also some scaling down and that scaling down is for  dimension  how many dimensions do you have here two dimensions right so this is one  dimension this is the other dimension now in the original case both these weights  actually seemed almost equal right i mean if you look at the w one coordinate and the w two  coordinate they were same now after this regularization what has happened is what are  the new coordinates for w one and w two this is the coordinate for w one right this is the value  of w one and this is the value for w two  both of them are admittedly smaller than the original values for w one and w two in the  absence of regularization or both of them equally smaller no they are being scaled  differently one rate has been scaled down more the other weight has been scaled down  lesser right and that is exactly what the math was telling us that they get scaled in  proportion to those lambda one by lambda one plus alpha and that is exactly what we see in  the figure is that fine  how many if you get this interpretation now is that ok so all of its elements are shrink  oh you have a question so this final resultant right it is so what would have happened  is that there would have been first rotation then scaling down and then again rotation so  what you are seeing here is the final rotation right so it is not it should have been  showed in three steps by just shown the final step  so its question was that we first had a rotation then had a scaling and then again a  rotation but i even as explained in the figure i spoke only about one rotation so i  basically clubbed both the rotations and so what you see finally is rotations scaling  down and again rotation  
task2/super_cleaned_audios/lesson49.wav,689.8244,just like to give you a flavor of why all this is important right why do we need to throw  away some dimensions and then how does it practically help  so consider that we are given a large number of human images right so this is like  some faces data set a database that says one of the intelligent agency someone is  maintaining one of the government agency or may be aadhar data bases or something  like that ok now each image here is one hundred cross one hundred that means it is ten k dimensions  right it is a very high dimensional data ok and your job is to actually store this on to do  some database for a large amount of the population right because you are collecting these  images from various people  so now we would like to represent and store this data using much fewer dimensions  right and you would be really ambitious that if you want to store that more than fifty to  two hundred dimensions right so you see the compression that i am looking at you have ten k  which is a big storage problem for me and i want to just bringing out to fifty to two hundred but i  have know that this is crucial data right i do not want to store information which is not  able to distinguish these faces i was still be able to reconstruct the faces from this  information right do well i mean minimum error reconstruction from this and that is  exactly what pcas are allowing us to do right so now we construct a matrix of m cross  ten k what is m  student refer time one hundred and forty-six  the numbers of samples you have the numbers of data points that we have and each of  this is of dimensions ten k ok so this is what matrix what do we call this matrix oh it  is already given right it is the x matrix the data matrix that we always have now each  row of the matrix corresponds to one image and each image is represented using ten k  dimensions just to reiterate  now let us see so now what would you do this is the original data i want a  dimensionally reduced data right you want store this ah is the mike working you  want this data to be represented by a fewer dimensions so what is your solution do  two hundred and forty-three    so we retain the top one hundred dimensions corresponding to the top one hundred eigenvectors of x  transpose x right so basically we do a pca find the one hundred find all the eigenvectors of x  transpose x and then just retain the top one hundred of those now what is the dimension of each  of these eigenvectors should be straight forward take your time it is early morning  student refer time 03ten  ten k right so now can you think of a physical interpretation of this so what are you  trying to do you are trying to store faces and now you have come up with these  dimensions no sorry we have come up with these basis vector which is eigen vectors and  each of them is also ten k which is as same as dimensions of your faces  can you think of a physical interpretation of what is happening here none of went you  through the slide except perhaps you or i do not know just think about it so what you  are trying to do is you are trying to represent any possible face in your database right  using a linear combination of some vectors ok now these vector should have some  interpretations right it should be connected to faces and somewhere otherwise how will  you construct a face from taking a linear combination or some random vectors do you  get the point  so can you think of each of these ten k dimensional vectors which is the same as the  dimension of your original data as a face and try to plot it can you try to do that at least  it make sense ten k dimensional ok that is the same what you are image size was i could  just arrange these ten k dimension as one hundred cross one hundred and try to plot it ok so let us see if  you do that what happens ok we convert each eigen vector into one hundred cross one hundred matrix  and treat it as an image and let us see what we get  this is what we get so this is the top sixteen eigen vectors that i have plotted now can you  tell me a physical interpretation of this this is the basis for constructing any face in  your data base right that what you are trying to say all the faces that you have in your  database or in the world you can combine them by looking at the these elementary face  structures right which are your basis  and then you could scale them up by using these alphas you will be multiply them with  the certain alpha right and when you combine them you will get the base any face that  you had in your original database does a physical interpretation make sense how many  of you get this     so that is what is happening here so we have constructed this basis now i will come to  that later so these images are actually called eigen faces and the form of basis for  representing any face in our database ok in other words we can now represent a given  image as a linear combination of these eigen faces so this is my original image ok i  want to reconstruct it now use sixteen or twenty-five of these eigen faces what do you think would  happen we will get some face which has some error there is some error in  reconstructing this face  so let us see what we get so i am using only one basis vector and i found out this alpha  one i how would i found it out  student refer time six hundred and eleven  dot product of the face vector with the basis vector ok now i instead of one i take two you  see i have took this two basis vectors scaled them with the corresponding alpha  coordinates and added it them up right and i am trying to get some face value it does  not look it goes to the original face that means the dash is very high the reconstruction  error is very high  that means i have still dropped some of the important dimensions i have still drops  some of the important eigen vectors right so the value of k which is the top k eigen  vectors is something that i need to take care of it and should be in a way i can always  construct the reconstruct i can always compute the reconstruction error here right how  will you compute the reconstruction error  student refer time seven hundred  take the square error between the original image and this second image that you see here  right so you will take the square error between this and this and you will end up with  the number which is not acceptable right now what i will do is i will go further i have  taken four ok still not quite there but i can see a shape emerging right  i go to eight things becomes better since you are already know what the original face at least  you can make sense of it and by the time i reach sixteen i am almost there right at least i  can recognize the face that is probably losing out some subtle things in the face but i can  recognize it  now how many of you appreciate what is happening here yeah of course now what is  happening here so think in terms of a practical application right what have you done  what have you able to be achieve how may basis vectors where you able to store or did  you store  student sixteen  sixteen that means sixteen into ten k values ok and suppose you had a million images in your  database how many would you require to store  student refer time 0eightoneone  wait let us we do it step wise forget about pca if you had a million image in your data  base and each of them have ten k dimensions how much storage do we need  student refer time 0eighttwofour  million into ten k floating point values ok now with if i say sixteen sixteen may be too  ambitious may be later on i will say fifty or one hundred is ok but let us say sixty then how much  data we need to store  student sixteen into ten k  sixteen into ten k and you can reconstruct any face  student refer time 0eightfour9  yeah alpha’s needs to be stored right so for every image instead of storing ten k  dimensions you will just store the sixteen alphas right and you can see that even if i go  there to one hundred and its still manageable instead of ten k i am going to just store one hundred alphas  right and as i go to one hundred what would happen  student refer time nine hundred and nine  the reconstruction error would become even lesser ok so is that is the intuition clear  ok so this eigen vector storage is a onetime storage we are going to store this k eigen  vectors each of them are ten k dimensional and k is one hundred or two hundred we do not really care  because the original data was very large right and for each image we just need to store  these alpha values k of them right so for each of them instead of ten k we will store  one hundred to two hundred alpha values and of course it is significantly reduces right  so this is why we need to do all this right and what is the other advantage of doing this  anything of something else so what is pca actually allowing you to do if you again  think of it not i mean subtract the math out just think it in terms of physically  interpretation and what is that it is allowing you to do if you had to say it in english  what is it allowing you to do compression is a loaded word can you just spell it out for  me what is this compression mean actually  student refer time tenonefour  right so it is storing all the relevant information in the image and discarding all the  error element information right now this also ensures that if you have multiple images  of the person then what would happen you should take the image under lighting  conditions or may be at person had applied some makeup or something like that right  what would happen  student refer time ten3eight  in the original space the ten k dimensional space what would happen to these images  they will be very far from each other right because the lighting conditions have changed  you see a dark person so have a light person something like that right and now because  pca has helped you to throw away this dimensions right may be the exact terminology  which i am using may be the lighting condition do not do it because you can imagine  that there would something right that suppose as some element which is calling the  image to look slightly different but that is not the important information right so that  would get discarded off and only the relevant information would stay  so then multiple images of the same person which were dash in the original space would  come dash in the new space  student refer time oneonetwotwo  far in the original space would come closer in the new space right so this is what  compression helps you to do so this is what you want to learn you want to learn the  important characteristics of your original data and that is what pca allows you to do  fine  
task2/super_cleaned_audios/lesson88.wav,975.5866,ok so now we will go to the next module this is for the camera and this is on  convolutional neural networks  so far we have done what we have just talked about a convolution operation you just  taken some input boxes and converted them to output boxes what does this anything of  this have to do with neural networks i keep saying that is a course on neural networks  right so everything has to link to that so what is the connection  so we will try to understand this by taking the example of image classification and i will  use the same trick to get everyone’s attention so the next few slides are going to tell  you the difference between machine learning and deep learning ok so now everyone  will pay attention     so this is the task you have give given an image and you want to classify it into one of k  categories and i am considering four categories here car bus monument flower ok what  is the simplest thing that you can do suppose this is a twenty cross twenty image you know the  simplest thing   student sir  given on the slide you will just take this as a four00 dimensional input feature vector right  and you will treat it as a four class classification problem train some multiclass svm or  anything on that right so you have a simple input so you are given some one million  images all of these are four00 dimensional and they come from one two three or four these are the four  classes which is car bus monument and so on     so you can just treat this as an input feature vector and do your classification right that  is the simplest thing that you do or else what you could do is you could do some kind of  feature engineering right you could say that actually this entire blue sky is not really  helping me in deciding anything these entire green lawns and all this is not helping if  monument car bus and flower are the classes what i care about is the shapes i do not  care about the details inside the shapes i am not trying to decide whether the car is of a  blue color or what model the car is and so on right all i want to see is that this particular  shape of a car is present or not  now what kind of filter gives you the shape of the image  student refer time 0twoonesix  edge detector right so i could use edge detector so now this is something that i have  used based on my domain knowledge that for these four classes actually just detecting the  shape is important so i will ignore everything else so there is a lot of details there  right so i have actually sparcified my entire input i have just looking at the edges in  the input and now this is a better refined feature as compared to the earlier feature how  many of you agree that this is a more refined feature representation right   but this was handcrafted i actually hand coded the edge detector kernel because i knew  that it is eight at the center and minus one everywhere else right that is how i thought of it that  that is what an edge detector is or at least i read about it somewhere right so that is how  you would do it so this is feature engineering   so what is this this is how you do machine learning right you take an input you do  some feature engineering and then you train a classifier on top of that but now you could  become even more creative with the feature engineering and that is what the computer  vision community was doing largely before twentyonetwo come up with different ways of  capturing better and better features from images so too popular in features from that era  and that is i am just talking about twentyonetwo not some like five hundred years back but from that era  was sift and hog features which actually tell you how do the gradients of these pixels  change across the image right   so this is again try to capture something like how what is the variation in the image  from pixel to pixel right so that is the essence that how is you do not care about these  entire blue patterns because they are just telling you sky it is redundant right if you have  seen some one0 pixels or twenty pixels which has sky you know that a large part of it is going  to be sky  so these try to capture some abstractions from the image and these are better than the  edge detectors and these features were extremely popular so what you would do is you  take your original input this is a deterministic algorithm you apply the hog algorithm  or the sift algorithm and it gives you a transformed representation for the image and  you can use this transform representation to do classification and a lot of work prior to  twentyonetwo twentyoneone show that these features work extremely well across a wide variety of across  a wide variety of image tasks ok   so again what was happening here this was feature engineering because someone  realized that what i care about is this gradation in the input images and i can capture this  by this nice algorithm called sift or hog of course someone came up with that  algorithm but it is still kind of feature engineering right   so this is how the learning is to happen is you are given some input you do a static  feature extraction no learning so feature extraction is deterministic you take the input  pass it through one of these algorithms either the edge detector or the blur detector or  sift or hog and you get some representations for the input and the only learning that  happens is on top of this transformed input so you now have a transformed input and  on top of that you are going to train a classifier and you are going to learn the weights of  the classifier so the only thing that you learn is the weights of the classifier    so that is equivalent to learning only the soft max layer in case of a feed forward  network that is the output layer right  now instead of using these so this is the question instead of using these handcrafted  kernels or features such as edge detectors or blur detectors or what not can we learn  meaningful kernels in addition to learning the weights of the classifier do you get this  question at least whether the answer or not but you get the question so what i am  asking is that why should i hand code this edge detector ok    why not have after what is the edge detector it is like a three cross three matrix right and i have  seen tons of such matrices in my feed forward neural networks i have dealt with very  large matrices which were called parameters of the network   so why not have a three cross three or a five cross five or whatever dimensional matrix and try to  learn what should be the right values in this matrix instead of hand coding the edge  detector matrix do you get the idea how to do that as still far but at least do you get  the idea that is what i am we are trying to do ok  so now instead of just learning the weights of the classifier we also want to learn the  weights of the kernels that means instead of using handcrafted features i am now going  to  student refer time 0sixthreeseven  learn the features so that is the difference between deep learning and machine  learning so you had handcrafted features there and now you are going to learn the  representations also by treating them as additional parameters in your networkhow you  will do that we will see and it is very simple given that you understand how to train  feed forward networks    but then why the stop there why just have one feature representation for the input can  i learn multiple such kernels right i could have one three cross three matrix which learned this  first representation another three cross matrix which learned this another representation and  yet another three  three or five  five or seven  seven matrix which learns this different representation so  instead of learning one static representation from the input i could learn multiple  representations from the input    in fact why not why just stop there what is the next thing that i am going to try to do  multiple layers of features right so that means at the first layer i learned this  representation now i could take this and try to learn an even more abstract representation   and then keep doing this to make it deeper and deeper do you get this ok  so at every stage now i have these parameters which are helping me learn the  representation of the input i am learning multiple representations at every layer and then  having multiple layers of these representations right and everything is learnable end to  end ok so you get the difference between deep learning machine learning now there is  no handcrafting of features you are learning the feature representation i know i  understand there is some confusion about how you would do this    but we will get to that just trust me on that that you will be able to figure out how to do  this ok  and all of this we have some weight matrices here we have some weight matrices here  these are the layer one weight matrices the other layer two weight matrices and these are the  output layer matrices and you see this layer wise arrangement of these weight matrices  and they are very comfortable with this because we have done feed forward neural  networks where we had these multiple layers and we knew how to back propagate from  the last layer to the first layer   now what i am trying to say is that i would like to adjust these weights of filters in such  a way that my classification loss is minimized and what is the loss function that i am  going to use here  student refer time 0eightfiveone  cross entropy because this is a multi class classification problem ok     so just hang on with this intuition and we will make it more clear fine  so such a network which has these multiple convolution learned convolution operations  at every layer and then multiple such layers is known as convolutional neural network     ok fine so get this idea that we need to learn kernel filters by just treating them as  parameters of the classification model ok but how is this different from a regular feed  forward neural network you could have taken a regular feed forward neural network  and i will show it to you on the next slide and what is the difference between that and a  convolution operation    so if you understand that then you would be done for this lecture yeah  so we have an input so let us say now i will take back the eminist case where you are  given an input as an image and these are digit inputs and you want to classify them into one  of one0 inputs and i am going to assume that my input is four cross four that means i have onesix  pixels ok   so the simplest thing that i could have done or the feed forward neural network way of  doing this is that i would just flatten out this image i will get onesix inputs i need one0  outputs at the output layer so i have an output layer which will have one of these one0  classes and then i add as many layers that i want in between ok this is what a feed  forward neural network would look like and if i consider any one neuron in the first  layer it takes inputs from all the onesix inputs right that is how a feed forward neural  network is you have these extremely dense connections where every output depends on  every input at every layer ok     now so this is the same story which i have said now let us look at what a  convolutional neural network looks like so again you have these onesix inputs i am using a  two cross two convolution ok now if i use a two cross two convolution if i place it here then i  am using pixels one two five and six and computing one value so you see the difference  between this and a feed forward neural network in a feed forward neural network honeone  would have depended on  student refer time oneone0three  onesix values onesix inputs and a convolutional neural network it is depending on  student refer time oneone09  only four only four neighbors ok and similarly honetwo i am using a stride of two by the way right  so i am not placing the filter here i am just skipping one step honetwo would depend on  pixels three four seven eight  ok and so on right  so one thing is clear that as opposed to a feed forward neural network you have sparser  connections here is that clear and why do we have sparse connections because we are  exploiting our knowledge about images that in an image you do not really care about the  interactions between on between a pixel at the leftmost left top most corner and the right  bottom corner right so there is sky here there is ocean here or there are trees here you  would want to capture the neighborhood around that pixel not really capture it with the  entire image that is why you do not want all of these onesix inputs to contribute you only  want a small neighborhood to contribute do you get that intuition ok     so this is the first property of a convolutional neural network that it has sparse  connectivity ok but its sparse connectivity really good i just made a case for that now  i am going to counter argue right is it really good that you have these sparse  connections because you are losing out information right you are using out interactions  between certain pixels so why can why is that good  student refer time onetwotwo9  i am hearing a lot of interesting answers but remember that you are always going to  have multiple layers ok so consider these two pixels in the first layer these two pixels did  not interact because htwo only dependent on these three and hfour only dependent on these three  there is no a there is no unit here which depends on both xone and xfive is that obvious  because i am just using a window of size three   but now once i go to the next layer once i go to gthree gthree depends on htwo hthree hfour here which  in turn depends on xone xtwo xthree xfour xfive right so even though at this layer xone and xfive are not  interacting with each other as you go deeper these interactions become obvious do you  get that right so that is why you will always use a deep convolutional neural network  where all the pixels get to interact at a deeper layer but at the more image it layers you  just want to capture the interactions between a neighborhood   so it is like you take this neighborhood find out something then take neighborhoods of  neighborhoods and then try to find out something at the next layer and keep continuing  in this layer how many of you get this right ok so this is what sparse connectivity  looks like      another characteristic of cnns is something known as weight sharing so let us see  what it is so remember i had considered this two cross two kernel and i was placing it at  these four pixels which is pixels one two five and six and i was pacing another kernel at these four  pixels which is pixels oneone onetwo onefive and onesix right these four pixels and i have used different  colors for them indicating that these filters are different so they are both two cross two  filters but i am assuming at the values inside them are different does this have to be the  case just think what a filter is trying to do  student refer time onefourtwoseven  it is striding across the entire image at every location i want to do the same operation  remember when we are doing blurring or edge detection or sharpening i had the same  filter which i was applying at every location so i want to see what is the effect of this  filter throughout the image so i do not really want to change this filter that means  these four weights would be the same as  student pink weights  the pink weights how many of you get this so this is a question do we want the  kernel weights to be different for different portions of the image so imagine that we are  trying to learn a kernel that detects edges so the same kernel configuration is required  throughout the image because that is the kernel configuration which will detect an edge    so you want the same kernel to be there everywhere so we are going to share these  weights they should not be these pink and orange weights we will just have the same  weights everywhere ok so this is known as weight sharing   so now this is something ridiculous if you think about it now how many weights do i  have in layer one  student refer time onefivethreethree  four weights that is all that looks too less right it would lead to  student refer time onefivethree9  dash fitting  student refer time onefivefourtwo  under fitting because we have very few parameters so how do i deal with the situation  student refer time onefivefourfive  you will have multiple kernels right so you will have another kernel which takes  something else you will have one more kernel which takes something else and you can  have as many such kernels right so the more the number of kernels will have you will  have that many into four as the number of parameters and that many outputs at layer one  how many if you get this ok good   so these are the two important characteristics of convolutional neural networks one is  sparse connections and the other is weight sharing ok     so so far we have focused only on the convolution operation now let us see what does  a full convolutional neural network look like or maybe i will do this next time i think  this is  
task2/super_cleaned_audios/lesson63.wav,49.4346,the next thing that i would like to talk about and this quickly go over this parameter  sharing and tying      so parameter sharing and tying i will just quickly go on this because for the sake of  completeness it is there in this lecture but it should it would really make sense when i  do convolutional neural networks so for the time being just take my word for it that in  convolutional neural networks you do a lot of parameter sharing where as the other  place that you have seen parameter tying so that is again something that i am not going  to talk about  so this is typically used in auto encoders where the encoder and decoder weights are  shared and that effectively reduces the number of parameters in the model which  effectively reduces the complexity on the model if the complexity of the model goes  down omega theta goes down because that is what which wise man told us that time  steins lemma  
task2/super_cleaned_audios/lesson77.wav,59.3041,so we will start from where we left yesterday ah so we did this whole story on starting from co occurrence matrices we learnt how to get better word representations and the key thing there was we used svd as a dimensionality comp reduction tool and we came up with this neat result that you could use w word as the representation of the mate of the words it has m rows and k columns where k is very less than the size of the vocabulary so you have achieved lot of compression and you are still able to learn very meaningful representations which you could use for several downstream tasks what to use these for and how to use these for you will see that later maybe four lectures from now i mean i say four lectures i mean four two hour lectures right so it might be more in terms of actual lectures so we will get to that but for now we have a way of learning representations for words 
task2/super_cleaned_audios/lesson76.wav,839.3016,so in this module we will talk about using svd for learning word representations so what does singular value decomposition do yeah these are all possible variants so people have tried various things and one of the ppmi one is the is the most reliable thing that is what is given but you can think of i mean you said one there are ten different things which we can do for the cooccurrence matrix right but this is the most popular and most stable thing to do yeah what is the single value decomposition do can you read it from the slide please it gives the rank k approximation of the matrix so let me start defining a few things so from now on when i refer to the cooccurrence matrix i would mean the xppmi matrix right which was the positive pmi which was replacing all negative pmis by zero and just do not have this nasty variable i will just call it as x so from now on whenever i say x i mean the positive pmi cooccurrence matrix ok so that is what this matrix is ok and we know that svd gives us this reconstruction of the original matrix and fine it gives us the best rank k approximation of the original matrix and it discovers the latent semantics in the corpus everyone remembers this like that is what we were by we were using pc and svd and auto encoders it was able to discover some latent semantics and we will concretize this intuition with the help of our current example but for now i just want you to recall that it helps in discovering the latent semantics now notice that this product and i think i have done this in one of the assignments or something can be written as a sum of the following products so i can write it as sigma one u one v one transpose sigma two u two v two transpose and so on can you tell me what this sum is this is the rank two approximation of the original matrix and i keep taking more terms i get more and more rank approximations of the original matrix ok now and we all know that ok we all hopefully know that what is the dimension of this it is a scalar vector matrix scalar vector matrix studentmatrix ok now of course you will say matrix but what is the dimension of the matrix why is it a matrix it is an outer product of two vectors right this what is the size of this n cross one into n cross one so that sorry one cross n that gives you n cross n matrix everyone gets this otherwise how is it a rank one approximation you have to get the original dimensions right everyone is clear with this is an outer product and it belongs to r m cross n ok and if we truncate the sum at the first term we get the rank one approximation and by svd theorem we know that this is the best rank one approximation now what does this actually mean that this is an approximation what do we mean by that so we will see that on the next slide and similarly in the same way if we truncate it in the second term you get the same best rank two approximation now what do we mean by approximation here actually and i mean to say approximation always in this course at least try to think in terms of compression how many elements are there in the original matrix m cross n that is how many elements you need to describe the matrix completely if you do a rank one approximation how many elements are you using m plus n plus one right so the original matrix has m cross n entries entries and when you do a rank one approximation you have m plus n plus one entry so that that is the approximation right so you are trying to really compress the original data using only these many variables you get that ok and if we do a rank two twice this right so as many rank i mean as deeper as you go in the sum you will have that many elements to do the approximation ok but what is important is that the svd theorem tells us that this is not just any random approximation but this is the best approximation that you could have done that means if you wanted to use only these many elements these are the best elements to use right everyone gets that so as an analogy consider this right suppose you are given eight bits to represent colors ok and this is how you represent very light green light green dark green and very dark green this is what your representation is in this original eightbit representation there is some similarity between the colors but it is still a bit latent but now if i were to ask you to use only four bits to represent these colors what would you do the lowest significant bits if you use the first four no then use only get very light that is not the essence of that color right you need the color to be there so if you compress what would happen is so that is what happens in when you go from two5six bit colors to higher or lower right the distinctions between the colors go off so all of them would be compressed to green well that is the most important important information in terms of the color right because you need to be able to distinguish between green and red as suppose to very dark and very light that is the more important information that is there right so when you compress it the most important information in that entity should be retained and that is exactly what svd does when it does a compression it retains the most important information in the corresponding entries is that clear is the intuition clear fine so let us actually do this so this is my original cooccurrence matrix x and i just repeat when i say x i mean x ppmi and now i have done svd and i have done a low rank approximation of it i do not know what was the value of k i selected but some value of k it was definitely greater than one or two so now you see a low rank approximation of x what is the first obvious thing that you notice it is dense now it is the longest sparse now can you tell me something about the colored entries what was happening in the original matrix x the word system and machine was never cooccurring because of which their value was zero same for human and user but remember there is some important information in this matrix which also tells you what are the words with user appears with and what are the words with human appears with and that actually gives you intuition that these two words are actually related right same for system and machine system and machine both would appear in the context of words like interface install run and so on so you know they are similar it just happens that these two words never appeared together so this similarity between them was latent or hidden in the original cooccurrence matrix now once i have done the svd what has happened because i have forced it to compress the data it has retained the most important in information and under that information these two words have actually come closer to each other right so you see that now you have a nonzero entry for the similarity between those two word pairs do you get the intuition and can you imagine that this would happen with svd and what is wrong in imagining you can but i guess right that is what is happening with this so you think about pca you think about svd you think about auto encoders all the intuitions that we had build there the same is being applied here right all if you get this ok fine yeah after svd you could have right that is not necessary that it should be positive in the original matrix you do not have negative entries now here is a question right recall that earlier each row of the original matrix x served as the representation of a word ok this was my original x ppmi not the rank approximation now in that case what would x x transpose give me what would the ij th entry of x x transpose be so let us look at this toy example you have this x matrix you have xi and xj now i take x transpose ok now this is xi this is xj just standing now what would be the ij th entry of x x transpose it will just be the dot product between these two right is that fine so this is just the dot product between them and we know that dot product is more or less the same as cosine similarity module over the normalization right you just need to normalize it by the norms of x and xi and xj in this case right so i will just assume that this is a substitute for the cosine similarity ok so every entry at every ij’th cell in x x transpose is the cosine similarity between the representations of the i’th word and the g’th word is that clear to everyone ok fine and in the original case which was the xppmi the cosine similarity between human and user was zerotwoone now once we do in svd what is a good choice for the representation of the word i after svd what is the dimension of x hat it is again n cross m because it is a sum of m cross n matrix so that the dimension of x hat is m cross n although it has been constructed using fewer information but the dimension is m cross n right that means what is the size of the representation of every word still high dimensional still the same n or v whatever everyone gets that is there any confusion with that ok now you could say that ok i will just take the i’th row of the reconstructed matrix and use that as the representation because i know that now this representation is better some of those zero entries have changed they have captured the latent semantics between the words so this is definitely better none is denying that that this compression has given us better representation because we are only keeping the most important information now if i do x hat x hat transpose remember x hat is the reconstructed matrix then again by the same argument the ij’th cell actually gives me the cosine similarity between the i th word and the j th word and you can see that now the cosine similarity between human and user has actually increased so this is just for me to convince you that we have learned more meaningful representations so now what do we choose as the representation i have still while computing this cosine similarity i have still used xi which is high dimensional which has the entire vocabulary as the number of columns as a representation right so there are two things coming out of here one is i really like this cosine similarity i see that it has improved that means the representations were computing something meaningful but on the flip side i am still not happy because the representations are still high dimensional so can you construct a wish list for me based on this i would want the same cosine similarity to be present as given by x hat x hat transpose right but i would like to represent it by fewer dimensions that is exactly what my wish list is ok so let us see how do we do that now for no reason i am going to construct a matrix w word equal to u sigma what is u sigma it is the part of the svd right the svd told us it was u sigma v transpose so i am just considering this matrix i am going to call it w for no particular reason ok now let me take x hat x hat transpose i can write it as this is that fine now what is the next step what does this mean i want an answer right this is that aha moment should be there or otherwise there is no point what is how many rows are there in w the same as the number of words in our vocabulary what is the dimension of each row k so now w word has low dimensional representations for the words in the vocabulary but while doing this what have we not sacrificed the cosine similarity the cosine similarity obtained by this is actually the same as this do you get that how many if you see this is very very important that if you have not understood this everything is meaningless so you see how from svd we got a low rank or a low dimensional representation for the words right w word is just to be clear k and k is very very less than v right so now we have representations for words which are much smaller they are no longer v dimensional remember in practice this k would be of the order tenzero twozerozero 3zerozero and remember your vocabulary was of the order 5zero k tenzerozero k and so on right so the huge reduction that you have got and you have still been able to learn meaningful representations which give you better similarity between related words right  so conventionally w word which is u sigma and belongs to m cross k so i am sorry for messing this up but i have used m n and v are interchangeably so you would understand it from context that m is v and the other matrix which is v is known as the w context matrix right what is the size of w context n cross k or k cross n right that means it has the representations for all the context words and w word has a representation for all the target words right so we had these words on the rows and the context words on the column so w word has the representations for the rows and w context has the representation for the corpus ok so this what we have seen so far and this is where we learn today is what a nlp was six years back right before the advent of deep learning if you wanted to use word representations this is what you would do you would do con construct a cooccurrence matrix try these tricks of pmi ppmi positive negative zero and all those things those heuristics then do a simple svd retain the most important tenzero twozerozero dimensions and treat that as word representations and use it for whatever you want to do now what needs to be seen is what happened with deep learning and how have this way of computing word representations changed over the past few years right so that is what we are going to see in the next lecture right 
task2/super_cleaned_audios/lesson62.wav,340.7906,so how with that heavy math i will just interline with this something very simple  which is something known as dataset augmentation  so what is dataset augmentation mean so you always given some training data so in  the case of mnist you had this training data where you are given these digits images  of digits and you wanted to train some classifier so in dataset augmentation what we  do is so now we have what is happening here right conceptual is that there some  seeing some training data and try to build a classifier and what you doing actually is  minimizing the empirical train error   that mean it will ensure that whatever you have seen in training is going to look it is  going to be perfectly classified whatever we have seen in training that is going to look  very good it is going to be the training error on that is the error of those training  examples it is going to be very easy  now my question is this if a training time you are seeing all this twos which are roughly  vertically drawn right and a test time you see at two which is written like this which is  slightly tilted what would happen it will not be able to do a good job on that that  means your model is not think of terms that you have used in this lecture not  generalizing  can you think of a simple trick based on your domain knowledge of how people right  digits to kind of overcome overcome this problem you get the question right i am telling  you that it is possible that someone writes to in a very tilted manner can you prepare for  eventuality eventuality the title of this module was dataset augmentation so what  would happen is are given some training data  you can always generate for training data from that see here is another training instant  that i have created i have just rotate it to two by some random angle i took this image i  just rotate it and this is a simple operation that all pixels are moving by a certain angle i  could have rotate it more i could have shifted it vertically that means in all my image  the two was actually exactly at the centre i just shifted at a bit vertically  so i am so think that you are reading one of those kyc forms or bank forms most  people would write at the center of the block provided but some people could write to  the extreme right or extreme left right so you are preparing for that they saying that ok  all my data the digits are well written at the centre but let me just shift them bit so that  i can also deal with people who write it at the corner  left align or right align instead of center align i could have even shift it horizontally  most people would write at the center but some people would write at the top or at the  bottom i could blur the image but someone has taken a photo and send it to me and the  photo is not very clear or i could just change some pixels randomly right i could add  noise all of this is dataset augmentation with the hope that i am capturing with these  variations i am capturing enough variations in the data  so that i have a better chance of doing something better on the test data is that fine  this is all still training data mind you i am still going to compute the empherical train  error it is just that now i have blown up my data but much more than what i had  initially do you all see by doing this you could have done better on the mnist  assignment you could have done better again i am not asking you to do this  so now i will do this then i will have supervised data because i know that by this small  variations the label is not going to change and what am i using there i am using my  domain knowledge right i cannot do this always right i hope you appreciated that  suppose that changes the domain a bit and i am given images of defects of motor parts  right where i have taken a image and there is a black spot somewhere which indicates  defect i cannot go about doing the same thing there i cannot change some other pixels it  will just means that the defects is at a different location right but in many cases you can  do that  so if you are given picture because of dogs and cats because the entire world case  about classifying cats and dogs then you could do some rotations you could blur them a  bit you could occlude certain questions of the picture and so on and still generate  training data right and what you are trying to do is trying to take care of cases that you  would end up dealing at test refer time 04two7 right is that clear ok and please be  aware that we are exploiting some domain knowledge here    typically more data is better learning works well for image classification in object  recognition these are the task where this is already been tried out and they have shown  to work very well in these tasks also shown to work well for speech where the people  have some speech training data they try to augment it for some task it may not be very  easy to generate such data right  so you could think of various nlp applications whereas given you a data document  right because always do what joe does in that friends episode do you remember what i  am talking you see what i am talking about see you wants to write a recommendation  letter for monika and chandler ok and he has a letter written any replaces every word  by its best synonyms from the thesaurus refer time 5two0 right that says a way of  generating noisy data and in that case it was actually noisy right so you could think of  doing here but as happened in that case it will not result in very good transformation  next for example i remember something right they are very warm hearted people got  translated as they have some warm cardiograph or something like that which do not  make sense so it is not very easy in almost all application should do it but in some  applications typically in vision application this is easy to do and you would gain a lot  by doing this right  
task2/super_cleaned_audios/lesson89.wav,1017.1005,so we will start from where we left yesterday so this is what we had seen yesterday  we saw what’s the difference between a convolutional neural network and a feed forward  neural network and we focused on two main properties one is sparse connectivity and  the other was weight sharing that is about it and then we saw that this representation of  i mean we saw this diagram about how to you could have multiple kernels and each  kernel would apply across the entire image and the weights would be shared for that  kernel     so far we have only focused on the convolution operation and even when you have seen  the neural network or the convolutional neural network we have only seen the  convolution layers  right so there is something more in a typical convolutional neural  network and that is what i was about to start yesterday so we just continue from there     so this is what a full convolutional neural network looks like so ignore these things for  now all these parameters etcetera as a just ignore for them i will just walk you through  what is important in this diagram right   so your input is an image and the tasks that you are dealing with here is digit  recognition or handwritten digit recognition right and what you see here is that you have  taken an input which is a two dimensional input and then the next layer you actually see one  two three four five six outputs so what does that mean  student refer time zeroonethreethree  you have use six filters apply them throughout the filter throughout the image each filter  gave you one feature map and so in this layer you have six such feature maps so the  original two dimensional input has now become six two dimensional outputs ok after that  there is something known as a pooling layer we will see what a pooling layer is in detail   now what i want you to understand is lets assume that what the pooling layer does is it  does some kind of a shrinking it takes the original output and shrinks it how it shrinks  it we will see in a while but let us see what happens after that so now you have one two  three four five six as your input so now you have this is your input this volume is now your  input i call it a volume because it has depth height and width and now you are again  going to apply convolutions to that and what do you see how many outputs do we have  now we have onesix outputs  right so what does that mean  student refer time zerotwotwo7  i took this threed input applied onesix threed filters on it each threed filter give me one feature map one  twod feature map why one twod feature map  student refer time zerotwothree9  because we are doing a twod convolution we are taking a threed filter but we are doing a twod  convolution  right and then after that again this becomes my input and then do a max  pooling on top of that and then something else happens after that so there as which we  will come to later  so right now i just want to say that there is this input then you come out will come with  some output after applying convolutions now this becomes the input for your next stage  where you have done pooling now the output of pooling becomes the input for the next  stage so you will take this as the input apply some convolution on that get some output  and continue in this way right and we will come back to what these are       so now let us dive deeper into what is pooling what does the pooling layer actually  do so here is your input again it is a volume and now when i say input you should not  just think of the input as this remember that all of these can be inputs right so now at  every stage once you have got an output for the next stage that becomes input that is  typically how it was even in the feed forward neural network right once you compute a  hidden representation that is the input to the next layer   so i have some input at one of these layers either the input layer or any of the  intermediate layers and i apply a filter on that and that filter gives me some twod output it  gives me one feature map and let us say this is what the feature map looks like   now what does the pooling operation do so i would apply two  two pooling with a stride  off two so let us see what that will do that means i look at this two  two region i will pick  up the max value from there that is why this is max pooling ok so the max value is eight  i will just keep that then i am going to do a stride of two that means i am not going to  place it on this block i am just going to shift to the next block again i will take the max  from there which is four right and i continue this and i get this is the output so you see  why the shrinkage happens because i am taking a two  two area and i am shrinking it by a  picking up only one value from there right   so it actually kind of half the width and half the breadth so total of one by four reduction is  what you get but you could also use a filter with stride off one so this is what it would  look like so you will place it here take the max value then the max value then the max  value and so on right so in that case you will get a lesser reduction and instead of max  pooling you could also do average pooling that means instead of taking the max of  these four you could take the average of these four is it fine so is the pooling operation  clear and how it results in the reduction of the size of your input ok    so now what we will do is now that we have some idea of what a full convolutional  neural network looks like so it looks like alternating convolution and max pooling  operations we know what a convolution operation looks like in particular we know that  a threed filter applied to a threed input results in a twod output because we are not applying the  convolution along the depth we just applying the convolution along the width and the  height right so that is what we know so far    and based on this knowledge now we are going to see some success stories of  convolutional neural network right  so we will start with the first one which is lenetfive so this was already the fifth  version this was around ninety-seven or 9eight or something and i had mentioned this when we were  doing the history lecture   so this is the input now you have decided to apply six filters you have said that the stride  is going to be one that means you are going to place at every location the spatial extent is  going to be five and the padding is going to be zero  now the question that i have for you is  how many parameters does this convolution layer have what are the parameters here  student the weights  the  student refer time zerosixtwofive  the filters the weights in the filters how many filters do have  student six  six how many ways does each filter have  student twofive  twozero  student five  twofive right five cross five is a twofive so the total number of parameters is onefivezero now i want you to  appreciate something here so the input was actually threetwo cross threetwo which i believe onezerotwofour  and the output was twoeight cross twoeight right that is what the output you got that is i guess 7eightfour  yeah so in a feed forward neural network if you had x belonging to ronezerotwofour and your h  belonging to r 7eightfour how many parameters would you need onezerotwofour  7eightfour how many  parameters do you have here   student onefivezero  onefivezero much much smaller right this is because of  student refer time zero7twoone  both sparse connectivity as well as weight sharing right so now you appreciate the  difference between the twook and thats one of the reasons that even before this is for  example from ninety-seven 9eight one9ninety-seven one99eight right so even before the deep learning wave of twozerozerosix  or the revival after twozerozerosix right convolutional neural networks must still being trained for  even deep networks four to five layers because they had much fewer parameters and that is  why it was relatively easier to train them as compared to a very dense feed forward  neural network    so i want you to appreciate that fact now after this i have the pooling layer when i am  going to use a stride of one and f equal to two that means i am going to pick up ok so  now i am going to use a max pooling layer where i am decided to use a stride of one and  the max pooling will happen in the region of two  two and again k six because there are just  six filters so max pooling happens on every feature map independently it does not  happen across the depth that means i am not going to pick the max along these six layers  i am going to pick the max along each of these feature maps so it is a per feature map  operation ok  and this since it is a two  two filter it will result in a size reduction and from twoeight  twoeight i will  get to onefour  onefour how many parameters is the max pooling layer have  student refer time zeroeightfourtwo  wow good there is no parameters in the max pooling layer because you are not having  any weight matrices just taking the input and applying a simple max operation on that  there is no w transpose x or any kind of a transformation happening there now this  becomes your input what’s the size of this volume  student threetwo  onefour cross onefour cross  student six  six so all the filters that i am going to use from now on what is the depth of those filters  going to be  student refer time zero9onezero  what is the depth going to be   student six   i want to everyone to answer  student six  six right because we are always going to assume that the depth is equal depth of the filter  is equal to the depth of the input     now here they decided to use onesix filters and by the way you did hopefully notice that  this twoeight how did you get it from which formula  student w n minus refer time zero9threefour  wn minus f plus twop plus one right so that is the formula   so now we have a onefour cross onefour input and you have onesix filters so what is the depth of the  output volume going to be   student refer time zero9foursix  what is the depth of the output volume going to be  student onesix  onesix right and a ok so you have onesix and you have a spatial extent of five  five just a minute  spatial extent of five  five how many parameters does this layer have i want everyone to  say it  student fourzerozero    fourzerozero ok fine so that is  student refer time onezeroonezero  there are onesix of these each is five  five  student refer time onezeroonethree  oh into six into d ok ok good then we made a mistake here also no there the depth was one  are you do you get it how you got twofourzerozero right we forgot about the depth so each of  these filters is five  five  depth right what is the depth six the same as the input right so  each of these filters is twofive  six which is onefivezero  onesix yeah fine is that ok did i confuse you  or everyone back on track pooling layer edge should have been two because of that half  reduction using  student refer time onezerofour9  yeah maybe can we just check this i think it should be two ok yeah there was a  question  student refer time oneonezerozero   student refer time oneonezerotwo  go from the pooling layer to the next layer from here to the next layer ok  so now what is the depth of your input volume six and what is the width and height onefour  cross onefour so now every filter that i going to apply at the next layer is going to have a  depth of six so i have decided to apply onesix such filters so what is the depth of this layer  going to be the depth of the output is equal to the number of filters so the depth is  going to be onesix and all my filters are five  five but what we forgot is that when i say the  filter is five  five it is actually five  five  six because the depth is also there right so it is  width into height into depth so that is this number of parameters in each of my filters  right and that is onefivezero and i have onesix such filters so that gives me a total of twofourzerozero  parameters is it fine ok ok  so now we have a volume of size onesix cross onezero cross onezero now i am going to do max  pooling on that maybe again this should be two there was the same doubt you had fine  so it will result in a reduction in the output and now what is the volume what is the  size of this volume five  five  onesix and the parameters is zero max pooling layer does not have  any pooling   now after this what we have is something known as the fully connected layer ok so  now as i said the size of this volume is onesix  five  five its arranged in these feature maps  but i can always flatten it to get one single vector do you get that so from these onesix  feature maps each of five  five size i can flatten it out and get the single vector of size fourzerozero  do you get that ok so thats what i do in the fully connected layer  so now i am going to flatten this treated as a single vector and then fully connect it to  the next layer what do i mean by fully connected dense connections no more sparse  connection so now we have a feed forward network from this point of view so you  have fourzerozero and that connects to a layer of size onetwozero so what are the number of parameters   student fourzerozero into onetwozero    fourzerozero into onetwozero plus we will have onetwozero biases so that is what this number is fine so this  is one fully connected layer of size onetwozero after that i have another fully connected layer of  size eightfour so the number of parameters would be onetwozero into eightfour plus eightfour and after that i have  the output layer so the output layer this was twosix or whats the output  student refer time onethreethreeone  oh but the this is digit this is alphabet recognition right ok so probably they have  done the computation using onezero but it should have been using twosix as the output layer  because you want to predict one of the twosix alphabets so you can assume this is twosix so it  would be eightfour  twosix  twosix right that is the size of the output layer right   now do you observe something immediately is a something very striking immediately  in terms of the number of parameters   student refer time onefourzerozero  the fully connected layers clearly dominated right here we were dealing of order twofourzerozero  and max and here we just start with foureightzerozerozero itself right so just keep this in mind that the  fully connected layers have the largest number of parameters that you have and we will  try to come back to this and see if we can solve this problem ok  so now when you see a convolutional neural network you should be able to reason  about the following things at each layer what is the size of the input volume what is  the size of the output volume what are the number of filters being used and what are  the number of parameters in that layer right if you can reason these things and you  have really understood what is actually happened and unless you can reason these  things i do not see how you can efficiently code it up right so you should be able to  know that this is the size of the input this is the size of the output and so on and i guess  all of us are comfortable with others     so now how do we train a convolutional neural network what is the answer your  nodding that means you do not know or you know it is too trivial to even ask this  question how will you train it  student refer time onefivezerozero  but how do you back propagate through a convolution operation that is a very nasty  looking operation     a cnn can be implemented as a feed forward neural network with what being the  difference  student refer time onefiveonefive  only some of these weights would be active all the gray weights will not exist only the  colored weights will exist right now can you back propagate to this network have you  seen something similar before  student dropout  dropout right so if you could do that you can do this also so now if you take this view  of a convolutional neural network where so this is just to give you an intuition or make  you feel confident that once you know the backpropagation algorithm there is no  nothing much different from training a convolutional neural network operation ok  so everyone is fine with that how many of you agree with that that you can actually  train using the same algorithm with some smart coding required to make sure that these  weights are not active and so on but in practice of course you will not do this in  practice you will define the convolution operation you will also define the derivative of  the convolution operation and then use that right because that would be much more  efficient this is very inefficient right because you are assuming that there is a fully  connected network and then some things do not exist there so that is not thats the whole  point i mean you wanted to avoid such dense connections right   but in principle you could have just used this and trained the convolutional neural  network in practice you do not need to worry because you just need to define your  forward convolution operations and people like google and all who release tensor flow  torch and all will do the hard work of doing the back propagation for you right so you  never have to write back propagation in your life apart from what you have already  written in the assignment right that is why i make you go through that torture once   so now afterwards afterwards whenever you use any of these platforms or pytorch or  torch or tensor flow the back propagation comes for free you just need to write the  forward progression that means you just need to write convolution operations and you  dont need to worry about how the derivatives will be computed but i what i want you to  understand is that conceptually it is the same you can still use or in fact you still use the  same back propagation algorithm to train a convolutional neural network also everyone  is fine with this pk ok so now we know how to trained a convolutional neural  network also   
task2/super_cleaned_audios/lesson11.wav,777.1452,"let us start with module two which is about mcculloch pitts neuron   so as we are done this during the history lecture way back in one thousand, nine hundred and forty-three mcculloch and pitts they proposed highly simplified computational model of the brain so now let us see what’s the motivation we know that our brain is capable of very complex processing it’s capable of taking a lot of inputs from various sources and then help us taking various decisions and actions now what if you want a computer to do this we want a module which is very similar to how the brain works or at least how we think the brain works which takes a lot of inputs and then does some processing and helps us take a decision  so what they proposed is this model which will take a lot of inputs and these inputs are all binary all these inputs that you see here these inputs are fed to this mcculloch pitts neuron which is an artificial neuron and it is divided into two parts so the first part collects all the input so remember you had these dendrites which were taking all the information from everywhere  so  this just collects all the information and then the second part is aggregation i have collected a lot of information from all the sources now the second function will decide what this aggregation is and based on that it will take a decision whether to fire or not  so the output is again boolean if it’s zero then neuron does not fire if it’s one the neuron fires so let us take a concrete example so suppose i am trying to make a decision whether i should watch a movie or not so xone could be is the genre of the movie thriller similarly there could be another variable say xn which says is the actor matt damon so these are all various such factors that i could take is the director christopher nolan the music given by someone and so on so all these are probably factors which help me decide whether i want to watch this movie or not and you want this neuron to help us make that decision  so now what is happening here is these all inputs they can be either excitatory or inhibitory so let me tell you what inhibitory is first so you are taking input from a lot of sources now see one of these sources or one of these inputs is am i ill today am i down with fever so  if that input is on irrespective of who the actor director or whatever is i am not going to watch the movie right because i just cannot leave from my bed so these are known as inhibitory inputs irrespective of what else is on in your input features if this input is on your output is always going to be zero that means the neuron is never going to fire so you could think of it as suppose my mood is not good today i do not feel like getting up or if i injured my leg or anything  right if  any of these conditions is on irrespective of what the other factors are i am not going to watch the movie  so that is an inhibitory input and excitatory input are on the other hand is not something which will cause the neuron to fire on its own but it combine with all the other inputs that you have seen could cause the neuron to fire and how so this is how so these are all the inputs that your neuron is taking all i am going to do is i am going to take a sum of these  i am going to take aggregation of all of these so what does this count actually give me the number of inputs which are on the number of inputs which are value one that is all this aggregate this is a sum of all the ones in my input now this is what g does this is a very simple function is taking a sum of my inputs now the function y takes this as the input that means it takes this sum as the input and if the sum is greater than a certain threshold then it fires if the sum is less than the certain threshold then it does not fire so again see what is happening here is it is same as now if you depend on the actor director and genre and so on and you fine at least two of these three conditions are satisfied at least i am happy with the actor and the director even though the genre is not something that i care about  i will watch the movie or you might be a very niche go movie watcher who only goes to a movie if the actor matches your requirement the director matches your requirement and the genre and the music and everything matches your requirement  so  you are threshold in that case it should be high so this is how it is going to help you make decisions now again a very simplified model and this is theta is called the thresholding parameter that is the value which decides whether the neuron is going to fire or not and this over all thing is known as the thresholding logic so this is what a mcculloch pitts neuron looks like  now let us implement some boolean functions using this mp neuron so from now on i will just called it mp neuron and we will try to implement some boolean functions using it so now why are we interested in boolean functions it is because we have overly simplified the way we take decisions we are saying that the way we take decisions is we take a lot of boolean inputs is actor matt damon and genre thriller and so on and based on that we produce a boolean output  so an input is all booleans so we have xone to xn which are all booleans and your output is also boolean so that is a boolean function that you are trying to learn from x to y is that clear you have x just happens to contain n different variables here ok and lot of decision problems you could cast in this framework you can just imagine right whether to come for lecture  today or not  again is you could cast in it depending on various boolean inputs  this is a very concise representation of the mcculloch pitts neuron what it says is it takes a few boolean inputs and it has certain threshold if the sum of these inputs crosses this  threshold then the neuron will fire otherwise it will not fire that is the simple representation of the m p neuron now suppose i am trying to learn the and function when would the and function fire all the inputs are on so what should be the value of the threshold in this case three everyone agrees what about the or function one let us see a few more this function so let me tell you what this function is so you see this circle here so that means that this input is an inhibitory input if that is on then the neuron is not going to fire that is how i am representing it so now tell me what should the threshold for this be it is not so hard  see if xtwo is on it is not going to fire so you have four rows zero zero zero one one zero one one so two of those are ruled out and it is not going to fire now out of the remaining two when do you wanted to fire so what should be the threshold one now what about this function zero or three three is not even a valid option zero everyone agrees to that and what about this zero so you get this so now if you have a certain number of input variables and the function that you are trying to model the decision that you are trying to make is a boolean function then you could represent using these mp neurons whether all boolean functions can be represented in this way or not that is still not clear i am just showed you some good examples we will come to the bad examples later on here is the question  so can any boolean function be represented using a mcculloch pitts neuron so before answering this question we will see a bit of a geometric interpretation of what mp neuron is actually trying to do so let us take or function where you have two inputs xone and xtwo and this neuron is going to fire  if xone plus xtwo is greater than equal to one that is clear that is how the definition is now if you look at this xone plus xtwo greater than equal to one now let us ignore the greater than part first so we will just talk about xone plus xtwo equal to one what is this equation of a line everyone gets that ok now in this case since we are dealing with boolean inputs and we have two access xone and xtwo how many input points can we have four right zero zero zero one one zero one one  so you could have these four points so just note that this is an xone and xtwo axis but only four inputs are valid here this is not a real numbered access this is only boolean inputs possible here now what is the line xone plus xtwo equal to one tell you which line is that  so one which passes through one zero here and zero one here this is that line now what do we want that for all those inputs for which the output is actually one they should lie on the line or on the positive side on the line and all those inputs for which the output is zero they should lie on the other side of the line is that happening so what is actually mp in unit actually learning linear decision boundary it just what it is doing in effect is actually it is dividing the input points into two halves such that all the points lying on that line right are sorry all the points for which the input should be zero lie below this line and all the points for which the output should be one sorry in both cases it should have been output  so let me just repeat it all the points for which the output is zero lie below this line and all the points for which the output is one either lie on this line or above the line is that fine and so let us convince ourselves about this  even it is not already clear from the equation for how many of you it is already cleared from the equation that this is exactly what it does for a large number of periods but still we will just do a few examples and move ahead   now for the and function what is the decision boundary it is xone plus xtwo no that is the decision boundary equal to two so again i have these four points only these four points are possible now where is my decision line passing through that one one and intercepting this somewhere around two zero and this around zero two so that is the line which i am interested in now again do you see that our condition is satisfied that all the inputs for which we want the output to be one are on or above the line and all the inputs for which we want the output to be zero or below the line now what about this function what is the threshold zero so what would the line be xone plus xtwo equal to zero which passes through the origin right and again all the points are either on or above the line so this part we are going to call as a positive half space and this we are going to call as the negative half space   now what if we have more than two inputs in a two dimensional case when we just had xone and xtwo we are trying to find a separating line in the three dimension case what will we do plane in the higher dimensions hyper plane so this is now your three dimensional case again there are three axis here but not all points are possible how many points are possible eight points and which is the function that we are trying to implement  or  so for these eight out of these eight points for how many is the output one seven and for one it is zero so what is the kind of plane that we are looking at we are looking for a plane such that seven points lie on or above it and one point lies below it and which is that point  zero so now what is the equation of that hyper plane x one plus x two plus x three is equal to one you see this so you see that all the seven points are visible but the points zero zero is not visible because it is on the other side of the plane so this is doable in three dimensions also and again in higher dimensions also right we could find in hyper plane  so the story so far is that single mcculloch pitts neuron can be used to represent boolean functions which are linearly separable so a linearly separable function is such that there exists a line such that for that function whichever points produce an output of one lie on one side of the line and whichever points produce an output zero lie on the other side of the line  "
task2/super_cleaned_audios/lesson39.wav,518.9021,so we were looking at these different variants of gradient descent we saw that gradient  descent has this problem that it finds it difficult to navigate the gentle slopes so we  came up with tricks on momentum based gradient descent and also nesterov accelerated  gradient descent   the trick in momentum was that if lot of your history is telling you to move in a  direction then just continue to gain momentum in that direction so instead of just  updating based on the current gradient you also update based on the history right and  there we saw that this is always going to be a problem that you will end up taking uturns  and we had this analogy of how you look for directions and you just overshoot your  destination and have to come back and take a uturn and come back and so on  so to prevent that we realize that the update done by momentum base gradient descent is  two step update you actually the first step is based on the history and then another step  based on the gradient at the current time step right so then instead of doing these two  steps at one go why not just update based on the history see what the gradient that tells  you and then we saw this nice figure i hope it was nice and where you saw that if you  look ahead point then you will be immediately corrected with respect to your errors so  that was about nag and momentum  then we saw the stochastic versions of these algorithms where we realize that if we do  the batch version then you go over a million points and then make only one update  which could be very slow in cases where you have large data so we then decided to the  stochastic version where we just update for every point that again had these oscillations  because we were taking greedy decisions we were just relying on one point to tell us  which was the right direction to go on and you saw that these esteem has become better  as you increase the value of this k  so k equal to one is the most stochastic version and then k equal to two you get the mini  batch version and then you could just have different values of k so that you have more  reliable estimates of the gradients and in the limit if you have the entire data then you  are just doing the full batch gradient descent right this is the vanilla gradient descent  anything else did we cover then we had some tips on the learning rate and the  momentum these are again heuristic i gave you some ideas and you could try these in  your back propagation assignment and see which one works better for you you could  see you have any peculiar observations while implement the back propagation  assignment  so now there are a few more things left in this lecture so i will start with the line  search first so this is one more thing before you move on to some more interesting  algorithms which are the current state of the art and lot of deep learning solutions    so most people that you read would look at would have algorithms that we will see  after one0 minutes    so now this is where just to contest contextualize things right so we are still trying to  see what is the light right learning rate to use a line search is one such method where  instead of just doing one learning so you can look at the code and just focus on this part  and tell me actually what are we trying to do how many of you get what the algorithm is  trying to do so far what we were doing is we were just having a single learning rate ok  and we saw that this learning rate can make a lot of difference right because if you are  on the gentle part you want larger learning rate and if you want steep part you want  smaller learning rate  so just fixing the learning rate to one value does not really help because then you will  make you will suffer on one of the two cases are either on the gentle case or on the steep  case now what line search does is instead of just using one learning rate at every step  now whether it is vanilla gradient descent which is the batch one or mini batch or  stochastic right just use a bunch of learning rate so i have used five different learning  rates here and i have computed the gradients that part remains the same the  computation of gradients does not change  now you have the value now you want to be conservative you want to multiply the  gradients with this eta right but you know that you do not always want to be  conservative in fact in some cases when you are on the gentle slope you do not want to  be conservative at all you want actually blow up the gradients so now try these  different learning rates and update w and b ok so if you have five learning rates you  will get five different updated values for w b  now plug in all these w b values into your loss function right and see whichever is the  minimum retain that w b value and repeat the process that means again you will  compute the gradients with respect to this new value of w b and the new loss function  again try out these five different learning rates and continue everyone gets that  so now are we using a fixed learning rate at every step no and now do you see that if  we are at a gentle slope it would pick probably this as the learning rate and if we are on  steep slope which should probably pick one of these as the learning rate and even lesser  than that if you have the disruption it does not make sense you see the advantage of  this now you are in some way heuristically trying to adapt to the slope of the error  surface right by just giving a different learning rates so try all of these and whichever  works best pick it up ok so that is about it we are trying different values  now what is the flip side of this now if you have k different learning rates that you are  trying then at every step you have now increased your computation k times so earlier  you just add one learning rate u just going by that but now i have k so now this is  again a trade off which is you have to see now i will give an example where this trade  off clearly works  so now if you are at the gentle slope now making k more computations and moving out  of that slope is definitely worthwhile as compared to just sticking to that slope where  even after hundred more computations you will not really move out of that slope so  remember that gradient descent algorithm that we have seen where you just stick to the  gentle slope after hundred iterations also right but instead if i tried five different learning  rates and there is a high chance that i could have moved out of the gentle slope does  that make sense you see the advantage of this    so this is something that i have to talk about when i back to second order optimization  so i will see when to teach that so let us see line search in action so this is again  gradient descent this black curve which is visible there this is the one i am talking  about which is run for few iterations and is just stuck on the steep curve you know this  story now and it is just get stuck there  now let us see what happens if i run so now i will start running the line search based  gradients descent so what do you expect now so it will just move very fast right so  on the first step itself it is crossed wherever gradient descent was stuck after fifty iteration  or so i will keep moving fast  now here is an interesting question would you see oscillations here so when you see  oscillations it is when your loss is actually increased from whatever it was currently  will that happen in line search the answer is always no  it could happen when could this happen so it depends on the learning rates that you  have chosen right so if you have chosen the learning rates so suppose at one point to  really be effective you needed the learning rate to be 00one ok and now if 00one learning  rate was not in your set right that means everything that is there in your set is faster than  00one so that it will again have the same problem as momentum because you will move  faster than what you should actually move so it depends on this careful choice of the  learning rate set  so that is all i have to say so there is a slight convergence would be faster than vanilla  gradient descent that is obvious and we see some oscillations ok and the statement is  actually wrong we need to remove that ok we see some oscillations and these could be  the similar wants to the once with that we see in momentum because we overshoot  because we have not chosen the right set of learning rates  one of the learning rates which was actually needed at a particular point right say at this  point suppose i needed to move very slowly and that very slowly say 000one and that was  not in my set then any other learning rate is always going to be much faster then so  you could see oscillation  
task2/super_cleaned_audios/lesson106.wav,622.0587,and that takes us to the problem of vanishing and exploding gradients ok so you want  to see what is a problem with this back propagation through time which could lead to  certain interesting situations  so we will focus on this st  sk and let me just go back so remember that this  formula had this st  sk right where st could be the last time step and sk could also be  the first time step because you are summing over all the time steps right    so you could have a term which is st capital t which is the last time step the first time  step and the derivative of the last time step with respect to the first time step right so  that is a situation that we are dealing with so we will consider one such generic element  which is st  sk and we will just try to expand it so remember i have done this short  circuiting so i am now just going to expand it again so this is going to be t  t  one t one t  two and so on up to k one sk ok and i can write it as this generic formula everyone find  with this i have just replace this as a product and written it more compactly  now let us look at one such term here sj sjone now just to confuse you guys from  next slide i will go over to sj sj one or not confuse you i just did not pay attention to  this so instead of s plus one and j and i am going to do j and j minus one right it remains  the same does not matter    so we are interested in this particular quantity so let us see what this derivative is and  remember that in the final formula we have a product of these quantities so i am  looking at one such term in my final product so just to jog a memory a j is the pre activation which is given by this and then s j is the hidden representation after activation  after the nonlinearity which is given by so let me just write it down as s j by s j minus one  can be written as this chain rule which is first compute s j with respect to a j and then a j  with respect to s j minus one everyone is fine that so far at this point please raise your  hands if you find ok   now let me just write down a j and s j explicitly so remember that a j is this d  dimensional vector which are the entries a j one a j two up to a j d and s j is the  corresponding activation applied vector which has these entries sigma a j one a j two and so  on ok now first question what is this quantity scalar vector matrix tensor  numerator is a  student refer time zerotwofourfour  denomitor is a  student refer time two5four5  that is why it is a matrix ok so that is the matrix that i am interested in if i can give  you that matrix and we are kind of done so it help me filling in this matrix  tell me what this matrix is going to look like even before we start filling it ok you are  right but it does not matter because you will have u x and then you are taking the  derivative with respect to s j minus one right so this does not matter ok so everyone gets  that you will have a u x j here right but that does not matter because you are taking a  derivative with respect to s j so that is a constant  so sj  aj  is what what does this matrix look like how many of you see a diagonal  matrix ok good so it is straightforward right what is the first entry it is going to be  sjone ajone what is that going to be it will be something but let us look at the second  entry sjone ajone what is this going to be what this going to be  student zero  zero because it does not depend on that right so now you can see how the full matrix will  look like all the ofdiagonal elements are going to be zeros and diagonal elements are going  to be sigma primes everyone fine with this ok so this matrix i am going to just call it  as diagonal sigma prime a j this is a diagonal matrix which i have and what is dou a j  by dou s j minus one scalar vector matrix scalar  student refer time zerofourone7  matrix which matrix  student w refer time zerofourone8  w right ok so now for some reason i am interested in the magnitude of this why i am  interested in the magnitude of this for some reason i am interested let us see why we  will become clear that for some reason i am interested in    and here i will write how i will write the magnitude of this right so this is the norm  that i am interested in so i have already said that this is actually equal to whatever is  inside this norm so i can just write it as this norm so i have norm of c is equal to norm  of a b which is less than equal to  student norm a norm b  norm a norm b ok this is fine ok now let us look at the norm of this now going to  say that sigma a j is actually a bounded function because we are using sigmoid or tan h  or something so it is a bounded function ok so that mean sigma dash a j is also going  to be bounded actually can you tell me what is the bound for the logistic function for  sigma dash a j   if sigma is logistic function what sigma dash what is the bound for sigma dash if i say one  by four how many of you will agree with that how many of you have a problem with that  if you do not understand this you not understand anything after that ok still do not have  a problem so for the logistic function the bound is actually onefour the maximum  derivative that you can get if you have this curve so then that would be onefour ok  what about the tan h function and that actually happens at this point right zero5 so zero5  into zero5 is onefour what about the tan h function the bound is one right so this is this  clearly an upper bond on these things the derivative is going to be an upper bounded  thing that means this magnitude is actually going to be upper bounded by something and  i will just call it as lambda sorry as gamma so this quantity is bounded and i am going  to call that bound as gamma  what about our weight matrix it is again bounded right we have real weights we do not  have like blowing we do not have very large weights it is all bounded so it is still going  to be some upper bound on this and i will call this magnitude as gamma right so this  quantity on the left hand side i can say that it is less than equal to some gamma into  lambda  now let us look at the product so this is a quantity that i was interested in and this is  actually a product of various such quantities so what is it going to be now can you go  to the next step it will be gamma into lambda raise to t minus t minus k right t minus  it basically as t minus this product as t minus k terms right so it will be gamma lambda  raise to t minus k now if gamma or lambda or rather gamma into lambda if it is greater  than one what will happen what will happen to the series explore if it is less than one  student refer time zero7twozero  it will vanish right so you get that so that is why you have this vanishing an exploding  gradients problem ok but why what if this vanishes what vanishes let us go back so i  have shown you that this quantity could vanish right if this vanishes the entire gradient  could vanish and if the gradient vanishes what would happen    student no updates  no updates and you just stuck where you are if the gradient explodes what happens  think in terms of the wb plane you suddenly have a very large gradient what will  happen is just gone way far from where you are right now because your update is w is  equal to w minus eta into this gradient and this you have got a very large value now   it just going to move somewhere very far from where you are and that is never go where  your suddenly jump to a different universe ok so that is the problem in training  recurring neural networks you could have this problem of exploding or vanishing  gradients and we have done a mathematical derivation of why you have this problem  ok    so one trick to do that is to avoid this is remember these are t minus k terms and the  problem appears when your t minus k is or rather you are t is close to capital t and k is  closed to one right in those cases you will have many terms in the product you will have  as many as t terms in the product so even if your product is even if this product is  slightly less than one if you raise it to capital t it is going to vanish right so can you  think of solution for this  and the last module in the title of this lecture was truncated back propagation can you  think of a solution for this so you do not back propagate through all the time steps yes  use an approximation that if you are at time step n we are just going to look at n minus  k time steps and we are not going to look all the way back right that is the common  trick used to avoid exploding and vanishing gradients  what is the other thing that you could do to avoid exploding gradients so remember  that you have some gradient right to think in terms of vectors we have some gradient  vector w whose magnitude is very large what will you do to avoid exploding gradients  in gradient descent your always interested in the direction so what can i do   student refer time zero935  just normalize it right so you can just do this so typically what is done is that you can  it is a normalizing it you can just say that you will clip the gradient so that it is  magnitude is less than a certain k right so normalize it in such a way that it is grade it  is magnitude becomes k so this is something typical that you will see when you use  tensor flow where you have something with says clip the gradients to a certain  magnitude and there are different ways of doing this so i just give you an intuition that  this is what is used for magnitude but there are other things that you can use for  magnitude so just go back and look at that ok  so that is a back propagation through time with exploding and vanishing gradients and  then the solution for that or a part for that is truncated back propagation ok we have we  have not yet done with this problem we will again look at other solutions for handling  this which will lead us to lstms which is long short term memory cells and gated  recurrent units so that we will do in the next lecture   
task2/super_cleaned_audios/lesson112.wav,1273.3481,and in this lecture we are going to talk about encoder decoder models and attention  mechanism so this is a very interesting lecture at least interesting to me because this is  very put all these pieces that we have learnt so far right we have learnt three types of  networks feedforward networks recurrent neural networks and convolutional neural  networks and we have seen independent applications of each of these word to vec and  image classification and so on now today what we are going to see is how do we do  different combinations of these networks and come up with a wide range of applications  like apply them to a wide range of applications ok so let me start by an introduction to  encoder decoder models and then we do various applications of encoder decoder models  so what we are going to do is we will start by revisiting the problem of language  modeling so the problem of language modeling was that you are given some t  one words  or characters and you want to predict the tth word or character right this is like auto  complete in short right whenever we are typing something you have type four words you  want to predict the fifth word or you have typed four characters and you want to predict  the fifth character ok   so more formerly this is what we are interested in how many of you get this equation  this expression so we are given a sequence of t  one words and you want to find out  what the value of y would be at time step t and we want to find out that value which  maximizes this property that is what this argmax equation means and now we will try  to see how to model this using a rnn   so let us see we are going to start with go that is that we want to start generating a  sentence and then we will produce the first word which is i ok and what is it that we are  predicting at this point what is the network supposed to predict what is the output  supposed to predict actually  it is supposed to predict a dash over the vocabulary a broadly distribution over the  vocabulary right so this is what is happening we will of course come back to this on  the next few slides but you have say words wone wtwo up to w v in your vocabulary at  every time step you want to find a distribution over these words and then pick the word  which had the maximum probability at that time step right that is exactly what this  quantity is that is what we want the rnn in to model and then we want to keep doing  this till we reach the end of the sentence ok so that is the language modeling problem  and as we had made a case for it earlier the word produce the time step t depends on a  few previous words how does a recurrent neural network ensure that at any time step  i am going to give it only one word as the input   so how does that ensures that it depends on all the previous words also through the  recurrent connections and the gate and sorry it is not the gate the state st ok fine    so we will see this of course in more detail and we will write down the model equations  and what is happening so we are interested in this quantity which is the probability of  the word at the time at the t th time step where this j belongs to vocabulary v and see a  vocabulary of onezerok words or twozerok words for english it is actually much higher but say you  are considering only onezerok to twozerok words then we want to predict a distribution over this  vocabulary so using an rnn what are you going to do at the output layer is the  following is this correct how many if you understand this equation not many why  what does this equation compute first of all softmax softmax means  student probability distribution  probability distribution ok what does it take as input at every time step the state  right what does it do with the state a linear transformation right and then a bias ok so  what is this quantity scalar vector matrix vector of size    students refer time zerofourzerozero  the refer time zerofourzeroone what is the gth element of the that  students probability of the gth word  the probability of the gth word right so i just have to explain it in that many words  everyone gets it now everyone gets it if you do not get it you will not understand the  rest of the lecture i am very serious everyone gets it   so in other words what we do this entire yone to ytone which we were conditioning on we  are just using st as a surrogate for that and that is fair because st has actually captured all  the previous information that we had now just using st as a state which captures  everything that happened so far  so that is actually how we are modeling this and the recurrent connections ensures that  st captures everything which has happened so far     so now let us look at the five things that we have in a typical supervised machine learning  set up which are those data model  students parameters  parameters  students objective function  objective function  student cross refer time zerofourfivefour  very good no someone said objective function and then loss function learning  algorithm right ok so whats the model here  students refer time zerofivezeroone  you know what you are trying to model which is a property distribution what is the  actual so here y is the probability distribution and your x is the input given to you can  you tell me what’s and we have already set always said in this course that whatever be  the y whatever the be the x we are interested in this function x sorry function f and we  should be actually expressively we able to write this function so what is the function  here can you actually write down the set of equation just think of what the output is  how you are going to reach the output given this network what is yt going to be what’s  the equation for yt   and then try to go back all the way back to xt so yt depends on something that  something might depend on xt so how do you go all the way back right that is the  thing which i expect you to do ok how many of you get it now please raise your  hands ok so let us see at every time step what am i interested in predicting  students probability distribution  a probability distribution that means i will have to compute which function  students softmax  softmax so the green vector is what i am going to focus on so what’s the equation for  the green vector is this fine now what does this contain apart from the parameters st  how do i get st is it fine you can write now you have written this output y as a  function of x because x appears here or other yt as a function of xt is not it is straight  forward right once i show you the answer is should be how many of you get it now  please raise your hands high up above okay what are the parameters b and c right  so these are the parameters what’s the objective function cross entropy or dash of  cross entropies  students sum of cross entropies  sum of cross entropies right so the loss is going to be over all the time steps at every  time step is the cross entropy loss right everyone gets this ok what’s the learning  algorithm back propagation  students true time  true time fine so that is what it is going to be right everyone is clear so you can see  that we have written the final output as a function of the input right and this is end to end  trainable that means the gradients can flow modulo this vanishing exploding radiant  problem and we have a way of handling that we can replace rns by lstms that is all  right  so that is what it is now this just make sure you understand this properly so  that we are going to do various instantiations of this for different problems ok    now here is one question we all smartly wrote this xt but why is the input at every time  step when i am predicting home the input was at but how did i get at that is what i  dash at the previous time step predicted at the previous time step right  so this is what the input looks like so at time step one i predicted i as the output at the  next time step i am going to feed that has the input does it make sense so just see if  you are doing auto complete you would select that i am fine with the word i at this time  step so it is going to take that as the input and then try to predict the next word that is  what exactly is happening here and now you are predicted am at the next time step you  are going to feed am as the input and continue this chain throughout ok  so the input at every time step is going to be the word that you have predicted the  previous time step and i am just going to represent it by a one hot vector right it is the  index of the jth word only that could be hot everything else would be zero and all of you are  fine with this no so at training time this is the inference time at training time we will  have the real inputs no that is at inference time at training time we will just use that  through because training time you know what the inputs are right you know the true  sentence you have the wikipedia sentence right and you know what the true sentence is  going to be  what i am talking about how will you generated at test time at training time you know  all these things right no about training time how will you do that you will know what  the next input is right so now ok so i said that the input is going to be a one hot vector  is everyone fine with that one hot vectors are ok what else could you use  students word representation  the word representation for that right so assume that you have already done the word  to vec assignment and you have completed all the word representations and you have  them with you now but instead of feeding the one hot representation of the input you  can just feed the word representation of it does that make sense one hot representation  is just one of the many representations possible for the world so why just do that you  could do s v d you could do one word vec or whatever you want right so that is in  practice what we will feed is the word to vec representation  so everyone gets this what is happening at every time step     now one more thing that you need to notice that szero which is the input at time step one  the previous so soneone so that we do not know what it is so we just keep it as a  parameter we say that szero is also weight vector and you are going to learn it along with  all the other parameters in the network does not make sense because you do not know  what szero means is a semantics of it is not clear like what was generated at the zeroth time  step we do not really know right so will just make it a learnable parameter and that  would be trained along with all the other parameters of the network    so before we move on what we are going to do is we are going to see a very compact  representations for rnns grus and lstms so remember rnn is the following  equation rnn is defined by the following equation the st is a recursive function of stone  and xt right so i am just going to write it as that st is equal to rnn of stone xt instead of  writing all these parameters and sigma’s and all that i am just going to write it  compactly as this now this is what what is this gru so how may going to write it as  students gru  gru of  students stone xt  stone xt  what is this  students lstm  lstm how may going to write it lstm of when the output of the lstm is both htone  and stoneright fine so in some sometimes i will just say st sometimes i will say both ht  minus ht and st as per whatever i needed right so this is i am not going to write these  equations and parameters again i will just say that lstm of this assume that is a  function which does this calculation and gives you back ok ok    so far what you have done is we have seen how to model the conditional probability  distribution given the previous t minus one words now let me give you a different  application right what if we want to generate a sentence given an image so this is what  i am interested in doing i am giving an image and i want to generate a sentence can we  just think of it formally what is it that you want to do so we saw that in this case  formally we were interested in this conditional distribution in this case what is it that  we are formally interest in  if i were to write it as something formal what would i write it as ok i will give you a  hint what kind of a distribution is this a conditional distribution right given the  previous sequences previous sequence of words generate the tth word now in this  situation can you stated an similar words given the  students image  image generate the  students sentence  sentence or given the image and the description that are generated so far because i am  going to write the description one word at a time given the image and the description that  have written so far generate the next word in the mission so what kind of conditional  distribution is that pyt given  students y one to t minus one  yone to t minus one  students comma  comma  students image  image does that make sense everyone gets that ok so what so this is what we want  right so here now we are interested in this quantity as a post to this quantity does that  make sense ok and this is again a conditional distribution     so earlier how did we model this we just modeled it as the following we said that the  whole context of yone to yt minus one is just contained in that blue vector which is st right  so remove this variable and replace it by a vector does that make sense ok  now you have the image also so how are you going to model this so what are you  going to write on the right hand side ok let me give you a hint we all agreed that this is  the quantity that we are interested in right we also agreed that the following is fine  replacing yone to t  one by st is fine now what about the image what do you mean by  objection in the image you will supply the words which are there the object names that  man fine that is fair enough well if want to make it more abstract more neural so what  you are saying is that whatever information is contained in the image should be passed  here  whatever information is contained in the image should be passed here how do you  what’s the way that you have learnt of computing the information in the image a dash  neural network  students refer time onefouronefive  a  students convolutional neural network  feedforward neural network  students convolutional neural network  convolutional neural network ok so but what from a convolutional neural network  how many representations that is a convolutional neural network learn how many does  v g g network learn v g g one6 the last layer is a softmax layer onefive right so which one  will you give now one before the last one that is called the  students refer time onefourthree6  dash layer  dash dash layer fully dash layer  students fully connected layer  fully connected layer ok at least your language moral works fine ok so that is the fully  connected layer remember that all the layers in the convolutional neural network learn  an abstract representation of the image and as she was trying to say that this abstract  representation contains or at least you believe it contains all the information that is there  in the original image just as st contains all the information that was there in the sequence  yone to t minus one this abstract representation that we will get from a cnn contains all the  representation all the information that is there in the image we all believe that ok  and we also believe that any of these representation is fine in practice the convention is  to use the fully connected layer that is called as f c seven the seventh fully connected layer  right and it is seven because you also start the numbering from the convolution layer one two three  four five and then the seventh layer ok so that is what you will take ok so does this make  sense and it is a very simple extension from what we were doing earlier this is what i  have circles is what we were doing earlier right where we only had st now i am saying  is just as you believe that st and codes all the information in the previous sequence   i am just asking you to stretch that a bit more and say that f c seven of the image contains all  the information that was there in the image is it fine     ok but still there are some issues and there are other ways of making this condition on f  c seven in particular what you could have done as she was trying to suggest initial is that  maybe you have a vocabulary of all the objects that are possible in your image right so  maybe in your image there is man woman there is flying desk frisbee or there is dog  cat and all these things right  so you do an object detection first get out all the object which are there and then make  the distribution conditional on these objects right so you can say that i will allow for a  onezero words to describe the image so there word one is equal to man because i have  detected the object man in the image word two is equal to frisbee because i have detected  the object frisbee in the image that is all that is another way of doing it ok so i just  want to make it clear that there are different ways of making the conditional distribution  conditional on the image itself we are choosing to make it conditional on f c seven of i right  that is the neural way of doing it ok    so let us see two such options the first thing that we could do is we could set szero to fcseven of  the image what is s zero the first thing that was passed to the language model ok so  remember we had this go symbol and we had this s zero which was mysterious we did not  know how it comes but now we know it that s zero could just be the image that is what my  starting point is so take this image and now start generating the representation  generating a description does that make sense ok so this is what the network looks  like  so what do you saying is that these things are of dimension d the cns output was say  of dimension fourzero96 so this has to be converted to size d right that means what will you  how will you do that we have a fourzero96 dimensional vector and you want to convert it to a  d dimensional vector w belonging to  students refer time onesevenfiveone  fourzero96 was d fine in general any two vectors if you want to make them compatible this is  what you will do you will project so that they are of the same dimensions x zero will be  the go symbol go is the special word in your vocabulary which says star generating the  sentence right so whatever vocabularies you will add two special words right one is go  and other is stop so whenever you generate stop you stop generating after that fine  what is the other way of so here now what happens is so this is what is happening  technically and that is why that is what i wanted you to understand this now s one depends  on szero ok what we are interested in is the following that yt should be conditional on yone to  t minus one comma image ok  they have make sure that i is s zero and this quantity is st you have to find now since the  first time step depended on the image all subsequent time steps will depend on the  image is that ok what is the other way of doing this what now in this looks slightly  inefficient what’s the other option that you could have used just feed the image at  every time right so that is the one constant thing that this is the image now whatever  you have generated so far considered that but in the addition to that also consider the  image    so what would the diagram look like just passing the input to every stage of the decoder  ok  i have already started using terminology which have not introduced but i will just  introduce it shortly    so let us look at what the full architecture looks like there is something known as the  encoder which takes your input encodes it and gives you a representation right then  you have something known as a decoder because given this input you want to decode  what the output is right so remember general terminology would be whatever input is  given to you you want to encode it and whatever is the output that needs to be decoded  right it is you could think of it that this is the image now i am trying to decode the  description for there is that fine ok and then you have an rnn which is used to  decode the sentence from this input  so such architectures are known as encoder decoder architecture and these are become  extremely popular and we will see why they are so popular and why they have led to the  popularity of deep learning in general ok so everyone understands this diagram anyone  who does not see a problem with this diagram there is actually no problem but i want  you i want you to see beyond the diagram and to look at the equations what do you  mean by that what do i have here as the input what is my x so this this looks fine i  have taken one box and connected it to another box and everything is fine right but that is  not what i am interested in  what am i interested in can you write the input as a the output as the function of the  input in this case is it possible to do that so that is what we need to make sure that we  are able to do right so we look at various applications suggest lepted criptical here but  i am going to come back to it so i just the emphasis that look beyond the diagram the  diagram looks very nice i hope it does thanks to the ta’s but it does and but we need  to understand what is the what is the set of equations being conveyed through this  diagram right what is the function that we are trying to learn we are going to write y  as a function of x are we able to write that function because now we are suddenly  thrown in a convolution neural network at some place we have an recurrent neural  network then we have the feed forward layer at the output which is the green vectors  so does all this combined together right   
task2/super_cleaned_audios/lesson113.wav,1011.5423, so we are going to see a lot of applications of the encoder decoder models  and for all these applications we are trying to answer the following questions what  kind of network can we used to encode the input in the previous application what do we  use cnn what kind of network can be used to decode the output what did we use  student rnn  rnn what are the parameters of the model we will see that and what is an appropriate  loss function right    so let us again go back to this task which was image captioning what is the input what  is the training data given to you what is x what is y x is the image y is the  description right so this is what is given to you given n such training pairs where x i is  the image and y i is the description and y i in itself is a sequence right so you have y i  one to y i capital t everyone gets the input and output  now what is the next thing model can you write down the model equations i want an  equation which starts from x and goes all the way up to y and since we have several time  steps i want an equation for y t this is generic for every time step right say can you write  that equation and feel free to use shortcuts so you do not need to write the entire rnn  equation just say rnn of something dont even try to write the vgg one6 cnn equations  just say cnn ok so we will go ahead  the first thing that i am going to do is i am going to write the equation for the encoder  so the encoder gives me cnn of x i whatever is the input given to me x i is the ith  training image given to me so i will just pass it through cnn i will get a representation  for that and i am just being cryptic here it could be the fc7 representation or the con five  representation or the max fool five representation or whatever you want right and it is  going to denote all of this as cnn of x i run this cnn take whichever representation  you want to take  now what is the decoder going to be decoder is the following rnn remember the  equation of rnn was stone comma x t what is the input of the tth time step whatever we  are predicted at the previous time step just the embedding of that so e means  embedding if you want take one odd embedding if you want take word  to vec  embedding is that fine ok   and then what is the output it is the soft max function of the following how many of  you get this now please raise your hands how many of you can say that y can be written  as a function of x is that pretty straight forward ok so you have an encoder you have a  decoder and remember that this final y is a composite function of the original input x ok  just that you are doing too many computations along the way but there is a path which  exists ok  what is the loss function everyone at this point should be able to say it  student sum of cross entropies  sum of cross entropies they just wait for me to say two more sentences what are the  parameters u v w b c  student refer time zero3onetwo  a b c d e f laugher everything right what is that all the parameters of the  student refer time zero3one7  convolutional neural network that means all the filters that you have all the parameters  of the rnn which is w u and the parameters of the output layer which is v right all of  these is that fine i am i may have missed some biases but ok the objective function as  you said is a sum of cross entropies where l t is the true character at time step true word  at time step t and what is the algorithm that you are going use back propagation through  time and with back propagate all the way through the cnn also which is an end to end  thing in practice of course you do not do that yes you could just said both to be the  same do you get that question is that ok ok    now let us look at another task we look at the task of textual entailment what textual  entailment does is that i give you a input or premise the premises that it is raining outside  and you need to tell me a hypothesis the hypothesis that the ground it is wet ok with  basically means that it is raining outside implies that the ground is wet ok now what is  the encoder decoder architecture that you will use for this problem what is the input  here   student a sequence  a sequence what is the output sequence it is all the hint that i am going to give you  so what will you do what is the encoder equation going to be  student rnn  rnn what is the decoder equation going to be  student rnn  rnn how will it become end to end by setting what to what  student refer time four hundred and forty-nine  szero of the decoder to to what of the encoder  student refer time four hundred and forty-nine  last time step of the encoder how many if you get that really we are on the same page  first time in i do not know how many lectures by finally it happened so here is what  training data is right it is a collection of premises and hypothesis and you have n of  these there are two options for the model the first option is that you encode the input  using an rnn feel free to replace and by an lstm if you want then you have the  decoder where and you set the zero time step to whatever you got from the encoder  then every time step you computed using the rnn where remember the input at every  time step is whatever you predicted at the previous time step and then the output is just  the soft max function is that fine and what is the loss function going to be loss  function  student refer time zerofive44  sum of cross entropies training algorithm  student back propagation  back propagation through time and really it is through time right all the way back ok so  we will see that let me see if i had any other question ha ok i will ask it parameters i am  not going to bother about ok now this was option one i have just clearly written what is  option two what is the set of equations look like for option two which of these equations  will change and how remember option two was maybe pass the input at every time step  which equation will change   st what will it become but s t can take only i mean rnn take only two inputs right s  t minus when you need to gave embedding you need to gave so how will you fit in the  third input this animation has it is own mind so this is how back propagation will  happen right so let us it is finish that so will actually back propagate all the way back  through time fine really all the way back through time    and same task textually entailment i want model two option two so this is what will happen  we will just concatenate h capital t which is this guy along with the input at every time  step right how many if you get this the rnn is still taking just two inputs one is the  previous state the other is the concatenation of the current input as well as input that we  got from the encoder everyone get this ok so this is model two i am going forward i am  not going to do both model one and model two it is model two is just a very simple variation of  model one a parameters loss function training algorithm everything remains the same ok      let us look at machine translation what is the input an english sentence what is the  output a hindi sentence what is the encoder going to be  student rnn  rnn what is the decoder going to be  student rnn6rnn what is the loss function going to be  student refer time seven hundred and thirty-seven  soft max who said soft max  student refer time zero74two  what is the loss function going to be  student sum of  sum of cross entropies training algorithm all the way through time right ok so let us  can you draw can you write the equations just copy it from the previous slide right  actually copy it from the previous slide right if you have the rnn you have the rnn as  a decoder again in option when you will set s zero to h t you have the loss function the  parameters and your training algorithm ok and for option two it is back propagation will  fine and for option two what will happen option two what will happen  student refer time zero8one7  this will change right so just focus on that we just passed in the last time say belong  with that     now transliteration what is transliteration what is transliteration if you do not know it  at least see it from the example what is it  student refer time zero834  writing the same word in another language right so this is typically done for named  entities very when you are when you are translating from one language to another you  do not often for thomas you do not come up with an indian translation right you just say  thomas in devanagari right you just right thomas in the devanagari right so for  names you typically just do a transliteration that means from the english script you just  write it in the native language script ok what is the input one word the input is the  word right what is it a sequence of  student characters  characters what is the output  student sequence of characters  a sequence of characters what will you use for the input  student rnn  rnn what for the output  student rnn   rnn so i will becoming too easy right can you write the equations for this yes you  will copy it from the previous slide yes ok everything remains the same right so you  see why this framework has become so powerful you do not see it still maybe let us look  at something else     image question answering tell me what is the data here what is the input image and  student question  question and what is the answer what is the output answer so for simplicity we are  going to assume that the answer here is a finite vocabulary we are not generating  descriptive answers we are not being overly dramatic let us going to say one word what  is the colour white we are not going to write i think the colour of the image is white  now ok just white so all these outputs are going to be single words and we have v  possibilities and we are going to predict one of those v possibilities ok  now give me a model for this now things are getting slightly complicated you have  one image as the input one sequence as an input and a dash as the output oh god now  think why would you generate the sequence of characters as the answer i said that the  answer is going to be come from a finite vocabulary that means you need a  student probability distribution  a distribution probability distribution is here enough said now tell me what is the  model a model should connect the input to the output you have two inputs here i see  some people doing this laugher i do not know what that means but let us just do it let  us make a train simple formula simple recipe and whatever input you are given just  encode it depending on the type of input you know what is the encoding is going to be  for images what is encoding sequences  student refer time onezerofivefive  now what do you do with these two separate things  student refer time onezerofive8  concatenate them ok and then  student refer time oneonezeroone  after that  student refer time oneonezerotwo  can you think of all the equations can imagine all the equations along the way  student yes  of course yes laugher right i mean imagination laugher you can always do that  laughter now just think about it can you write the output as a function of the input  where the input is actually a pair now it is image comma question what is the model  going to look like let us see so model will first have an encoder for the image let us call  that as ˆ h i it is going to have an encoding of the question let us call it as it as h  i am  going to concatenate these two as someone rightly gestured and then what am i going to  do after that pass it through a  student feed forward network  feed forward network and predict a probability distribution what are the parameters of  this network parameters of the feed forward network the parameters of the recurrent  neural networks and the parameters of the cnn right so everything that we have done  so fine because ok how do you train it back propagate through time and space ok also  go back to the image also fine is that ok ok    document summarisation what is the input sequence what is the output rnn rnn  everywhere fine i will not even bother to ask you     video captioning sequence of images i want to hear the choice of phrasing that you use  i just want to hear that i love hearing that every time  student refer time onetwo3zero  rnn of cnns whatever that means what rnn of cnn every time i do this everyone  says rnn of cnn laugher i do not know what that means but it is the right answer  what does it mean what is the video it is a sequence what is the sequence of  student images  images so what will you do encode every image and then pass it through a  student rnn  rnn can you imagine the equations ok let us see ok and in this case what is the output  again the sequence so what is the decoder going to be  student rnn  so here is the model so first what you do is for every time step you compute the cnn  encoding of the frame then you pass it through an rnn to get the final time step t and  then you feed it to a decoder and generate one word at a time is that fine so this thing  apparently is called rnn of cnns ok and so that is and loss function would again be  the same sum of cross entropies and back propagation through time and space ok good  please do not quote me on this thing also this is getting a recorded but ok    the next one video classification what is the decoder decoder is probability  distribution okay what is the decoder  student feed forward neural network  feed forward neural network     ok this one dialogue how are you i am not fine ok input  student rnn  output  student rnn  right so you see this why this has become so popular we took a wide range of  problems different modalities right we took images we took videos we took sequences  a combination of these right image question answering has a combination of images  and sentences and the output you have a probability distribution all of this could be  model by this unified end to end network all of the components are neural network  based components whether it is a convolutional network or a feed forward network or a  recurrent neural network right  now let me stretch this right what if you have video question answering what is the  input going to be sequence of images and sequence of  student words  words the output is  student refer time one446  no just a word right we will pick from a fixed vocabulary how are you going to model  the input  student refer time one4five3  rnn for the question rnn of cnn for the video then  student concatenate  concatenate and  student feed  feed it to your feed forward network right so all of these become to what extent that  work is a separate question but all it was not even possible to model all of this as an end  to end network right but now i just because possible to model it as an end to end  network with all the components being neural components right and this is this story  still not complete everything is not as easy as it looks they still and very crucial  component that we have missing in the architectures that we have seen so far and we will  talk about that soon ok    now let us just continue that i challenge you to do this pick up any problem there are  student from relevant different departments pickup any problem do not say that i want to  design some gear for certain aeroplane and all that and i want to use neural network to do  so no something which involves machine learning right not problem is does not  involve machine learning and see if you can model it using the encoder decoder frame  work just try to do this  take problem from biotech right for example they given a sequence of genes and you  want to predict whether this person is susceptible to a certain disease what will you do  conduct a blood test ok do not do not laugher do not go and do neural networks for  that but if you had to do that this is what you will do it will take a sequence you will  treat the sequence of sequence at the given dna as a sequence of genes and then you  will try to predict something as the output try to predict a probability distribution over  disease it a possible right  so you can think of many applications from many domains and all of that you could  problems involving machine learning with potentially model than using the neural  encoder decoder architecture but there is a very important part missing from this whole  story which is attention which is a very important idea and we will spend some time on  that in the remainder of the lectures so we will first motivate why do we need attention  and from there we will see that how do you make how do you integrate attention with all  these encoder decoder architectures that you have seen so far   
task2/super_cleaned_audios/lesson107.wav,375.9829,before that i will just go into some more gory details about the math this is not to  scare but just to make you more comfortable that we can actually deal with something  which is not very straightforward or very neat as compared to what you are seen so far  so i just go back to the formula which i had for computing the gradient of the loss  function with respect to w and i cannot repeat enough times that all these notations are  actually a bit of abuse of notation because these are gradients and not partial derivatives  so i should actually be using this notation but for ease of explanation i use i stick to  my original notations ok  so now let us look at each of these quantities here and tell me the dimensions of these  quantities let us start with the left hand side what is the dimension of this w was what  matrix what is i am talking about the circle entity what is the magnitude what is the  dimension of that k  d  student refer time one hundred and nine  d  d  student refer time one hundred and eleven  d  d someone n  d  student refer time one hundred and thirteen  n  d and n  k are the two options which are left w is the recurrent weight so w is  what dimension  student refer time one hundred and twenty  d  d  so what is this gradient d  d ok what about this fast st what the hidden  representation so that was d dimensional so what is this d one ok what about this  why do you guys still struggle with this  student refer time zeroone4two  d  d and this d cross  student refer time zeroone44  d cross it is very straightforward right what is the dimension of numerator what is the  dimension of denominator thats all right     so you see the kind of multiplication that you are doing here say of d cross doned d   d and then d d  d ok let us look at each of these quantities and see if you are actually  comfortable in implementing these are you comfortable with this the loss function  with respect to the hidden representation we have done this enough times in back  propagation what about this we just saw a formula for this right so we know how to  compute this quantity we have seen this in back propagation this is the derivative of a  scalar with respect to a vector and we are very comfortable in computing this  this was slightly tricky but we just derive this formula on the previous slides everyone  okay with that what about this this is a tensor how do we compute this tensor what is  our standard recipe focus on  student refer time two hundred and thirty-eight  the little guy one element of this tensor and then you can generalize somewhere right so  this is the tensor and we will just see that this just to make you all comfortable is this not  like just to intimated you with all these large sized tensors but i am just trying to show  that this is all easy this is not hard ok so how do we compute this all the other terms  are covered  this is the only one that we do not know     so we will just look at one element of this tensor and it is going to be skpwq r  so let us just see that you have s k as this vector and you have w as this matrix so i  am considering one such weight which is w p comma q and one such element from  here which is s k sorry so q r and i am considering one element from this which is s k  p so i am trying to compute the derivative of one element of the vector with respect to  one element of the matrix so this is going to give me one entry in my tensor and that  entry is going to be what p q r how many of you are fine with this ok fine    so now recall that a k was equal to w into s k minus one plus b and s k was sigmoid of a  k i think again i have miss that u into x k but that will not matter because that is not  there in the derivative ok you are fine with so far    so now let us look at this because the other two terms do not matter so i just look at a k  is equal to w into s k minus one so this is the matrix way of writing it ok now i am  looking at one of these elements which actually comes from the multiplication of a row  and a column the highlighted row and the column everyone gets this ok  now so i can write it as a k p is actually equal to this summation which is nothing but  the dot product of this row with this column ok now s k p is just the sigmoid of that so  now if i want to compute s k p with respect to w q r i can just write the chain rule that  s k p with respect to a k p which is straightforward and then a k p with respect to w q r  and i already have a formula for a k p how many of you are fine so far please raise  your hands high up if you are fine ok so what is the first term going to be sigma  student refer time zero5onezero  sigma prime of a k p and what is the second term going to be this is what the second  term is now what this is lot of terms here which of these terms would actually remain  only the once where only the terms where i is equal to  student refer time zero5two8  r and  student refer time five hundred and thirty  p is equal to q so only that term will remain in that case it would be this right and in  the other cases going to be zero right so now you have one element of this tensor and you  have it as a very generic formula you can just fill in all the elements of the tensor right  so what does this tensor look like it is a very  student refer time zero55two  sparse tensor right that is all i wanted to convey ok so this is again the same thing  right is that fine so even though it is a nasty looking tensor if we just break it down to  one element it is going to be very easy and now from this element you can just  reconstruct the entire tensor do not worry i am not going to ask you to implement this  but if someone were to maybe at some point then you should be able to do it right that  is where we will end today   so we have finished recurrent neural networks and the next thing that we are going to  look at is lstms and gated recurrent refer time zero6two5 ok  thank you  
task2/super_cleaned_audios/lesson38.wav,758.6287,tips for adjusting the learning rate and the momentum  so before moving on to these slightly advanced optimization algorithms we will revisit  the problem of learning rate in gradient descent    so one could have argued that we could have solved this problem of this slow  movement on the gentle slope by increasing the learning rate remember that we have  this eta and we deliberately chose to be conservative that we will take a small value for  the eta but what if i just blow up the eta i could just take a very large eta what would  happen it will overshoot right  so what will happen is i will see what happens when i take eta equal to ten ok so so i  will see what happens when i take eta equal to ten    so this is stepone step two step three its moving very fast on the regions where the slope is  gentle but it also moves very fast much faster on the regions where the slope was  already steep  so when the gradient was actually high you ended up blowing it further by multiplying  it with the eta which is ten so it is again going to have this effect that you will move  much faster in the steeper regions and again you will see these oscillations because you  will overshoot your objective does that make sense right so it is not that you can  always choose a high eta and get away with it  so what do you actually want what is your wish list regulate theta you want a adaptive  eta right that it somehow figures out that i am on a gentle slope so i should move  slowly i should move fast and i am now on a very fast loop so i should move slow so  this having this one eta is not working for every point on the error surface right for  everywhere on the error surface is that clear ok so ok so we will see such algorithms  soon where we try to adjust this learning rate    now here are some tips for the learning rate so how do you if you are just going to  deal with this gradient descent or nag or momentum how do you adjust these learning  rate so how do you fix a learning rate so a learning rate is typically something known  as a hyper parameter so why is it called a hyper parameter so what are your  parameters  student which i learned  which i learned using the objective function eta is not a part of the objective function  you are not computing radians with the respective to it is a hyper parameter so you will  try to tune this hyper parameter so what you will do is in practice you could try these  different values on a log scale next what will you do run this all these for a few  epochs note down the dash just note down the loss function  so run all of these with different learning rates for say five epochs you will get some loss  right now which one will you pick the one which led to the maximum decrease in the  loss i will keep that learning rate and now what you will do you just stick to that i  started off with a dash scale  student log scale  log scale now what will you do ok so now run it for a few epochs figure out which of  these learning rates on the log scale works well now do a finer search around the best  learning rate that you discovered right so say zeroone was the best on the log scale so  now look at zerotwo zerothree four zerofive look at values around it and see which one works better  so this is how you will tune the hyper parameters otherwise there is a very wide range  right if you put tune from zerozerozerozeroone to zeroone there are just too many values to consider so  we will have to do this log scale and then a linear scale will that make sense  these are just heuristics there is no guarantee that will always work or which of these  are clear winner strategy but you have to try this so tuning a learning rate is an  important part when you are working in deep learning so at least when you are  working with gradient descent or nag or momentum based gradient descent    now here some tips for annealing the learning rate so there is something known as  step decay so what you can do is halve the learning rate after every five epochs can you  tell me the intuition for this what do you expect after five epochs that you have moved  enough and now you are closer somewhere to the solution so if i closer to the solution  if i closer to phoenix market city you want to move fast or slow  student slow  what will you do  student refer time fourthreetwo  decrease the learning rate right so after every five now this is again what is so sacrosanct  about five it is just a magic memory so this is again hyper parameter so you could fix  some number of epoch and after these i will just halve the learning rate ok now this  second one is what my favourite is and i typically use this what i do is i compute the  loss after epoch t i run epoch t plus one i compute the loss again if the loss has increased  what will i do i will just throw away all the updates that i have made in this epoch i  will decrease the learning rate and again learn again start this epoch what do i mean by  throw away all the updates  student refer time zerofiveone7  so after epoch t i will save my model i will save all the w values that i have computed  and i will let it run for one more epoch after this epoch if my loss function actually  increases i reload this model which i had saved half the learning rate and then run this  epoch again does that make sense so i have run till epoch t i have some values of w’s  and b’s i will save this values i will just save it as a numpy array  now i will with the same learning rate that i have been using so far i will run the epoch t  plus one ok and i get some new values of w comma b right i will plug this into the loss  function i will plug this into the loss function i will get two loss values if this loss value  is greater than what i was at the previous time step that means things did not work out  well in this particular epoch  so i will throw away all these updates i will just reload the model which i had saved i  will just start from where i was at epoch t i will decrease the learning rate i will make it  half and run this epoch again right and hopefully now i should do better because there is  something i am just making a hypothesis that the reason i did not get to a better loss  function was because my learning rate was not adapting to it  so i will just halve the learning rate because this solution was good this was a low loss  function i just want to be something around it i do not want to make any drastic steps  so i will just half the learning rate from there so then you not see this drastic change  that your loss function should not improve so first of all local minima is known  problem in deep neural networks  so what happens is that in deep neural networks you do not have something which is  like a neat convex function as your loss function right it is a non convex function which  means there is no one unique minima there could be several minima and there are  several analysis which show that a lot of these minima are equivalent so in practice  these are the things that you do either once you reach a minima you just stay there the  second thing that you could do is you have trained your algorithm trained your  parameters for say tenzero epochs and you have stopped now  now again go back and start with a different initialization you started with some w  naught b naught and you have reached to some solution keep this solution now start  with a different initialization that means if you look at your wb plane you have started  from some other point that means you started from some other error location right and  run this algorithm again and see if you reach a different minima  so the only thing you the way you counter this is you just try different stochastic things  right should try to start with ten different initializations every time reach a minima and  then at the end select the lowest possible of these did this make sense to most of you  how many of you got this oh cool i thought i was just rambling but yeah fine does that  make sense to you at least ok does it fine  yes a local minima is a severe problem in lot of deep learning optimization and typically  people get away by that by just picking up one of these minimum fine now the other  thing is you could use exponential decay where with each time step you just keep  decreasing your learning rate and if this case two that means at every time step you are  halving the learning rate so you just get with something like this  but the reason i do not like this is that you have one hyper parameter which is eta which  you are trying to tune and now to tackle that problem you have introduced one more  parameter which is k hyper parameter which is k so it becomes harder to tune that  now and there is a similar thing which is one by t d k where you try to use this formula to  decay or learning rate so both of these i typically do not use in practice i use the  second one i prefer the second one    now tips for the momentum can you make sense of this you just stare at it it looking  just come back ok let us see what happens at t equal to zero this becomes zero  student log one  log one is zero this is two raise to minus one minus zero which is just two raise to minus one which is zerofive  so what is your mu t at t equal to zerofive does that make sense is it fine with everyone or  is it confusing no ok mu max is typically this let us assume mu max  now what happens at time step twofivezero this is twofivezero by twofivezero so this becomes one one plus one is two  the best thing that you learn in this course log of two is one so this become two raise to  student two raise to minus two  minus two which is zerotwofive so what is this  student zero7five  zero7five let us do one more i had t equal to 7fivezero one minus one by eight so that is what is going to be  right ok so then what is happening as my time steps are increasing what is happening  to what is happening i am having more and more faith in the history or the current  gradient what am i increasing actually i have made a mistake actually this is mu is  gamma there is not we did not use mu anyway what you guys just went along so this is  gamma actually right that was a momentum term that we had so as a number of time  steps is increasing my gamma is increasing that means i am having more and more  faith in my  student refer time oneone49  no history learning rate is eta momentum is gamma so its gamma into update t minus  one and eta into gradient at the current time step right and here gamma is actually equal to  mu is there any more confusion that i can add so when i say gamma i mean mu and so  that is how it is so as i am increasing the number of time steps i have more and more  faith in the history that means i do not want to now get distracted by this one update  which i am making right i want to go by the history and i am not increasing this gamma  or mu indefinitely i am capping it by a max right max i will have this much faith which  is zero999 in the history does that make sense this is again just a heuristic do not worry  too much about it so that is how it is   
task2/super_cleaned_audios/lesson10.wav,410.0622,algorithm and convergence multilayer perceptrons mlps representation power of mlps so welcome to lecture two of cs seven thousand and fifteen which is the course on deep learning so we will talk about mcculloch pitts neuron thresholding logic perceptrons and a learning algorithm for perceptrons and talk about the convergence of this algorithm and then we will talk about multilayer network of perceptrons and finally the representation power of perceptrons so  let us start module one which is on biological neurons so  remember during the history we had started all the way back in the one880s when we spoke about biological neurons so we will just start there spend a few minutes on it and then go on to the computational models which is mcculloch pitts neuron so now this is a course on deep learning so we are going to talk about deep neural networks now the most fundamental unit of a deep neural network is something known as an artificial neuron and the question is why is it called a neuron where does the inspiration come from so we already know that the inspiration comes from biology and more specifically it comes from the brain because we saw that way back in the one890s this term neuron was coined for neural processing units or the cells in our brain so now before we move on to the computational neurons or the artificial neurons we will just see the biological neurons in a bit more detail and then we will move on from there so this is what a typical biological neuron looks like so  here actually there are two neurons this portion is called the dendrite so it is used to receive inputs from all the other neurons so that is the place where the input comes in then remember we said that in one950s we discovered that these neurons are actually discrete cells and there is something which connects them so that connection is called a synapse and it decides the strength of the connection between these two neurons so there is an input there is some strength to the connection then once this neuron receives inputs from various other neurons it starts processing it so that is the central processing unit which is called the soma and once it is done this processing it will it is ready to send its output to other set of neurons so that output is carried on by the axon so we have inputs we have some weights attached to the input we have some processing and then an output so that is what a typical biological neuron looks like and let us see a very cartoonish illustration of how this works right how the neuron works so our sense organs interact with the outside world and then they pass on this information to the neuron and then the neuron decides whether i need to take some action in this case the action could be whether i it should laugh or not right whether the input is really funny enough to evoke laughter so  if that happens this is known as something that the neuron has fired now of course in reality it is not just a single neuron which does all this there is a massively parallel interconnected network of neurons so you see a massive network here now the neurons in the lower level site so these neurons they actually interact with the sensory organs they do some processing based on the inputs so they decide whether i should fire or not and if they fire they transmit this information to the next set of neurons and this process continues till the information is relayed all the way back and then finally you decide whether you need to take any action or not again in which this case it should be laughter so  that is how it works and when i say massively parallel interconnected network i really mean it because there are one0 raise to eleven which is roughly one00 billion neurons in the brain now this massively parallel network also ensures that there is some division of work now what do you mean by that is not that every neuron is responsible for taking care of whether i should laugh or not or not every neuron is responsible for processing visual data some neurons may possess visual data some neurons may possess speeds data and so on so there is this division of work every neuron has a certain role to play so for example in this cartoonish example that we took so there might be this one neuron which fires if the visuals are funny right whatever you are seeing is funny there will be one neuron which finds sheldons speech to be funny the way he speaks so  that might be funny and there might be another neuron which actually  finds  the dialogue content to be funny and now  all  of this  pass on the information to the next level and this guy would fire if at least two of these three inputs are funny so that means i have some threshold based on which i decide whether to react or not if it is really funny then only i laugh it otherwise i will not laugh so the neurons in the brain as was obvious in the previous slide are arranged in a hierarchy and i will take a more realistic example where we look at the visual cortex so is  this  is  the  portion  of the  brain  which  is  responsible  for processing visual information right so as you see here you have our retina from where the information starts flowing and it goes through various levels so you see you follow the arrows and you will see there are several levels there is one level here then another here another here and so on right so it is again as i was trying to illustrate in that cartoon the information is relayed through multiple layers and then it goes all the way back to the spinal cord which decides that in this case i need to move the muscle right so that is what is being decided here right so the information  flows through a hierarchy of layers and in this particular case i am going to focus on these three circled layers which are vone vtwo and ait right so these actually form a hierarchy and let us see what this hierarchy does right so at layer one you detect edges and corners so i am looking at you all i just see some dots and some shapes so that is what layer one recognizes i just recognize some edges and some dots and so on now layer two tries to group all of these together and come up with some meaningful feature groups right so it realizes oh these two edges actually form the nose these two dots actually form the eyes and these two edges actually form the mouth right so that is slightly higher level of processing that it is doing and then layer three further collects all this and leads to higher level objects right so now it is realizing all these things put together is actually a human face right so you add edges and circles or dots then you had some feature groups and then the feature groups combine into objects right so that is how this hierarchy processes so here is a disclaimer i understand very little about how the human brain works right and what you saw is a very oversimplified explanation of how the brain works right what i told you is there is an input a layer of networks which does a network which has many layers which does some processing and then you have an output right that is the very simplistic view that i gave you this is an oversimplified version but this version suffices for everything that we need for this course right this is not a biology or a neural processing course right so it is enough for this course so that is where we will end module one 
task2/super_cleaned_audios/lesson12.wav,617.5353,"now let us go to the next module which is perceptron   so far the story has been about boolean input but are all problems that we deal with we are only dealing with do we always only deal with boolean inputs so yeah so what we spoke about is boolean functions now consider this example this worked fine for a movie example where we had these as actor so much and his director and so on but now consider the example where you are trying to decide you are in oil mining company and you are trying to decide whether you should mine or drill at a particular station or not  now this could depend on various factors like what is the pressure on the surface on the ocean surface at that point what is the salinity of the water at that point what is the aquatic marina  aquatic  life at that point and so on so these are not really boolean function the salinity is a real number density would be a real number pressure would be a real number and so on right and this is a very valid decision problem companies would be interested in doing this so in such cases our inputs are going to be real but so far mcculloch pitts neuron only deals with boolean inputs so we still need to take care of that limitation  now how did we decide the threshold in all these cases i just asked you you computed it and you told me right but that is not going to work out i mean it does not scale to larger problems where you have many more dimensions and the inputs are not boolean and so on so we need a way of learning this threshold  now again returning to the movie example maybe for me the actor is the only thing that matters and all the other inputs are not so important then what do i need actually i need some way of weighing these inputs i should be able to say that this input is more important than the others now i am treating all of them equal i am just taking a simple sum  if that sum causes a threshold i am fine otherwise i am not fine but maybe i want to raise the weight for some of these inputs or lower the weight for some of these inputs so whether it is raining outside or not maybe does not matter i have a car i could go or i could wear a jacket or an umbrella or something so that input is probably not so important and what about functions which are not linearly separable we have just been dealing with the goody stuff which is all linearly separable  but  we will see that even in the restricted boolean case there could be some functions which are not linearly separable and if that is the case how do we deal with it so these are some questions that we need to answer  so first we will start with perceptron which tries to fix some of these things and then we will move forward from there so as we had discussed in the history lecture that this was proposed in one thousand, nine hundred and fifty-eight by frank rosenblatt and this is what the perceptron looks like do you see  any  difference  with  the  mcculloch  pitts  neuron  weights  you  have  a  weight associated with each of the input otherwise everything seems  so this is a more general computational model than the mcculloch pitts neuron the other interesting thing is that of course we have introduced these weights and you also have a mechanism for learning these weights so remember in the earlier case our only parameter  was  theta  which  we  are  kind  of  hand  setting  right  but  now  with  the perceptron we will have a learning algorithm which will not just help us learn theta but also these weights for the inputs  how do i know that actor is what matters or director is what matters given a lot of past viewing experience past given a lot of data about the movies which i have watched in the past how do i know which are the weights to assign this so we will see an algorithm which will help us do that and the inputs are no longer limited to be boolean values they can be real values also so that is the classical perceptron but what i am talking about here and the rest of the lecture is the refined version which was proposed by minsky and papert  which  is  known as  the perceptron  model  so when i say perceptron i am referring to this model so this diagram also corresponds to that  so now let us see what the perceptron does  this is how it operates  it will give an output of one if the weighted sum of the inputs is greater than a threshold so remember that in the mp neuron we did not have these weights but now we have these weighted sum of the inputs and the output is going to be zero  if this weighted sum is less than threshold not very different from the mp neuron  now i am just going to do some trickery and try to get it to a better notation or a better form so is this i have just taken the theta on this side now is this notice this here the indices were one to n now i have made it zero to n and the theta is suddenly disappeared so what has happened  student w zero is  minus theta right and xzero is one does anyone not get this right if i just start it from one to n then it would be summation i equal to one to n wi xi plus wzero xzero but i am just saying wzero is equal to minus theta and xzero is equal to one which exactly gives me back this right so very simple xzero equal to one and wzero is equal to minus theta  so  in effect what i am assuming is that instead of having this threshold as a separate quantity i just think that that is one of my inputs which is always on and the weight of that input is minus theta so now the job of all these other inputs and their weights is to make sure that their sum is greater than this input which we have does not make sense so this is how this is the more accepted convention for writing the perceptron equation so it fires when this summation is greater than equal to zero otherwise it does not fire  now let me ask a few questions so why are we trying to implement boolean functions i have already answered this but i will keep repeating this question so that it really gets drill in why do we need weights again we briefly touched upon that and why is w naught which is negative of theta often called the bias so again let us return back to the task of predicting whether you would like to watch a movie or not and suppose we base our decisions on three simple inputs actor genre and director  now based on our past viewing experience we may give a high weight to nolan as compared to the other inputs so what does that mean it means that as long as the director is christopher nolan i am going to watch this movie irrespective of who the actor is or what the genre of the movie so that is exactly what we want and that is the reason why we want these weights  now wzero is often called the bias as it represents the prior so now let me ask a very simple question suppose you are a movie buff what would theta be zero i mean you will watch any movie irrespective of who the actor director and genre now suppose you are a very niche movie watcher who only watches those movies which are which the genre is thriller the director was christopher nolan and the actor was damon then what would your threshold be three high in this case i always ask this question do you know of any such movie always takes a while interstellar so the weights and the bias will depend on the data which in this case is the viewer history so that is the whole setup that is why you want these weights and that is why you want these biases and that is why we want to learn them now before we see whether or how we can learn these weights and biases one question that we need to ask is what kind of functions can be implemented using the perceptron and are these function any different from the mcculloch pitts neuron so before i go to the next slide any guesses i am hearing some interesting answers which are at least partly correct   so this is what a mcculloch pitts neuron looks like and this is what a perceptron looks like the only difference is this red part which is weights which has added so it is again clear that what the perceptron also does  is it divides the input space into two halves where all the points for which the output has to be one would lie on one side of this plane and all the points where which the output should be zero would lie on the other side of this plane so it is not doing anything different from what the perceptron was doing so then what is the difference you have these weights and you have a mechanism for learning these weights as well as a threshold we are not going to hand code them so we will first revisit some boolean functions and then see the perceptron learning algorithm  so now let us see what does the first condition this condition if i actually expand it out then this is what it turns out to be and what is that condition telling me actually w naught should be less than zero clear so now based on these what do you have here actually what is this a system of linear inequalities right and you know you could solve this you have algorithms for solving this not always but you could find some solution and one possible solution which i have given you here is wzero is equal to minus one wone equal to oneone and w2 equal to oneone  so just let us just draw that line so what is the line it is oneone xone plus oneone x2 is equal to one that is the line and this is the line and you see it satisfies the conditions that i have is this the only solution possible no right i could have this also as a valid line if i could draw properly right all of these are valid solutions so which result in different wone w naught and w zeros so all of these are possible solutions  in fact i have been telling you that you had to set the threshold by hand for the mcculloch pitts neuron  but  that is not true because you could have written similar equations there and then decided what the value of theta should be so you could try this out for the mcculloch pitts neuron also you will get a similar set of conditions or i mean similar set of inequalities and you can just say what is the value of theta that you could set to solve that  "
task2/super_cleaned_audios/lesson111.wav,1373.6356,"ok so will start from where we left off so in the last class we started with this  motivation that recurrent neural networks have this problem of vanishing and grade  exploding gradients and we wanted to arrive with some principle way of avoiding this  so you have first started with this intuition that in many real life situations like for  example the human brain or the whiteboard we tend to these to these three operations  called selective read selective write and selective forget and they essentially help us in  dealing with these finite sized memories right or whether it is a whiteboard which is  finite sized or your brain or whatever it is right  so can we is it possible to kind of improve rnn’s which also suffer from this problem  that they have this finite sized memory and hence if you are trying to capture  everything from time step one then by the time you reach time step t where say t is thirty or  forty or so on its quite natural that whatever you have learned earlier will get move off to  an extent that it just is not recognisable anymore right  so you wanted to deal with this problem and with that we motivated selective read write  and forget and then we introduced some equations or converted this into a model and  this is the diagram that you see is the model actually that is the lstm cell yeah and it  has these three gates output gate input gate and forget gate and which perform these  three functions of selective read write and forget so intuitively all these was fine but  we need to be more technical in terms of you trying to deal with a problem of vanishing  and grade exploding gradients  so how does it solve that problem all that makes or the story seems fine but how does  this actually relate to the math so we saw some intuition for that and the intuition  hinsed on this observation that during forward pass the gates control how much of  information passes from one state to another and in particular if you have the situation  that from one time step to another say the forget gate tells you that keep forgetting point  five of the previous state then by the time you reach say the one hundred state you would have  forgotten zerofive raise to one hundred of the first state  so that means even during forward pass the information from state one vanishes so if it  vanishes during backward path that is also fine because state one did not contribute to state  one hundred and that was the intuition that all this hinsed on now we are not going to do much  different from this intuition we just going to see the corresponding equations for these  intuitions and just make a more i would not call it rigorous but more mathematical proof  on why lstm solve the problem of vanishing gradients  and we are also sure that they actually do not solve the problem of exploding gradients  and then we will see a simple trick of dealing with exploding gradient that is what we  will do in the remainder of this particular lecture and then will move on to the next  lecture in this lecture    so we will now see an illustrative proof of how the gates control the flow of gradients  right    so we call that this is the control this is the flow diagram or the dependency diagram  that you had for rnn’s and in particular because you are dealing with an ordered  network we add this explicit and implicit derivatives and finally you came up with this  multiplicative form and this term here is actually a matrix because it is a derivative of a  vector with respect to a vector  and then this same matrix was getting multiple times and then we did this proof it  showed that this term is actually lambda gamma ok it is actually proportional to this  term right and as if lambda into gamma is greater than one then this will explode if it is  less than one then it will vanish given sufficient times that is    now in particular what is happening here is the following that you have this loss at time  step t you have the time step is four now what if this loss or this error occurred because  w was not good enough to compute a good value for sone right so w was at a certain  configuration based on that you computed sone and that sone was not good enough which  eventually led to the error at time step four all of you if you can imagine this situation that  you mean you not being not be able to do something well at sone now this needs to be  told to w so that it can improve right and that information has to come through sone that  information is already going from here but this information is about how badly it  performed in computing sfour  this is not how badly it can perform in computing sone so that information has to travel  to w all the way through sone and that was not happening because this path do not look at  the bullets this path was actually vanishing and that is what this multiplicative term says  that as the number of times that increased that time that path would vanish ok so that is  the actual problem that we are trying to deal    so now what is the general situation here right the general principle is that the gradient  of l theta at particular time step say here we are considering lfour so i will just call it lt  with respect to any parameter theta i the parameters at w u v b and c with respect to  any parameter it would vanish if all the paths leading to that parameter if it vanishes so  with respect to this particular path so that is the only path which leads to w through sone  if there were multiple paths if there was say one such direct path right if we had you  some other kind of connection which gave us this direct path then it would still have  been fine  but there was only one path leading to w through sone at the gradient vanishes along that  path then the gradient will vanish ok if there were multiple paths then only if the  gradient varnishes across all the paths then the gradient would vanish is it fine what is  the corresponding rule for exploding gradients if there are multiple paths the gradient  would explode if  student refer time six hundred and six  if it vanishes through any one if it explodes though any one of the paths ok    so these are the two things that we need to consider ok so to prove that in the case of l  st m this does not happen for the first case will have to show that there are at least one  path through which it does not vanish and for the second case because we are going to  show that it explodes we just have to show that there is at least one path through which it  can explode ok so these are the two things that we need to prove and the first thing that  we are going to focus on is the vanishing gradient problem    so will start with the dependency graph for lstm’s that means i want to draw  something similar for lstm’s involving all the different elements in lstm so what  are these different elements the two rhyming things one being gates states ok so gates  and states that the two things that we care about so let us look at all these so starting  with states at time step k  one at time step k one you have this two states sk one and hkone ok  using hkone you are going to compute the output gate at time step k and it is also depends  on these parameters wo uo and bo right which is obvious from the equation  just to make sure that this diagram remains tractable i am going to get rid of the  parameters and i will come back to them later so right now will just focus on the states  and the gates ok and then you have these other intermediate states and the other gates  right so you had fk you had ik so add these three gates the temporary state and then  what else what are the other two things at time step k so we saw this diagram about all  the computations which happen at time step k right how many computations happen  three states and three gates right  so you seen the three gates and this one temporary state so which are the other two  things there is no selective forget with you guys is early everything forget hint look at  the grey cells and change the time step what will you get away are you all i mean we  did lstm’s two days right i mean are you all with that or should i we need to revise  something mean i do not need to revise it but we going to is it fine ok so sk and the  other thing hk remember that sk also depends on hk just stare at this for thirty seconds and  make sure that you are with it right all the equations are there these are the see six  equations that or the six computations which happen at time step k  there are three gates and three states and the dependency graph is obvious from these  equations except for the fact that i have ignored the parameters how many if you are  comfortable with the equations and the graph corresponding graph please raise your  hands high so i think it should be right we have these six equations and we have this  dependency graph    now starting so what happened in the graph is we started from skone and hkone and we  reached sk and hk which were the outputs at the next state now what will happen from  here we were looking at recurrent neural networks recursion is the answer what will  happen now  the same graph will keep recusing right for the next time step and up to the last time  step right does that makes sense ok this looks much more complicated than the  dependency graph that we had for rnn’s right by just because there are so many we in  rnn we just had this one state and no gates so here but we have these three states and  three gates that is why this so many paths ok now for simplicity what i will do is i will  not draw separate nodes for the parameters all the in the case of the rnn dependency  graph i had drawn them separately what i am going to do is i am just going to put the  parameters on the corresponding edges right  so fk actually depends on wf it also depends on uf and it also depends on that bias but  i am just going to take a small set of parameter i am only going to focus on the w’s not  the u’s and the biases ok there is only for illustration for no other reason right and  whatever arguments or proof that we are going to see it holds for all the parameters but  we just need to prove it with respect to one parameter and the same story repeats for  everything ok so this is the dependency graph and these are the parameters now what i  am interested in knowing is that there was some loss at time step t and maybe that loss  happened because wf was not good enough to compute sk  of course wf computes fk and then fk helps in computing sk but maybe wf was not i am  just short it short circuiting it and saying that wf was not good enough to compute sk  right and that is why i want the gradient to reach to wf through this sk that is what i  want do ok    and this exactly what i said i am interested in knowing that if this loss can reach wf  through sk right so all the three highlighted things that what i am interested in i am  interested in the path to wf through sk of course there are many other paths to wf but  they do not account for the problem in sk is that fine everyone is clear the setup ok  now and we can ask similar questions about all the other parameters the w’s the us the  the input gate parameters the output gate parameters and so on right there is nothing so  special about wf the same question holds for all these other parameters also ok now  how does lstm ensure that this does not vanish so let us see that    as i argued earlier it is sufficient to show that this gradient does not vanish ok if i can  show that this gradient does not vanish then i am pity sure there is only there is no  recursive connection here because it just a single connection so there is no recursive  connection here so if i can show that the gradient reaches up to this point then after that  i can be sure that it is going to reach wf everyone buys that set up right that is what i  need to show  so to prove that the gradient reaches wf i just need to show that it reaches sk that is the  only thing that i need to show and the first thing i am going to observe is that there are  multiple paths to reach to sk which are these paths one through skone because sk  contributes to skone the other through  student hk  hk which is visible but now also notice that how many paths are there to reach hk itself   four not four actually that is going to be combinatorial because there four outgoing edges  from here but then again there will be four next stage and four next stage and so on  right so let us not count the number of paths but let us just convince ourselves that  there are many paths to reach to sk from lt everyone is convinced about that we are  not counting the exact number of paths that is not very hard to do but all we are saying  is that we know that there is one path through skone one path through hk and hk itself  seems to have many incoming path during back propagation  so there are many paths which are reaching from lt to sk everyone is convinced  about that anyone who has a problem with that now to show that the gradient does not  vanish what do i need to show of all the paths the set there exist at least one path through  which the gradient can flow that is what i need to show ok even if i vanishes across all  the other paths i am still fine with it ok    so now consider one such path which is this highlighted path that is a valid path to reach  to sk now let us denote the gradient along this path to be t naught and the total gradient  is going to be a sum of many such paths right so i am calling this path as t naught and  this is what the gradient look like ok so this is simple just this red path the next red path  and then the series of problematic multiplications right you have this recursive  multiplications again so everyone agrees that red is good the red path there is no  recursion the gradient will flow right we just need to focus on the blue path everyone is  convinced about that right ok  so that is good the first term is fine as i said because it directly connected to l t there is  no recursive or no other intermediate nodes so the gradient will just flow through that  there is not a problem there and now we look at the other terms which is first is dht  st  and the other is this ok    so let us look at ht  st what is this going to be tensor vector matrix scalar at this  point in the course i want a unanimous answer  student matrix  matrix right and recall that in particular the equation was of this form ok so what is the  derivative going to look like even without computing can you tell me something  profound about it it will be a dash matrix big matrix how many if you say diagonal  matrix how many if you do not think it is a diagonal matrix please raise your hands  total sum is never one so remember that ht is equal to htone ht2 up to htd and you have ot  equal to otone ot2 otd and st equal to stone st2 std so ht2 depends only on ot2 and st2 right it does  not depend on in particular does not depend on any of the other st’s  so we have already seen this before in such cases whats the i jth entry of this matrix of  the gradient matrix derivative of hti with respect to stj which of these terms are going to  be zero wherever  student i not equal to zero  i is not equal to zero that means it results in a  student diagonal matrix  diagonal matrix    so that is exactly what is written here and the diagonal elements are going to be this is  that fine everyone with this ok so now this diagonal matrix which contains this on  the diagonal i am going to represent it by the following notation is that ok  fine so  this is a diagonal matrix where every element is i mean this is actually a vector right  everyone agrees this is a vector so this diagonal is this vector is along the diagonal of  this matrix how many if you get this notation if you do not get this you will not  understand anything else    now let us consider st  stone ok this is what st is equal to so what is the derivative  of st  stone  ft right ft right what else why no why are you rebelling what the i mean  st only right if it is can you treat this as a constant no why because this is a dash  network  student refer time onesix3four  so in an ordered network the derivative will have  two terms which are those  student explicit  explicit and implicit in the explicit term what you assume the other terms to be a  constant right fine so st i mean s tilde t also depends on stone so we cannot treated as a  constant so once again this derivative is going to contain an explicit term and an implicit  term now i am going to make a worst case assumption i making this assumption that  actually the implicit term vanishes notice that this not favourable to me i am trying to  prove that the gradient does not vanish the gradient is a sum of two terms i am saying it  let the worst case be that one of these terms vanishes ok  so this is not a favourable assumption this is a unfavourable assumption which i am  making so let us fine so i making the assumption that the implicit term vanishes so  what is the explicit term actually  student ft  ft and what kind of a matrix is that  student diagonal matrix  if you agree that it is a matrix first of all it is a diagonal matrix again and what is the  diagonal  student ft  ft right so i am going to represent it as d of ft is that fine     so remember that the original equation had three terms all of these the last blue once  for all identical so this is not problematic because this is a directly the last layer this  we have already derived a form this is sum diagonal and now for each of these we have  a form do you get that these are the three paths that we have done so far so let me just  substitute them this is what it looks like ok now this is a product of diagonal matrices  what will the product look like  student diagonal matrix  a diagonal matrix and each element would be each element on the diagonal would be a  student product  a product of all those things right so is it fair if i write it as this right which i can write  it as this ok now just stare at this equation and tie it back to the intuition that we  developed something about the gates regulating the flow of information you have a  multiplicative term here right whenever there is a multiplicative term we have a  problem because remember these gates are between zero to one  so there is a chance of vanishing everyone sees that you are multiplying t terms all of  which are between zero to one so there is a chance of vanishing but i am going to end this  proof by saying that the gradient does not vanish so by what am i going to do now ok   make the statement the gradient could vanish but this kind of vanishing is fair what do  you mean by that now when will the gradient vanish  student product  at this product of the forget gates vanishes but if the product of the forget gate vanishes  that means what would have happened during the forward pass that information was not  carried all the way back all the way front two times step t right do you see that ok so  that is the main reason here right so the red term does not vanish the red term time  zone vanish the blue term can vanish but it will vanish only if during the forward pass  also this multiplicative term at cause the information to vanish by the time you are reach  the time step t how many if you get this and this exactly what i meant earlier by  saying that    if during the forward pass st did not contribute much to stone because the forget gate was  tending to zero then during backward pass there is no need to pass this information back to  st right because during forward pass you did not contribute so during backward pass  why should i hold you responsible right and this is absolutely fine to do this and this is  exactly what the equation tells us that they gauze the gradient will vanish only if things  vanished in the forward pass ok and the gates are doing the same regulation in the  forward pass as they will do in the backward pass so everything is fair is that ok  and does there exist one path along which the gradients will not vanish when they do not  need to vanish so if during forward pass all the gates were on that means the  information from state one was actually carried all the way up to state t then during  backward pass what will happen the information will go all the way back right is that  fine so the gradients flow back only when required as regulated by the forget gates and  this is fair because if you are regulating the same thing in the forward as well as the  backward direction then you are not doing anything wrong ok    now that is a proof for lstm solve the vanishing gradient problems or in other words  the gradients vanish only when required and not unnecessarily or arbitrary as is to  happen in the case rnn’s now we will show there exist one path along which the  gradients can explode right so let us show that path so consider this path ok a this  path is again also active so i if we consider the path to hk there is going to be active for  all the gates and all the states right so in whatever gates or states you are considering  this paths would be there  and this is what this path looks like you have the derivative with respect to the last layer  and then you have these guys ok these pairs ht by ot ot by htoneand so on everyone fine  with this so far what is the derivative of ht with respect to ot we do not remember the  equations so i will just tell you directly so based on whatever we have done so far just  trust me that this is what each of the terms in the bracket looks like we can go back and  check this this is just comes directly from the equations now what is happening here  does this look very similar to the situation that we had with rnn’s  we had a diagonal matrix and a weight matrix and a repeated multiplication of these  right and again the diagonal matrix is bounded the weight matrix is bounded so now  the repeated multiplication could explode is that fine so it does not solve the problem of  exploding gradients but it solves the problem of vanishing gradients but now still this is  bad for us right whether the gradients explode or vanish our training is going to get  messed up so how do we deal with this for exploding gradients what will you do  student refer time two thousand, two hundred and twenty-seven clipping  we will do  student clipping  clipping right    so in practice the way of dealing with this is gradient clipping if the norm of the  gradient exceeds the certain value then we are going to just clip it to a certain threshold  and this is fine because we care about the gradients only for the direction and not for the  magnitude anyways when we introduce a learning rate we are doing some kind of scale  down for the gradient magnitude so this is just being more explicit and being careful  that if the gradients are non manageable in terms of their magnitude  then we just going to keep them to some manageable value while being faithful to the  direction and the direction is what matters so that is why exploding gradients is easy  but in the case of vanishing gradients you do not have direction also because the entire  gradient becomes zero so there is no direction there right so that is why vanishing  gradients is more serious than exploding gradients and as long as lstm solve that they  are fine with it is that fine ok so that is the end of this lecture  "
task2/super_cleaned_audios/lesson105.wav,818.1949,so that was recurrent neural networks now whenever we propose a network what do  we do next training right so what we will look at it back propagation through time  this is not the title of a fiction and movie or anything this is an algorithm that we will  see  so before we proceed right let us look at the dimension of the parameters that we have  and i expect you to tell me the dimensions so i will define somethings for you which  are very hard so xi belongs to rn so let us be clear about that si belongs to rd that  means the si is a d dimensional vector and yi belongs to rk which has k classes ok  so now what is u what is v d  k is it d  k i am asking soham now i mean we  have be written it as d  k and w is  student refer time one hundred and five  d  t sure everyone sure ok right so these are the dimensions why am i talking  about these dimensions whenever we talk about gradients what we talk about partial  derivatives or gradient or something we need to know what is the size of the parameter  with respect to which we are taking the gradient because that is what the size of the  gradient matrix is going to be right that is why i am asking you to focus on this    now how do we train this network title of the module   student backpropagation  backpropagation ok how why do i have a module if i am only going to tell you about  backpropagation do you see any problem with this why cannot you just apply the  standard backpropagation algorithm ok so we will try to understand this with the help  of a concrete example and we will go back to our example of predicting characters ok     so this is the auto completion task and for simplicity we will assume that english has  only these three characters d e p and then a stop to indicate that the word has been  completed ok this is what you are going to consider that my vocabulary size is just four  that means i can only predict one of these k four classes k is equal to four ok   and at each time say i want to predict one of these things what is the suitable output  function for this task can everyone say with probability nine hundred and ninety-nine percent  student half max  half max ok what is the suitable loss function for this task small pleasures in life that  is all i get ok    suppose we initialize u v w randomly and networks predicts the following  probabilities ok so let us understand what is happening i fed it d as the input i have  just started training so my u w and v are all some randomly initialized weight  matrices right now and so it has predicted this as my probability distribution this is the  predictions that i have got from the network   and i also know what is the true probability distribution what is the true probability  distribution for the first time step zero one zero zero and so on right you can see it second times  that is also zero one zero zero third is zero zero one zero and the last one should have been zero zero so given the  situation and before i talk about learning algorithms what is the first thing that i need to  define objective function right so what is the objective function here how many  errors do i have i mean i can make my errors at four places whether i making an error or  not is the separate case but i can have four loss functions  so then these are the two questions that i am interested in what is the total loss made  by the model and how do we back propagate this loss and update the parameters of the  model as usual i am ignoring the biases which is w u and v so we can answer these  two questions then we are done right if you can do this then we are done    so the total loss what is the total loss actually take a guess sum of all the loss right  good so just going to be the sum of the loss over the times steps that you are i mean  very logical and what else would it be and we know that the loss at every time step is  so this is the loss at time step t hence y t and what is c actually the true class at time  step t right so it is would be e at first time step e at second time step then p and then  stop ok so that what c is  so this is we all comfortable with is this is the cross into p loss and i am going to sum at  over all the t time setup that i have now for back propagation what we need is we need  to be able to compute the gradient of this loss function with respect to w u v     if i give you your formula for the gradient the rest is straight forward you will just apply  gradient as well ok so let us look at each of these parameters we will look at the easy  one first which is v so what is the derivative of the lost function with respect to v  have you ever done this in life  student yes  yes  when  student refer time zerofour55  now i am asking the date ok so you have done this when you doing backpropagation  this is the gradient of the loss function with respect to the weights in the output layer  and we know how to do that right that is very straightforward and there is no  complication there and you will see what i mean by complication later on  so all i need to do is take this loss function and compute its gradient with respect to v it  is very simple chain rule which i can update there apply there and i can compute it  separately for all these guys and i can just sum it up right so this is the easy part this  is very straight forward so where one parameter we are all set we know how to do that  right we can just add up all these gradients the some lose notation here this is actually  an addition of four matrices right each of this i hope is a matrix is that a matrix or a scalar  or a vector or a tensor  student matrix  matrix ok so we have already seen how to do this back propagation and this is a  smallest chain possible in the back propagation and we have enough confidence in doing  this     now let us considered the derivative of the loss function with respect to w just take a  minute and see if it is complicated or if it is straight forward to see a lot of w’s in the  figure ok so let us see how to do that right   so again the loss with respect to w or the derivative with respect to the loss derivative  of the loss with respect to w is going to just be the sum of these four or t derivatives and  by changed of derivatives we can just sum the derivative across all the paths which lead  from the loss function to w is that fine right whenever you want to compute the  derivative of the loss function with respect to any parameter a recipes to look at all the  paths which go from the loss function to that parameter and some of the gradients across  those paths how many if have fine with this what are the paths which are actually  connecting the loss function to w  student refer time zero65one  there will be t paths good so let us see we will consider lfour this is the last time  step    so lfour actually depends on s four s four depends on what w and s three s three depends on what  w and s two s two depends on what an s one depends on w and s zero always assume there is s zero  what kind of a network is this what kind of a function is this what did i ask to revise  this is not an order derivative what kind of function is this     so we have an ordered network with i will give it to you and it is not be to say in an  ordered network each state is computed one at a time right so we will first compute s  one then we will compute stwo because s two depend on sone there is no other way we can  compute stwo then sthree s four and then finally the loss function  so now we have the following situation that the derivative of lfour with respect to w  can be written using this chain rule which is the derivative with respect to s four and then  the derivative of s four with respect to w and that is that looks manageable there is nothing  fancy here or is it i see a lot if people that looks manageable right everyone is not  refer time zero8zeroone   student refer time zero8zeroone  even though you have done the assignment everyone is not even though you have revise  the assignment everyone is not refer time zero8zero6 so this part we have already seen  this is not the tricky part lfour sfour is straight forward because it only depends on this v  and so its fine that part we have seen this is same as computing the gradient of the loss  function with respect to the hidden layer but now let look at the derivative of sfour with  respect to w     what is sfour actually  four   ws b    so now if i want to compute sfour by let me just  remove the sigma right i mean we can always get back the nonlinearity so i want to  compute sfourw so it will just be s three s three is again  student depend on w  depend on w right so that is the problem with an ordered network in such an ordered  network you cannot compute the gradient of a s four with respect to w assuming that sthree is a  constant sthree is not a constant its again a function of w and w is the parameter with  respect to a computing the derivative right that is the problem here   so in such networks the total derivative has two parts what are these two parts okay  how many if you have revise this what are the two parts called explicit and i mean at  least your language model should be fine at explicit and what else can it be think on at  least have that much smartness either you do not read its fine so that is going to be  explicit and implicit what do we do in the explicit case if you can read the slide we  treat all the other inputs as constant right an implicit is summing over all the indirect  paths from sfour to w so let us actually try to derive this whole thing right    so this is what the total derivative looks like all of you are comfortable with this right  i mean this is all we have done this in the assignments i will not go into the theory and  all that you should be comfortable if you have not revised you have to be blamed sorry  for that but i cannot go into the details of that but i still derive the whole thing   so this is what it looks like the plus here indicates that we are going to treat everything  else as a constant and just take the derivative with respect to w and then the implicit  part would be this we are going to sum across all the paths so this is a path ok  now here again we have a total derivative sthree  w so what am i going to do for  that again explicit and implicit again i have this stwo by two by w which is again  explicit plus implicit again sone  w is that fine and then this is finite because s one does  depends on s zero which has no connection to w so this is what your entire formula looks  like now this sum slide abuse of notation here because what is each of these actually  scalar vector matrix  student refer time onezerofourone  s four is   student s four is vector  vector w is  student matrix  matrix the derivative of a vector with respect to a matrix is  student tensor  tensor you cannot do this in your head is it these three sentences is one after the other ok  so for simplicity what i am going to do is i am going to short circuit some of these  paths right so let us i will just tell you what i am going to short circuit so i am going  to write just for ease of coming up with the generic formula the first term i am going to  write as this and this is fair because this is just one right the second term also is fine  the third term i am going to short circuit this path i am just going to write as sfour  stwo  and then stwo  w and again i am going to short circuit these paths and just write it as    sfour  sone and then this  the reason i am doing this then i can write it as a very simple summation where i have  sfour by sk where k goes from one two three four and then i just have the explicit derivative of sk with  respect to w just stare at this for the minute and not a minute actually just onezero second or  something if you have any problems with this let me know i will use my standard trick  if you do not understand this you will not understand anything afterwards no one is  falling for that ok everyone is comfortable with this ok  so we have a formula for sfour  w and we have dealt with the tricky situation where  we have these multiple paths in an ordered network and hence we are to split into  explicit and implicit derivatives so we have done all that refer time onetwotwozero math and  you have come up with the simplified formula for this ok so finally this is what we  have    you noting it down right   student refer time onetwotwo8  laugher i do not see you noting it down ok so now let us look at sfour  w that is  exactly what we have derived on the previous slide and that was a summation of t terms  and for us t is equal to four ok and in general lt by the this was for lfour so in general if i  want to do lt then it is going to be this which i am replaced by t and this which i have  replaced by this formula everyone is fine with this what were this means everyone is  fine with this formula right this is generic formula with respect to any time step the  only thing is that on the previous slide we are derived with respect to sfour now i have just  come up with the generic formula ok  so this algorithm is called backpropagation through time because now we have taken  care of this ordered network and you have a way of computing this gradient once you  have this gradient your life is simple because now we can just supply the gradient  descent update ok so we have dealt with v we have dealt with w and as the name  suggest who will deal with u you ok fine so you will to find out what it is for u ok  by its going to be something very similar and i do not want to do it because that is not i  mean going to be something very similar you can do it on your own but i want to focus  on something which is important   
task2/super_cleaned_audios/lesson104.wav,526.5106,so we have seen sequence learning problems now we are interested in the question of  how to model these right so we look at something known as recurrent neural networks  and our question that we are interested is how do you model tasks which involves such  sequences ok    so here is the wishlist that we have what will model will come up with should account  for the dependence between inputs because that is the strong case that we have made  that the output actually depends on multiple inputs and not just a single input you  should also account for variable number of inputs because a video could be three hundred seconds  it could be twenty seconds twenty-five seconds a sentence could be of arbitrary lines and so on  and it also makes sure that the function at each time step is the same right but every  time step they are trying to do the same activity ok so we will focus on each of these  items from our wishlist and then try to arrive at a model for dealing with such problems    so first let us ask this question what is the function being executed at each time step  what is the function being executed at each time step either should come after dealing  you have an input your ability to go blank on me is just amazing actually you have a  hidden representation and then you have an output the first time we are seeing this  situation in the entire course where we have an input a hidden representation and an  output  what is the function being executed remember the output is always a function of the  input lecture two or three i do not know definitely not lecture fourteen so what is the function  can you write y i as a function of x for my sake if not for god sake you can ok what  is it ok first tell me what is s one u s one no nonlinearity no bias which all that plus bias  then no nonlinearity who cares about nonlinearities ok and then what is yone ok some  output function ok for some reason i have written sigma here i will just call the output  function as o always maybe in this case sigma would work but o is what i will call it  just to make the this thing clear ok is that fine  so this is the function being executed at this every time step we can just write it using  these two equations which are seeing for a first time and i is a time step since we want  the same function to be executed at each time step we should share the same network at  every time step that means what do i mean by share the same network share the same  parameters good so this is the same as this because u v and b and c are the same ok  so that is an easy way of taking care of the requirement that i want the same function to  be executed at every time step    and this parameter also sharing also ensures that the network becomes agnostic to the  length of the input because now whether i have a word which has onezero characters or twenty  characters it does not matter because at every time step i am going to execute the same  function that is why it is important that at every time step we have the same function  so since we are going to complete the same function the number of times it does not  matter and we can just create multiple copies of this network that we have and for any  arbitrary length n we can still compute the output ok still not quite there we still need  to take care of a lot of things but we are just slowly addressing each item from our  wishlist    now how do we account for dependence between inputs or rather actually the right  way of asking this is how do we account for the case that the output actually depends on  multiple inputs and not just the current input ok how do we account for that feed in  the ok good  so let us first see an infeasible way of doing this ok so you are given the first time  step xone you have a network which predicts one from xone you know at the same second time  step you also want to look at the previous inputs so why not just feed it xone and y xtwo both  and then try to predict y two at the third time step feed in x one x two xthree and predict ythree and so  on forever is this fine probably the word infeasible is there so y is so what is the  problem with this yeah good so i am looking in terms of the conditions that we have  on the wishlist which condition does is violet what is the function being executed at  each time step ok so let us see    the function being executed at every time step is different so yone is function of x one y two is  a function of x one  x two remember that this is not just saying that you are passing to inputs  everything changes because you now you need to have uone and utwo here you need to have  u one u two u three right so everything changes it is not the same function how many of you  get this it is a different function being executed at every time step  so now if i have a sequence of length onezerozero what happens i need y onezerozero which takes f x one  to x onezerozero as inputs and has how many parameters u one to u onezerozero right you could you could  share u one to u ninety-nine for y ninety-nine and y onezerozero but we still need those many of that right so that  network is now sensitive to the length of the input and on the length of the input goes  you will have to construct more and more functions right  and imagine that if the training time the maximum sequence length that you had seen  was twenty-five and suddenly a test time you get a sentence which is of length threezero you do not  even know how to compute that because you have not train any parameters for doing  that    so then the final solution is actually to add a recurrent connection in the network why  does this work ok before that now can you tell me what is the function being executed  at every time step assume there is a s zero here these are a s one s two s three up to s n and there is  a s zero  now what is the function being executed at every time step can you write it down if  you it would help if you think in terms of y two and not in terms of y one y one is the boundary  case was special case the thing in terms of y two or any other of the y’s and first think of  what s two is from s two y is straight forward how many of you can write the function so s i  in general s i is u into x i plus w into s i minus one plus b how many of you get this and  then what is y i again this has to be output functions ok but how does this solve our  problem does it take care of everything on the wishlist one the way we have written it  in terms of i which is the time step definitely the same function is getting executed at  every time step there is no doubt about that right modulo this boundary case of s one  where will assume that there is an s zero ok  so same function being executed at every time step can you deal with inputs of  arbitrary length yes as long as you ensure that the same function is executed its fine  does it ensure that the output is actually dependent on the previous inputs how  student refer time zero7zeroone s i  through s i minus right so that is an interesting thing that this guy actually depends on  this guy which depend on the previous input and also on this guy which in turn depends  on the provision inputs so recursively you can see that you depend on all the previous  inputs that you had ok that is a very neat way of ensuring that your output depends on  all the previous inputs and you do not blow away the parameters blow of the parameters  by sharing this recurrent connection and that is this is a compact way of writing is that  your y i is now function of x i s i and has these parameters w u v and b and c so s i is  called the state of the network at times step i  and what is see here this is just for the sake of completion this is known as a recurrent  neural network because of this recurrent connection and s i is a state of the network at  time step i so as when you start working in deep learning and you are dealing with  sequence problems state of rnn or state of lstm or state of grv something that you  will be hearing or reading often so this is what you mean by the state of the recurrent  neural network this is the current state which kind of encode everything that is happened  so far right it has a encoding of all the inputs that you had seen so  the parameters of the network are w u and v which i shared across time steps i  obvious forget the biases and the same network is getting executed every time steps i  do not need to worry about whether i am computing y one y two y three or y onezerozero right so  everyone agrees that this solution takes care of all the things that we had on our wishlist  how many of you agree with that    and this is a more compact way of representing that that you say that you compute s i  and then you are feeding it back so this is just more compact way of representing a  recurrent neural network    so let us now revisit the sequence learning problems that we have seen so now just  correct each of these networks so what would happen each of these things i was  thinking of all the inputs as independent so now what will i do what is the only thing  to be done just add the recurrent connection right  so once you add the recurrent connection now you can go back and relate to all these  problems that one i am trying to predict the character which appears after e i also have  the information of d and e and same argument you can make for all the other examples  that you have but i am trying to predict this final state i have the information of all the  previous inputs here ok  
task2/super_cleaned_audios/lesson110.wav,448.463,so that was lstm and grus  now the issue is that i have given you a very explanation that why you selectively read  write and forget should work but you have not actually formally proven or even given  an intuition for with these sets of equations how are we sure that the gradients will flow  back right we introduced a bunch of equations remember in the case of lstms sorry in  the case of rnns the problem was because of the recurrent connections right because  you had these recurrent connections this w which was the recurrent parameter right  which was connecting cell state stone to cell state st   this was repeatedly appearing in your gradients right and that was causing the problem  because when you had this multiplicative factor lambda into w and then if you compute  the and this was some raise to t so then if you compute this magnitude then if the  magnitude of w blows up then the whole thing will explode if the magnitude of w  vanishes then the whole thing will vanish right that is the problem that we had    so this was because of the recurrent connections do we have recurrent connections in  lstms or grus for that matter do you have recurrent connections yes or no  student yes  yes so then that problem could still occur right i mean if you had that the crux of the  problem for the vanishing gradient was this recurrent connection which is getting  multiplied and hence reading to problem but we still have recurrent connections the  case of lstms also and why should things become any easier in this case how many if  you get the question how many if you can give me the answer selectively that is a good  answer so can you think of what is happening here so first thing that we going to do  now so i will go on to the next module     when i going to give you intuition for what is happening and then we will do slightly in  fact a rigorous proof of why it actually solve the vanishing and exploding gradient  problem ok so let us look at the intuition first how lstms avoid the problem of  vanishing gradients i am only focusing on vanishing gradients exploding gradients are  actually easier to deal with why  student refer time two hundred and twenty-six  what can you do what are we interested in when we compute a gradient direction so  if the magnitude is very large what can we do just normalize it and restricted to be a  certain magnitude so that is known as gradient clipping so exploding gradients in that  sense is still not a big problem but vanishing gradients is because if it vanishes you  cannot do anything because you could think of it that you already have a learning rate  which is getting multiplied with the vector the gradient now in addition to the learning  rate which was anyways clipping the norm of the gradient right so you are doing an  expressive clipping also  so it just like a additional learning rate inductions right ok    so here the intuition and then will go to the more rigorous stuff not in this class  probably so during forward propagation the gates control the flow of information right  the gate decides how much of stone should be pass to st ok and they prevent any relevant  information from being returned to the next state similarly during back propagation the  gates will regulate the flow of information so what i mean by that is that if at a certain  state you have computed st  f t stone  it s t right  so this gate is actually deciding how much information flows in the positive direction  ok and suppose this gate value was five so the five of this information from st has been  carried on stone ok now during back propagation what is the derivative of st with respect  to stone going to be partial derivative is going to be a ft think of st and stone as single  variables like you know n dimension variables then the just ft of course you are  forgetting that what kind of a network is this ordered network right so you cannot  read as till de t as a constant  where s tilde t also somewhere depends on s t minus ok but just this assume that  maybe this vanishes and that is the worst case assumption right because i do not want it  to vanish but i am assuming that the second term vanishes but even then with the first  term i will have a gradient which is proportional to the gate why is that fine so  remember that i am not making a easy assumption i am making a worst case  assumption this is not favourable to me i am saying that the second term vanishes and i  dont want it to vanish but i am just trying to prove that even in the worst case by the  second term vanishes you still have this gradient f t from the first term right  and why is that good why is that ok because f t decides how much flew in the forward  direction and it is also deciding how much goes back in the backward direction so it is  a fair regulator with says that if i passed on only this much information in the forward  direction then during backward pass also i should only make a responsible by this  much ok now let us look at a situation where you had f one f two f three upto f t and all of  these gates were five now five seems a reasonable value but when we have five raise to t  and t is a large value what is going to happen this quantity is going to vanish  so what is happening is that sone contribution to st in the forward direction itself had  sone’s contribution to st in the forward direction itself was had already vanished right  because it was continuously getting multiplied by five five five so it is like this chinese  vespring problems right so this guy said something whereas next guy added noise the  next guy again added noise and so on till the time it reach the tth guy this information  was completely lost so in the forward pass if sone did not contribute to s t in the  backward pass should i make it responsible for the crimes of s t no  so what is happening in the backward pass again the gradients are getting regulated by  the same forget gates so again in the backward pass will have a situation that by the  time the gradient reach is sone it would be five raise to t and that is fine it is going to  vanish but that is because even in the forward pass it vanished so let it vanish in the  backward pass also so this kind of vanishing is ok    so this is just the same thing written in words so if the stated time t minus one did not  contribute much to the state at time t because ft was tending to zero right then during  backpropagation the gradients flowing into stone minus one will also vanish because again  during backpropagation the gradients will get multiplied by ft and they will vanish   but this kind of vanishing gradient is fine this is fair because if we did not contribute in  the forward direction why should i help you hold your responsible in the backward  direction right so that is fair so the key difference from rnns is that the flow of  gradients is now controlled by gates which give the same regulation in the forward pass  as well as the backward pass right so only if you contributed to something you will be  held responsible if your contribution vanished your responsibility in the backward pass  will also vanish right so that is the intuition     and will next see an proof for this a proof actually it s as based on the intuition but i  just make it more formal in terms of introducing the notations and so on so that problem  we will do it in the next class ok   
task2/super_cleaned_audios/lesson13.wav,219.0245,before we go to the next section which is on learning  i just want to introduce the concept of errors and error surfaces and tell you what it relates to these multiple solutions that we were talking about  so for simplicity what we will do is we will just set the threshold to minus or minus w to one which is setting the threshold to minus one and now i will try different values of wone and wtwo ok so i was saying that there are multiple values of wone and wtwo possible and these are all real numbers we are not constrained by having them as boolean values so now this is one solution which i tried i tried setting wone to minus one and wtwo to minus one what is wrong with this line does it lead to any errors how many just one error so this makes an error of one out of the four inputs now let me just try some other values of wone and wtwo this line again one error what about this line not four three because zero zero is anyways on this side of a line so now given this now tell me that i my quest is to find these w so i would want to find wone wtwo and so on given this discussion on errors can you tell me a condition that i am looking for i want to find wone wtwo or up to wn such that errors are minimized and in the best case errors are zero so that is what i want so this just i want to make a case that these search for w’s is driven by certain objective and this objective is to minimize the error so now since we are doing this let us plot the error surface corresponding to different values of w naught wone and wtwo  once again for simpler analysis we will just keep w naught to be fixed at minus one and now what i have so just do not read this bullet as of now even this one so i have this wtwo here so that is my one axis and i have wone here which is my another axis now what i am going to do is i am going to try different values of wone and wtwo so this axis can go from minus infinity to plus infinity of course for showing the sake of showing here i have just had it from minus four to four  so now what i am going to do is i am searching for some values of w’s wone and wtwo so that my errors is zero and let us do a brute force and i will just try every value between minus four to four ok in fact one of the solutions which i proposed actually was this oneone oneone right that is the line which we saw on the previous slide and which led to zero errors and that is the dark blue surface here  so  how did i compute this error actually i just substituted minus sorry oneone oneone here and then i put in all the four values combinations for xone x two and i realized that i am able to satisfy all of them so i do not get any error now instead of that if i had put something different so let me just go back to the previous slide which was see minus one minus one which is i think yeah somewhere around here right minus one minus one i guess so for that i am in this light blue region where the error was one i make errors for one of the inputs so it is a very brute force way of finding this and this is not going to work because we have lots of inputs to check but this is just to give you an intuition that we are looking at errors and we are trying to find a value of wone wtwo which minimize this error so that is the idea behind errors and error surfaces 
task2/super_cleaned_audios/lesson17.wav,768.0196,we will go to the next module where we talk about a network of perceptrons and then  we talk about the representation power of a network of perceptrons so this module  should have been titled as network of perceptrons so now in particular what we are  going to see is how any boolean function whether linearly separable or not can be  represented using a network of perceptrons now what do i mean by represented during a network of perceptrons what it means is that i will give you a network of perceptrons you take any boolean function feed any value of xone to xn and the network will give you the same y as it is expected from the truth table ok that is what representation means just to put it out clearly and now i am going to again do a setup i am not giving you the solution i am just making some set up and then we will discuss the solution so for this discussion we will assume that true equal to plus one and false equal to minus one so instead of zero and one we will assume minus one and plus one and these are your inputs xone and xtwo we are taking the two input case and i will have four perceptrons first i will have four perceptrons and i will also have very specific weights connected to form the inputs to these perceptrons so red means minus one and blue means plus one right so the first two inputs are connected with a weight of minus one the next two inputs with minus one plus one plus one minus one and the last would be plus one now once i have this i will set the bias of all these perceptrons to minus two so that will that means they will fire only if they are weighted sum of the inputs is greater than two now after this i will have one more perceptron so i had two inputs i converted that to four values these four values are now going to feed into one more perceptron and these weights i will not fix them these are the weights that i am going to learn ok these and the final output of this perceptron which is the green perceptron is the output of the network right so now coming back to what i said that it can represent any function what i mean is that you take any function feed in any combination of xone xtwo this network will give you y and i am telling you that it will match the truth table of that function now let us define some terminology this will also stay with us for the rest of the course so this network contains three layers  the layer containing the inputs it is called the input layer very creative the middle layer containing the four perceptrons is called the hidden layer and the output layer which gives you the output is called the output layer output perceptron which gives you the output is called the output layer right so you have a input layer a hidden layer and an output layer  and the outputs of the four perceptrons i am going to call them as hone htwo hthree and hfour and the red and blue edges are called the weights for the layer one which we have not learned we have actually set them by hand and the weights for wone wtwo wthree wfour are called the weights for the second layer other the layer two weights and these are the weights that we want to learn now i make this claim that this network it can take any boolean function it can implement any boolean function so this same network can implement any boolean function that means if i take this network and if i try to learn the values of wone wtwo wthree wfour for any boolean function whether it was originally linearly separable or not i will be able to implement it so isn’t this an astonishing claim any boolean function do you think this is an astonishing claim well not really if you really understand what is happening here right so let us see what exactly is happening here so when will perceptron one fire when the input is false false zero zero will it fire for any other input when will perceptron two fire any other input same for the next perceptron same for the next perceptron so you start getting an intuition of what is happening here you do ok let us see so now for this particular network now that i have given you some intuition of what is happening basically every node or every neuron in the hidden layer is catering to one of the inputs and it will fire only for that input it will not fire for anyone else so now let us try to implement the xor function and see what are the so now let us try to implement the xor function and see what are the set of inequalities that result from this earlier when we try to look at the set of inequalities we ended up with a contradiction let us see if that happens now so this is xone xtwo this is your xor function so that is just like any truth table then i am noting down the intermediate values and then my final input to the green perceptron is going to be summation of these and it will fire if this summation is greater than equal to zero or else it will not fire now for the first case when the input is zero comma zero what is hone going to be one and everything else is going to be zero that is exactly what we saw in the previous slide so what is the summation going to be just wone right so it is wone hone plus wtwo htwo so on but htwo to hfour are zero so only thing that remains is wone for the second case wtwo for the third case wthree for the fourth case wfour so is it clear now what is happening let us go a bit more into detail right so now for the xor condition what are the conditions that we need wone should be less than wzero because this should not fire wtwo should be greater than equal to zero wthree should be greater than equal to sorry w naught not w is not zero and wfour should not fire so wfour should be less than wzero are there any contradictions here by design no right so we have made sure that for the final layer only one of these guys feeds to it so it does not matter what the remaining outputs are they do not interfere with each other unlike earlier where we had conditions like wone should be something wtwo should be something and then wone plus wtwo should be something there are no such contradictions here because we have made sure that every neuron in the middle layer actually caters to one specific input and now the weights in the final layer can be adjusted so that we get the desired output for that input so i can set whatever value of wone i need to set so that i can fire the neuron in fact i could just fix wzero as zero and then i can adjust the weights of wone wtwo wthree wfour and i can implement the xor function so are you convinced that this can be used to implement any boolean function how many if you are not convinced so the negative question never works how many if you are convinced sure now what if we had three inputs before that it should be clear that the same network can be used for any of the remaining one5 functions and for each of these functions we will end up with a different value of wone wtwo wthree wfour but you will be able to satisfy the truth table right and you can go home and try it which i am sure you will do ah so what if we have a function of three inputs two56 what is two56 no eight fine sure so this is what it will look like and anything specific about the weights of the initial layer can you tell me what the weights would be just tell me red red red red blue blue whatever colours you like this thing first perceptron what would the weight colours be red red red then enough so this is how it will look right right and now this same thing will work with the same logic for any boolean function of three inputs you will get these eight inequalities and they will not interfere with each other and you can set the values of wone to weight so that you can satisfy it ok fine so what if we have n inputs two power n perceptrons in the middle layer right ok so now here is the theorem any boolean function of n inputs can be represented exactly by a network of perceptrons containing one hidden layer with two raised to n perceptrons and one output layer containing one perceptron we just saw an informal proof of this we just constructed i just gave you the answer it this is how you will get it now note that a network of two raised to n plus one perceptron is it sufficient or necessary or both sufficient yes that is what it says is it necessary  no we already saw the and function which we can just represent using a single perceptron right so it is not necessary but it is sufficient so this is a very powerful theorem if you think of it right so now this whole objection right remember this history and when we have the c i winter when people showed that perceptron cannot handle the xor function that is for a single perceptron if you have a network of perceptrons you can actually have any boolean function but what  is  the  catch  as  the  value  of  n  increases  the  number  of  neurons  increases exponentially right but still in theory you could have a solution now again why do we care about boolean functions i keep coming back to this why do we care about boolean functions because you took this and so the question that i the question that i want to answer is how does this relate back to our original problem right we know any boolean function can be implemented how do we go back to our original problem which is whether we like a movie or not right and you could see that there is a whole family of problems there right whether we like a movie or not whether we like a product or not whether i want to go home today or not yes no any kind of a yes no problem it is a whole family of problems there so let us see so we are given this data from our past experience right so we are told that this is what the movie looks like these are the actor’s director’s joiners everything we also know whether we like these or not so we have a set of positive points and we have a set of negative points right and now we want to have a computational model which can satisfy this data that means once the model is trained once whatever algorithm i algorithm i use has converged it should be able to give me the correct output for a given input that is what we are interested in and that is a real classification problem that we are interested in now for each movie we are given these factors as well as the decision and i said pi’s and ni’s are positive and negative points the data may or may not be linearly separable it is not necessary that the data is linearly separable those were the goody cases it but in general that may not happen but do we worry about it now no what the previous theorem told us is that irrespective of whether your data is linearly separable or not i can give you a network which will be able to solve this problem modulo that it might be very expensive in the number of neurons in the middle layer but if you keep that aside i have a solution for this and that is why we care about boolean functions because many problems we could actually cast to it in a simplistic way if we ignore the real inputs and if you even think of the real inputs suppose it could take all values between zero to one you can always make it binary you could say that is the value between zero and zeroone is the value between zeroone and zerotwo and you could make it as small the scale as small as possible right so that is why we care about this so the story so far has been that the network of networks of the form that we just saw it which contain one input layer output layer and one or more hidden layers these are known as multilayer perceptrons but a more appropriate terminology would be multi layered network of perceptrons  because the perceptron is not multilayered you have a network of perceptrons and that network has many layers right but generally there is abuse of notation we always call it mlp which is multilayered perceptrons and the theorem that we just saw gives us the representation power of an mlp and basically tells us that it can represent any boolean function that we want to deal with so that is where we will end this class and in the next class we will talk about sigmoid neurons 
task2/super_cleaned_audios/lesson114.wav,1615.8264,so let us go on to the next module which is attention mechanism  so let us motivate not the task of attention let us motivate attention mechanisms with  the help of machine translation ok so what is happening in the models that we have  seen so far the current model that we saw for machine translation by the way all the  models that i have shown you so far are wrong or rather incomplete we will complete all  of them right and that is where attention fits in ok that was for the camera  a encoder reads the sentence and its computes the encoding once right we read the  entire sentence and be encoded it and then we have these two options either the pass the  encoding at the zero time step or pass this encoding at every time step is this how humans  translate a sentence what is the human analogy for this you have read the sentence  once done and now we are going to remember this entire thing throughout and then  translate imagine if you doing this for sentences which have twenty-five words which is a typical  wikipedia sentence what is wrong with this we have read the input ones and we have  encoded it what is likely to happen you will forget something you are going to lose  information not just that is the entire sentence important that every time step  student refer time zeroonetwo8  only certain words are important you see this conceptually something wrong that we  are doing here is is saying ok i have encoded the sentence and then start decoding from  there ok thats the conceptual error that we are making so let us see how humans  actually try translate it right    so when producing one word in the output suppose my input is the hindi sentence and i  have the output sentence when i am trying to produce the first word i actually compute  this probability distribution which tells me which of the input words that i need to focus  on at this point it is ok if i dont know what is the translation for ghar or ja or raha or  hoon as long as i know the translation for mai i am done because that is the word which  i first need to produce there right   so i am going to say that at this point i only need to pay attention with the first word in  the input and i can ignore everything else what about the second time step i just need  to focus on the last word what about the third time step is it always going to be that i  only need to focus on one word at a time  student no  no what about the third time step  student fl  i am sorry i am assuming everyone understands hindi but i think that is this is small  sentence i can assume that what will you focus on  student ja rahi  ja and rahi right you want to focus on both these things and not an anything else and  what about the next one hoon right so just on ghar and not an anything else is this  what the model encoder decoder model is doing what is it doing actually the every  time step is focusing on the entire sentence because that is the encoding that your feeding  to every time step that is the problem that we need to correct we need to learn to pay  attention to certain important parts of the sentence is the setup clear to all of you is the  motivation fine not your motivation layers is the motivation for this fine or not ok  the distribution actually tells us how much attention to pay to each input word at each  time step and ideally at each time step we should face pay attention to only certain  words in the input    so let us revisit the decoder that we have seen so far this is what the decoder looks  like in fact i also have the encoder there now suppose sorry so currently what we are  doing is we are either feeding s zero at the i mean we have either feeding the input  embedding or the encoder embedding at time step zero or at every time step the suppose  there was an oracle which told us exactly which are the words important at time step t  right so in our example at time step three suppose it told us that the word going is  important actually we need to flip the input and output here also right but you can still  understand right  so i am saying at time step three certain words are important and suppose a oracle actually  told us that these are the words which are important what would you do assume that  you have already run the encoder what will you do now and say someone told you that  only this word is important word why weighted i am just saying binary weigths right  only this word is important what would you do ideally  student refer time zerofourtwofour  just feed this blue vector to the decoder and do not feed everything else does not make  sense suppose i told you that two words were important send those two words but how  concatenate but now at certain time steps four words will be important and you cannot  concatenate four words right because then the dimensions will change so what do you do  student refer time zerofourfour7  a weighted  student refer time zerofourfour9  weighted some of the important inputs is that make sense at time stamp three we saw that  ja was zero5 important and rahi was zero5 important just a weighted combination of those  two blue vectors and feed that to the decoder so you are not changing the dimensions at  each time stamp because the blue vectors have the same dimension i am just taking a  weighted combination of those i am going to give you the same dimension does that  make sense ok    so in fact what i am saying is that i could just take a weighted combination of all the  blue vectors that i have at the encoder and the weights of this weighted combination  right now i am assuming that someone oracle has given me is that ok if i had his  weights does this makes more sense then having the vanilla encoder decoder model  everyone agrees with that ok  now the question of course us who is going to give us these weights we will come  back to that later but at least given the weights this make sense so at every time step  they just going to focus on the words which are actually important just take a weighted  combination of those words and we will just feed that to the decoder and intuitively this  should work better because unlike before where we were overloading the decoder with  the entire sentence remember twenty-five words threezero words entire sentence was being passed to  the decoder now you are just overloading it with the amount of information that it  actually needs to produce that particular word hence intuitively this should work better  right ok  now how do you convert this intuition into a model    in practice of course there is no angel who is going to coming give as these weights is  no oracle the machine will have to learn this from the data whenever you need to learn  something you need to  student refer time zero6threefour  introduce parameters so i am going to now introduce a parametric form for the from  the figure which thing for those of we cannot see these are alpha one alpha two and so on  so now from the figure we are going to introduce a parametric form for  student alphas  for the alphas ok so i am going to introduce i will come to alpha but and what you  think this weight should depend on what i am trying to say is that at the tth time step of  the decoder so this is ejt at the tth time step of the decoder i want to find out how  important is the jth word in the input that is exactly what i am interested in at every time  step of the decoder of all the input words i want to see which of them is the most  important right so this is the quantity that i am interested is how important is the jth  input word at the tth time step this should depend on what what should it be a function  of  for one it should depend on what that word is right the other is should depend on what  has happened in the decoder so far what is the decoder produce so what is the input  and what is the decoder state at so far right so as the decoder has already decoded the  word ghar or home it does not need to look back at home right that is why need to  know what is the state of the decoder what captures the state of the decoder at time step  t ht and what is the state of all the words that we have it is captured by what the  hj’s right this is h one h two h three h four does that make sense how many of you have fine at  this point please raise your hands high above ok how many of you have questions  please ask specific questions if you have a question  all i am saying is a couple of things one is at every time step instead of the oracle giving  me these weights i want to learn these weights whenever i want to learn something i  have to introduce a parametric form and then i learn those parameters ok now what is  the quantity that i am interested in i am interested in this for all the input words for the  jth input word i am interested in knowing how important it is for the tth time step there  are several ways i could write this function i am saying that the two things that are  important is one what is this jth word which is captured by hj right and what is the state of  the decoder up to this time step which is captured by stone  you could think of various other equations at this point i am fine if you by the intuition  that this quantity should indeed depend on these two terms it should depend on what has  happened in the decoder so far and what is my current word actually look like how  many of your fine with that please raise your hands up and high ok now also the other  thing that i want is that across all the input words this should actually sum to one right i  just want a weighted combination i do not want arbitrary weights it just like taking a  probability distribution over what which word is important by how much so if i have  this ejt how will i convert it to a probability distribution  student softmax  softmax so i will just compute the alpha j’s as a softmax of ejt e j’s is that fine did not  get this ok now we have still not seen what the exact form of attention is of the f  attention function is    so this is what the equation for the jt is that we had an alpha j actually denotes the  probability of focusing on the jth word at the tth time step ok now we are now trying to  learn these alphas instead of an oracle telling us what these alphas are so learning is  always going to involve some parameters so let us define a parametric form for alphas  and just a couple of notations    so from now on we would not change this we are going to refer to the decoder state as  st and the encoder state as shj ok so these blue vectors are s s and these blue vectors are  hs ok  given these new notations one among many possible choices for f attention is the  following i wanted it to depend on the current decoder state i am making a dependent on  the current decoder state but i am also adding a parameter in front of it right i also  wanted to make a dependent on h j i am making a dependent on that i am also adding a  parameter here   why do i need this parameter what is the dimension of this let us assume this is also  what is the dimension of this remember after multiplying with u attention and after  multiplying with w attention the two vectors should be addable is that fine something  cross d what about this same thing cross d good ok so let us call that same thing as d  one what is this output then the tanh output is vector scalar matrix vector of size  student refer time oneonethree8  you said matrix or scalar it is  ask r raise to d one what is the quantity on the left hand side vector scalar matrix  vector even though it has two indices it is a vector what is this quantity capturing at  time step t what is the importance of the jth input that is a i will keep asking till everyone  replies that is a  student scalar  scalar ok now you have scalar equal to something multiplied by rdone so why do you  need this something  student refer time onetwoone6  so what is the dimension of that going to be  student rdone  rdone so that is the dot product so you see why we have these parameters ok so what  we have done is made it dependent on stone and hj and also made sure that the output is a  scalar that is what these three parameters are doing ok and these parameters will be learned  along with all the other parameters of the encoder and the decoder    so now this is all fine you would actually someone had given me the true alpha j’s and  i had predicted this alpha j’s to that fancy equation which i just showed you and then i  added a dash function softmax that is the laughter safest choice in this course i want  to learn something so what do i what should i add  student loss function  loss function what would the loss function be  student refer time onethreezerofour  say a squared error loss and then i want to adjust the parameters to minimize this  squared error loss then all of this makes a lot of sense right because then you can  imagine that your u attention w attention and v attention will get tuned in a way that  the predicted weights are very close to the true weights this we all understand  given an objective function we understand that the weights will get adjusted so that you  are there to the objective function but the whole premise was that we do not have the  true alphas because in the case of translation no one is going to tell you that the kth word  can come came from the jth word or the set of j words do you all agree with that so if  we had the true alphas this makes a lot of sense because then we could have added a loss  function which takes the loss of alpha true with respect to alpha prediction and then an  addition to our lost theta which was the sum of the cross entropy errors and then we  could have jointly minimize this and we could have hope that the attention parameters  would have been learnt accordingly    in practice we will not have this in our translation example we would want someone to  manually annotate for every word in the output which is the set of input word from is this  which it came is not going to be possible this we cannot collect so much annotated data  so what do we do why should this model then work they does not have any  supervision why should this model work in the absence of such data  how many of you get the meaning of the question how many of you see the problem  please raise your hands we are not given the true alphas and that is what a problem is  then why should this work better this works better because this is a better dash better  dash choice language model better dash choice what is the possibility is there better  modeling choice why ok i give you the answer this was better because this is a  better modeling choice why so so i will give you analogy and the reinforcement  learning fans will cringe but they can just go out  so suppose i trying to learn a bicycle how to ride a bicycle that’s why i said they will  cringe i already see some of you can see as if you guy have a copyright on bicycles ok  so suppose you trying to learn a bicycle and for some reason you in your infinite  wisdom you decide that you can learn how to ride it without holding the handle and you  start trying to do it its conceivable that you know few years or decades you will actually  know how to ride the bicycle right even if you are not holding the handle right people  do that and people can ride it before without that  now the only thing that i do is i come and tell you that instead of doing this why not  you try to hold the handle and then try to ride the bicycle right that is all i am telling  you i am not giving you any other supervision i have just given you a better model i  have said that instead of just trying to adjust the parameters with respect your feet and  the pedal and your back position why not you also introduce this additional parameters  where you are holding the bicycle which your hands and now try to figure out what kind  of weights you need to put on your left hand right hand and so on  i am not giving you any supervision for that that you need to still discover on your own  that you will start riding it you might fall on one side you might fall on the other side  but you will eventually figure out what these weights need to be right because a second  model where you hold the handle is a better model than the first model where you are not  holding the hand  in the second model you have additional parameters where you could adjust these  parameters so that you could learn to drive better that some more natural way more  close to human way of learning how to ride a bicycle the same thing is happening here  the second model where you have a way of learning these attention on the weights even  though i am not all i am telling you that look maybe if you decided every time step  which were to pay attention on you might be able to do better than feeding the  information from the entire sentence at every time step  that is all the information that i am giving you which is very similar to saying just hold  the hand that is not going to teach you how to ride a bicycle right you still have to do  the extra work of learning these parameters but now you are given a chance you are  giving the model a chance to learn these parameters they are telling it that this is a better  way of modeling it with this you should be able to learn better right    so there is a hope of doing better because now the model is actually making a more  informed choice right it is a more informed way of learning how to do translation by  focusing on certain words at every time step  and now these parameters how will they get adjusted they will get adjusted because at  every at a given time step you produced a wrong output you did that maybe because this  parameter was wrong which is the v parameter or maybe because these recurrent  connections were wrong or maybe because your attention weights are not proper so  now adjust the attention weights and that should given sufficient data it should be able  to learn which words to focus on just as humans learn how to do translation right  even when we are doing learning how to translate or when we learn translating from one  language to another we are not given this word by word supervision right we just do a  lot of translations or read a lot of translations and somehow understand that while  translating i need to focus on certain words and at every time step this is the word that i  need to focus on so given enough words it should be able to learn that at least someone  gets the joke good so that is the hope and in practice indeed these models work better  as compared to vanilla encode you do not know where the statement comes from    so now let us what we will do is so this entire thing hints on hope only right that is all  that is all i am saying but it does makes sense right because you have these additional  parameters which you can learn and you can back propagate through them i will just  not stop there will actually prove what happens not proved by demonstrate what happens  in practice right  so with this attention model in mind let us look back at the encoder decoder model that  we had for machine translation integrate the attention mechanism with it and then let us  see the end to end equation that we get ok    so this is what the diagram looks like the input and output still remains the same we  have just given the source sentence and the target sentence in particular you are not given  which words to pay attention to every time step right that is not given so remember  that my input is not changing it still the same source sentence and the target sentence  what is the encoder  now try to work out the equations i wanted to write the equation for y t which is going  to be some composite function of x where x is a vector it is a x one x two x three all the words  in the input and somewhere along the line it also going to have this attention equation  it is going to take a while but at least try to imagine it there is some hints in the diagram  itself you could take a look at it i am just asking you to convert the diagram to a set of  equations   so encoder part is fine i have computed the representation of each word at time step t  so this is a contextual representation the word because it is aware of what the  neighboring words are right now what is the decoder going to be what is the first  thing at time step one or a time step t in the decoder what is the first thing that i am going  to compute the dash weights the last time step of the decoder of the encoder sorry  what is the first thing that we need to compute a time step t t attention weights speak  up please what is the first thing that you need to compute at every time step how what  kind of a combination i take off the inputs or rather which are the words that i need to  focus on from the input who tells us this  student attention weights  the attention weights how will you compute the attention weights using this fancy  equation that we have seen earlier is this enough i need to convert this to a  student probability distribution  probability distribution right that is just to ensure that everything is a neat combination  once i know the attention weights what do i need to feed the decoder a dash  combination of the inputs a weighted combination how do i take a weighted  combination of the inputs summation i is equal to one to capital t  student jt  jt into h j right no j t is the decoder time step j is the input word so at the tth time  step of the decoder i am taking a weighted combination of all my inputs the index over  the inputs is going from j equal to one to t by the way did that answer your question that is  what you are asking right ok is that fine ok  now what next now i want to produce a word at the output what is the decoder going  to be first thing that i am going to do is decoder is a dash rnn ok what is the input  to the rnn every time step the previous predicted word as well as the weighted  combination input that you have given it does that make sense ok and then finally  how do i get the probability distribution is that fine yeah i think this should be a  distribution right l t does not make sense l t is r max of this right and what is the loss  function  student cross entropy  cross entropy there is no change in the loss function right loss and the algorithm  remains the same say seen these set of equations now how many of your confident of  going back and modifying all the wrong encoder decoder modules that we have covered  in the initial part of lecture modifying them to add the attention equations in it how  many of you can do that please raise your hands i am not going to ask you just do it so  that i feel happy you can do right any questions at this point very good ok    so you can go back and try adding attention mechanisms to all the models that we have  seen before right see how will you compute so remember the only purpose of ok  what kind of a network is the attention network it is a single feed forward neural  network right this is just transforming a simple linear transformation of the inputs and  in a nonlinearity on top of that and then just again one more transformation right  it is a simple feed forward neural network only these three equation somehow need to be  fitted in all the other models that we have seen so far right this is a very generic  framework just as the encoder decoder framework or the very generic framework the  encode attend decode framework is also very generic framework you can go back and  model all the applications that we saw and you can change them change them to at the  other case ok try to answer the same set of questions what’s the data what’s the  encoder what’s the decoder what’s the loss what’s the training function  and in particular remember that in when you go back represent all the applications that  you have done the data is not going to change no one is going to give us the supervision  for the alphas that is one thing which is not going to change ok    so here is one more thing so this probably tie to this question like how do we be sure  that the alphas actually learn something meaningful now what do i mean by this if i  have to convince you that alphas are actually learning something meaningful and let us  take the context of machine translation what do i need to show you suppose the model  has generated an output for a given input sentence it has generated a translation what do  i need to show you to convince that it is learn some kind of weights at every time step  what should i show you  student refer time twofourfour6  what does the attention weights look like right so let us see    this is a common trick or not a trick actually it is a probably a trick only but this is the  common thing which is used in several papers and that is why i call it a trick because it is  a trick to get a paper accepted that you actually show what the attention weights actually  look like right so on this is the input document and this is the summary that you want  to generate ok  and what you see here is that at different time step so look at the last time step terrorism  it paid maximum attention to the word terrorism in the input right so we can draw this  matrix suppose you had capital l time steps in the output and capital t time steps in the  input so you could draw this l cross t time step or t matrix which tells you what was  the attention paid to every input word at every output time step do you get that  you see what is matrix is this heat map is essentially a matrix of size l cross t and every  cell here tells you how much attention you paid to a particular word at that time step and  the darker the cell that means more the attention that you paid everyone get this ok so  what this is saying is that probably see when you wanted to generate russia the  maximum attention was paid to russian and maybe some other words also sometimes it  does not work very well but sometimes it does right   so for calls the maximum attention was paid to called and then similarly for front with  the maximum attention was paid to front and so on you see some meaningful patterns  that it is learning here  and here is another example for machine translation so roughly to quickly understand  what this figure is right so this is i think english to french is it french yeah its  french or french to english translation which is largely monotonic right that means at  the fourth english word you would end up paying attention to the fourth french word right  that means you are almost doing a word by word translation   and that is exactly what you see the most of the attention is along the diagonal right so  it is learning some meaningful attention weights as always helpful if you are using if  you are using an attention mechanism to plot the sense see if it is actually learning any  meaningful attentions or attention weights or not right so that is a common trick which  people use   
task2/super_cleaned_audios/lesson100.wav,636.8903,so we will start so we were in the 3rd lecture on cnn’s where we were looking at  different visualization tools for understanding what your convolutional neural network is  learning and we did a bunch of things and now you move on to the next module where  we talk about something known as deep dream very interestingly titled but i am sure  most of you have already seen this or read about this  so here is the idea right so far we were seeing that if we start from a blank image then  we could suitably modify it by constructing an optimization problem whose parameters  are the pixels of the image and we can modify the image so that it starts looking like a  certain class of interest right but now suppose instead of starting with a blank image i  start with a natural image right say a sky or any image that you have in your dataset  i start with this and then i focus on neurons in some layer of the convolutional neural  network i am focusing on these neurons say any one of these neurons i am focusing on  and i want to change the image so that these neurons so when i say neurons i actually  mean only a single neuron but for illustration i will show multiple neurons so i want to  change the image so that this neuron fires even more so how would we achieve this  what will we do   so say this is the neuron which i want to fire even more so what is my optimization  problem first of all what are the parameters of the optimization problem  student refer time one hundred and fifty-one  the pixels of the image that is clear now i want this to fire even more so what is the  objective function what you are going to maximize lets call this neuron as hij what  you are going to maximize   student refer time two hundred and eighteen  sorry  student no refer time two hundred and twenty-one   no i want this neuron to fire more  student refer time two hundred and twenty-seven  maximize hij right i mean that is i mean why so that sort of a thing ok  but of course we will do something so that it is a neat differentiable thing and so on so  you want to maximize the activation of one such neuron hij so we could just formulate  the following optimization problem that i want to maximize hij2 ok and of course the  parameters of the optimization are the image pixels and if i consider one such pixel in  the image then i essentially need to compute this gradient  the gradient of the loss function which is hij2 with respect to this image pixel and i can  do it in these two parts the lead ability of the loss function with respect to hij and the  derivative of the hij with respect to the image pixel this we have seen a million times  while doing back propagation of course you are not gone all the way back to imn but  we saw last time that it is just one more term in the chain rule and this again looks  straightforward right the derivative of the loss function with respect to hij looks straight  forward so i have a very simple way of computing the derivative of the loss function  with respect to any pixel of the image     so now i can apply gradient descent and i can update the image so i started and now  remember that the my original i mn was not blank or random or zero or anything it is  actually the sky image so maybe it was blue or cloudy or whatever pixel that i have in  my original image and that pixel i am changing   so i have started with the sky image i have changed a bit based on this gradient update  rule gradient descent update rule and now i feed it back to the network what will  happen what will happen to hij   student fire a bit  it will fire a bit more because that is exactly how you have changed the image with  exactly that objective function right  and now if i keep doing this what will happen so remember what does h ij actually  capture now this is where so if you understand this right you will really understand and  appreciate everything about convolutional neural network and i will be sure that you  are actually understood the details and not just these boxed architectures right so what  if this happens right then what does actually hij capture it captures certain  student patterns   patterns in the image right now if hij is firing that means these patterns have started  student refer time zero457  appearing in the image we started with a sky image  and now hij is firing more and more that means it is now the image is suddenly  becoming more and more or containing more and more patterns for which h ij should  fire does that make sense ok yeah so let us run this algorithm we will start with this  image and we will run this algorithm so i will run it before that i want some guesses  what kind of patterns do you think will start appearing here and this is deep dream is the  title right ok so fine  so let us see so i will run this algorithm so what i am doing is i am starting with this  image and running exactly what i showed you that i will compute the gradient with  respect to one of the neurons and i will keep updating the image so that it becomes  more and more like the patterns that i am trying to capture so lets run this and observe  carefully it is almost a magic trick i hope this does not disappoint what do you see  student refer time zero551  most of them are what  student refer time zero553  they are dreaming so they are literally building castles in the air right so what is  happening why is this happening everyone sees castles right that is the first thing  otherwise   student laughter  ok good why is this happening have you seen the disney logo the castle what does it  have in back background how many of you find this interesting how many think this is  ok expected ok why is this happening think about training data think about what  would have happen or you missed the magic show so what is the convolutional neural  network actually trying to do  student refer time zero632  i will give you a hint its being over enthusiastic how many of you get that ok so here  is what is happening right should i explain it or no i am not going to ask you a quiz  question i am just saying that i have some more images to show ok i will explain it first  so this is what is happening right  so in the training data whenever the castle appears it is typically has the sky as the  background ok so now the convolutional neural network started drawing these  correlations so whenever it sees a sky it is trying to find a castle somewhere but  because it knows that most of the times whenever i see a sky there is a castle in the  foreground  so those neurons are firing a bit and then now you are trying to fire them even more and  more so that keep trying to change the image till this castle actually appears on the  image how do you how many if you get this explanation please raise your hands ok  so let us see some more examples right so now guess what will happen here ships ok   again a generation which thinks of  student refer time zero738   a ships is ok i shouldnt comment on that  student birds   fishes  refer time zero743   student birds  birds what else but there are also mountains   student  refer time zero747   ice ok interesting  student  refer time zero75zero   now our expectations are increase let me just run this and see what happens oops oh  no  student refer time zero756    i have my final trick ok  student laughter  the prestige is gone ok yeah so what do you see here so actually if you go back and  look at it carefully right this is very interesting a lot of fish eyes actually start appearing  here and some shapes like fishes actually start appearing here go back and look at it  carefully and all on the mountains and the green regions a lot of birds and animals start  appearing right which is again expected because in your data set you would have seen  birds and animals with a green or this kind of a background right whatever you call it a  mix of green and brown background right  so now it is trying to find those things even though they do not exist and as it try to  force it more and more it starts creating those images as you start asking to dream more  and more right and since this is about dreams i could not let this go it has to had  inception in that so what will happen here now  student refer time zero851   there is actually nothing interesting is this for my own sake that i put this unfortunately  nothing interesting happened with this  student oscars  wow  student laughter  if only but thats the point right this is so data set specific that it cant really generalize it  cannot dream beyond the data set actually nothing interesting happens it is just a lot of  these men are wearing brown suits and in the data set unfortunately all brown things  were dogs laughter so this is what will happen we will start seeing dogs appear  everywhere you see one here  student laughter  you see many here actually   student refer time zero931   it is like a few more and this would have turned into laughter something unpleasant  right so that is what is happening actually see a lot of dogs here here in many places  right  so this is its still running so what exactly is happening here the same thing that i had  detected right the network has been trained to detect certain patterns dogs cats birds  etcetera which appear frequently in the imagenet data and with these backgrounds that i  am trying to do or these textures that i have in my images it starts seeing these patterns  even when they hardly exist and now as i start focusing on these neurons which are  firing and try to modify the image to make them fire even more it will start producing  these pixels or these images in the original image right  so you can read this explanation which is from the google blog on this they have some  really some code and something on this so you can just read this explanation if a cloud  looks a little bit like a bird so that will make it look more like a bird this in turn will  make the network recognize the bird even more strongly on the next pass and so forth  until out of nowhere a bird actually starts appearing in the image right so that is exactly  what is happening so this is deep dream  
task2/super_cleaned_audios/lesson101.wav,306.5045, ok now we will go to deep art now here any questions on that ok  so now here is what here is a again an iq test right so what will happen ok so this is  deep art ok someone wanted to try this that if you take natural images or camera images  and if you have art from various famous artists and i want to render this original image in  this art form and how can i do so i will explain this the bit of a leap of faith in what  is happening here but just indulge me right so let us see    so to design a network which can actually do this we design we first define two  quantities one is the content targets so i call this image as the content image because  this is the content that you are interested in right i want my final content to look like  this for the content we would want the following thing that if i am able to create a new  image when i pass it through the same convolutional neural network we want these  hidden representations to be equal right because that is the assumption here is that the  hidden representation actually captured the essence of the image which is this face and it  is various attributes right  so if i create a new image in a different style still this content should be present in it  and my way of ensuring that or rather the way of the author’s way of ensuring this was  to make sure that the embeddings that i learn for the new image and the original image  are the same ok so i want these to be equal and i have just shown one for illustration  but you could have the same objective function for all the representations right  remember that we learn multiple representations and a convolutional neural network  so this is what my objective function would be for the content i would want that this  tensor which is the volume ijk every pixel or every feature value in the tensor for the  original image should be the same as the generated image ok and again my optimization  problem is with respect to what image i am going to change the image and this is the  loss function that i am interested in is that fine ok fine  so i think x is my original image and p is the new image which i am going to create  right     now next and here is where there is a bit of leap of faith we want the style of the  generated image to be the same as the style image so i gave you one content image and one  style image so for content the loss function is clear now for style how do you capture  the style of the image so the explanation given here and i am not very sure about this  but maybe it comes from some traditional computer vision literature but i just take it on  faith that if you have this volume here which is say sixty-four  two56  two56 or any other  dimension right then v t v which is a sixty-four  sixty-four dimensional image or matrix captures  the style of the image so this is what has been written in the original paper i am not  really dug deep but my feeling is it comes from some of the traditional literature from  computer vision right so that is not important we will just take it for granted that that  gives the image and here is the illustration for that as you go deeper and deeper so this  is if you plot the sixty-four cross sixty-four image that you got then you get different styles as you go  deeper and deeper you get a better representation of the style of the original image right  so that is the argument made in the original paper  now if you assume that this is correct then can you design a loss function for the style  part of it i want the style of the created image to be the same as the style of the style  image    so how would i do that so this is the content image this is the actual oh sorry this is a  style image correction ok so i would just want that this vt v which captures the style  and i could do it for any one of the layers or all layers depending on what i want to do i  just want that this style should be as close to each other  so i can have a similar matrix squared error kind of a function right so that is what this  is trying to capture these are the style gram so this is v t v for the style image and this  is v t v for the generated image if i pass it through the convolutional neural network i  want both of these to match    so i want the content to match i want the style to match so then what is my total  objective function going to be  student sum of these  sum of these right so this is what my total objective function is going to be i want the  content to match and i also want the style to match so i will use an objective function  which tries to balance between these two and alpha and beta are some hyper parameters ok  and if you do this and train the algorithm and try to modify the pixels along with some  other bunch of tricks then you will get this gandalf rendered in this style that you have  given right so this is again some code is available for this you can go and try it out  and it is interesting it is in a very interesting idea that you could have taken these two  things and now you could be imaginative right you could do all sorts of things with if  you have two different images how do you want to combine them and so on right so  that is the basic key idea here   
task2/super_cleaned_audios/lesson115.wav,115.7438,so let us start s last lecture we are looking at encoder decoder models and we saw that  a bunch of problems from different domains and different modalities images text videos  and so on and even this cross modal or multi modal applications where you are taking  a video and trying to describe it so video is one modality description texts is another  modality and so on  we were able to propose modals for all of these using this encoder – decoder  architecture and then we motivated this attention mechanism where we said that  encoder decoder is trying to do this silly thing where it tries to encode the entire input  once and that is what how humans do it he do this back and forth thing where at every  time step if we are trying to produce a translation or a single word in the translation we  just focus on certain words in the input sentence and kind of ignore the other  so the attention mechanism which is this bunch of equations that you see here that  allowed you a neural way of modelling attention and the key thing to note here is a there  was a supervision for the attention no one actually tells us that this is the portion of the  text which is important at time step t but they still works better because this is the better  modelling choice and i give you that bicycle analogy and also it is a better modelling  choice we are able to no one has given you these supervisions but you are still have  more parameters in the model to learn this kind of a behaviour    and then we also saw that we could actually visualise these attention based and from  some experiments on some papers we saw that actually learn some meaningful  attentions in the particular case on the figure on the on the right hand side so the one  that clearly shows that for a monotonic kind of a translation scenario between english  and french most of the attentions weights are along a diagram and that is exactly what  you would expect right  so that is where we end it  
task2/super_cleaned_audios/lesson16.wav,336.1861,so in this module we look at linearly separable boolean functions again and we will try to make some more statements about them so what do we have do so the guiding question that we have is what do we do about functions which are not linearly separable and let us see one such very simple function can you guess what function i am going to talk about all of you are paying attention in the first lecture so here is the xor function now these are the set of inequalities that result from xor function i hope right now let us see the first condition implies that w naught should be less than zero second condition implies this third condition implies this fourth condition implies this just looking at this can you tell me can you find a configuration for w naught wone wtwo such that these inequalities can be satisfied together no right because two and three want it to be greater than minus one minus w naught and when you take an addition of that it has to be less than minus one so that is not going to happen so you see a contradiction so this is a simple boolean function which the perceptron cannot handle because it is not linearly separable it is not linearly separable there does not exist a line if there does not exist a line you cannot find the line in fact you can look at it visually so these are the red points for which the output should be zero or one and the blue points are the points for which the output should be zero if we need to change this i think we were using blue as positive and red as negative and you cannot just draw a line there is no way you can draw a line such that the blue points lie on one side and the red points lie on the other side so it is a simple two input function so it is not that i have taken a very contrived example most real world data is not linearly separable and it always contains some outliers right so here maybe you have some data where you are trying to say that people which live in this part of the world belong to a certain or maybe people who live or work here have a certain qualification people who work in this company may have a certain different qualification and there might be some outliers right it is not that is always going be very clean so now what do i mean and it is not necessary that the points will only be outliers in fact there could be a clear case where there are no outliers but still you cannot find a line such that you separate the positive from the negative can you think of such an example good right this is clear data there is no outliers here as well i mean it is just saying that everyone who lies within this boundary has a certain characteristic and outside that boundary people have a different characteristic right and there is no outlier here but you cannot separate this data with a line so all functions that you deal with will not go or are not going to be linearly separable so we have to work around those right and while a single perceptron cannot deal with this we will show that a network of perceptron’s can indeed deal with such data so that is where we are headed so before going there we will discuss some more boolean functions in more detail and i will try to see what kind of nonlinearly separable boolean functions are there so first of all how many boolean functions can you design from two inputs how many can you design one6 looks like a good number from three inputs two56 how many if you understand this let us see so let us begin with some easy ones that you already know right so these are two inputs xone xtwo what is this function always off the other extreme is always on and i have already given you the answer f one6 so then you have the and function and or function then some other functions right so why did you reach one6 actually because with two inputs we will have these four values to take care of and each of these are again binary so you actually have two raise to two raise to n right so for three inputs two raise to two raise to three would be two56 now that is the easy part of these how many are linearly separable i will have to do any actually stare it in and seriously try to find the answer when you cannot really do that so turns out all of them except xor and in not of xor ok so for the two input cases there  are two functions  which are not linearly  separable for n inputs how many functions would be not linearly separable it is an arbitrary n is not the answer you are not going to disappoint me not n ok but what is the answer so for n inputs we will have two raise two n functions of these we do not know how many are going to be not linearly separable that is not a solved problem although i encourage you to go and find the answer i am looking for a good will hunting kind of a moment but all it suffices to know is that there exists some which are not linearly separable and that everyone agrees that there exists some right and as n grows probably that number will increase and so on but it is not known exactly you cannot write it as a function so what we have done so far is looked at boolean functions how many boolean functions can exist and of that we just have concluded that there would be some which are not linearly separable 
task2/super_cleaned_audios/lesson9.wav,243.5872,"so lot of fields have adopted deep learning now and lot of state of the art ai systems are  based on deep neural networks but now what is needed is after all this madness were  deep learning has taken over a lot of research areas can we now bring in some sanity to  the proceeding so this is really a need for sanity and why i say that is that because there is this paradox of deep learning so there is this interesting question that why does deep learning works so well despite having a high capacity so the deep neural networks have a very high capacity which means that susceptible to over fitting so most of you would have done some course on machine learning so there you know that over fitting is bad because you are just memorizing the training data and then you might not be able to do so well and at tested and over fitting happens when your model has a high capacity so  even though deep neural networks have high capacity why are they doing so well we will focus on this high capacity but when we talk about the universal approximation theorem and give some arguments for why deep neural networks have such a high capacity the other thing is they have this numerical instability right so we spoke about these vanishing and exploding gradients  and again we will talk about this later on in the course so despite this training difficulties why is it that deep neural networks performs so well and of course they have this sharp minima which is again it could lead to over fitting so if you look at there is an  optimization problem it is not a  neat convex optimization problem so it is a non convex optimization problem so why does it still do so well so it is also not very robust so here is an example on the right hand side the figure that you show so the first figure is actually of a panda and the machine is able to detect this panda with some fifty-seven percent confidence right we have trained a machine for a lot of animal images we have shown it a lot of animal images at test time we show at this image the first image that you see on the right hand side and is able to classify this is a panda with fifty-seven percent confidence but now what i do is i add some very random noise so that second image that you see with some very random pixels if i add it to this image i will get a new image so every pixel in this image is added to this new noise image and i get the image which is see on the third the third image that you see right to you and me or to any average human this still looks like a panda there is hardly any difference between this image and the original image but now if you pass this to the machine all of a sudden instead of recognizing this is a panda it starts to recognize it as a gibbon and that too with ninety-nine percent confidence so why is it that they are not very robust and despite this not being very robust why are deep neural networks so successful so people are interested in these questions and people have started asking these questions there are no clear answers yet but slowly and steadily there is an increasing emphasis on explainability and theoretical justifications so it is not enough to say that your deep neural network works and gives you ninety-nine percent accuracy it is  also good to have an explanation for why that happens is it that some components of the networks are really able to discriminate between certain patterns and so on so what is going on inside the network which is actually making it work so well right and hopefully this will bring in some sanity to the proceedings so instead of just saying that i apply deep learning to problem x and got ninety percent success we will also make some kind of more sane arguments just to why this works and what is the further promise of this and thinks like that so  that is roughly a  quick historical recap of where deep learning started and where it is today starting all the way back from advances in biology in one thousand, eight hundred and seventy-one to recent advances till two thousand and seventeen and so on deep learning right and here are few url so you could take a look at this for a lot of interesting applications of recurrent neural networks  bunch of startups which have come up in this space is working on very varied and interesting problems and here are all the references that i have used for this particular presentation so that is where we end lecture one and i will see you again soon for lecture two "
task2/super_cleaned_audios/lesson28.wav,1219.7059,now we will go to the gradient with respect to the hidden units  so this portion so you already see there is a repetition here and i do not need to treat  each hidden unit separately i can just have a formula for the hidden unit and then i could  compute it for all the hidden units so that is what our aim is so let us do some simple  stuff first and then you will come back to it    so suppose you have a variable x you compute two functions from that one is x square the  other is x cube i will call this as y one and i will call this as y two and i take y one and y two and  compute a z which is say a log of y one by y two now what i am interested in is this what is  the answer for this how do you get this this is a fair question to ask y one y two are  functions of x z is a function of y one y two hence z is a function of x so i can compute this  derivative and i can ask for this derivative how would you compute it if i cannot really  do this right  so if this path did not exist then it is trivial it is just the chain rule along one path but  now you have two paths so what will happen add them right so can you tell me a  formula for that so let me know if this makes sense to you ok does this make sense  now let me complicate this a bit just let me just do it as y three now  student refer time 0twoone5  what will happen  student refer time 0twoone6  that is all right so you see that if there are multiple paths you can just add up the chain  rule across all these paths right that is what chain will across multiple paths does      so with this we will go back to this figure so now i am interested in i am interested in  going to the hidden layers again i will do this to bit calculation where i first asked for  this guy and then i will ask for the light blue guy right and am going to look at one unit at a  time now what is the what am i interested in the derivative of the loss function with  respect to say d h two two right the second unit of the second hidden layer    now what i am going to say here is exactly what i had written on the previous slide this  was our final function right which was z so z was sorry again i have not chosen my  variables well ok but if so we had exactly the same situation right which is which  you see here ok so we will just have to sum up the derivatives partial derivatives across  all the paths which lead from this guy to this guy and there could be as many paths as  there can be but i do not care i will just sum across all those paths in fact actually here  there are not just two paths because we have always assumed there are k classes so there  are actually k of these paths right  so this form this is exactly the formula which i wrote on the next slide right this one but  just written in terms of the network that we are dealing with so you can just go back  and look at this but as long as you understand this figure you from my point of view we  can go ahead so everyone understands this figure that we just need to compute the  derivatives across all the paths and add them up    so now let us start we again the same recipe we will compute it with respect to one guy  and then go towards the gradient so what is this now let me explain right so dl theta  there are k of these guys between right so there are k paths so this summation has to  happen over k paths just as you told me when there were two paths the summation was two three  paths to three that is k paths of the summation over k guys the derivative with respect to  each of these guys and the k’th the m’th unit rate that is the index that i am iterating over  and then the derivative of this guy with respect to whatever you are interested  that is just that there are only two nodes in the path in the chain but there are k such  chains how many of you exactly get this ok how many of you have a problem want me  to repeat this you have problem oh many of you ok good please do this so i am  interested in this quantity that means i am interested in the partial derivative of this loss  function with respect to this guy    and this guy is nothing but h ij that much is clear is the j’th unit of the i’th hidden layer  in fact this is actually h two two so my i is equal to two and j is equal to two now i just made a  case on the previous slide that if you have such a function which first computes some  intermediate values and then your final function is computed based on all these  intermediate values right and now you are trying to find the gradient the partial  derivative of this with respect to the original input that you had  so then what you will do is you will sum across all the paths that lead from this guy to  the output how many such paths are there you already see two such paths here right but i  am saying there are k such paths because there are some other nodes here which i have  not drawn we have already said that in the output layer we have k nodes right so there  are k paths so that takes care of the first bit that the summation is going to be over the k  paths  now what is each of these paths composed of this intermediate value and this quantity  that we are interested in first we will take the derivative of the out of the loss with  respect to this intermediate value what is that that is the unit in the that is the unit in  the previous layer or the next layer rather so i am interested in i so i am looking at the  unit in the next layer hence i plus one right because that is what comes in my path the next  layer is what comes in my path we have always the special case that this guy feeds  into k guys but all the other hidden units before that feed into n guys right  so that is let us just keep that complication aside for the minute and we just look at this  case ok is that fine so we have agreed there are k paths and each path is composed of  these two nodes from the last loss function to this intermediate value and then from this  intermediate value to the quantity of interest and why is this i plus one because the next  node in the path when i am at the i’th layer  so i will be feeding to the i plus one’th layer right and in fact i will be feeding to all the  nodes in the i plus one th layer that is why i am taking or all the k paths right and then that  node which is this node with respect to the quantity that i am interested in is this clear  now right this is very similar to the toy example which i did i just have k paths now  instead of two paths there  so let us move ahead now what is ok which of these quantities do we already know is  there any quantity that we know this one why because in this special case i plus one is  actually equal to l right because we are feeding into the last layer and they have already  seen how to compute the partial derivatives with respect to the last layer  so this quantity is known we do not know this for the generic case yet but we will get  that but for this special case when we are feeding into the last layer we know this does  everyone get this ok now do we know this quantity so what you have told me is that  we know this quantity because that is what we have computed in the previous module  do we know this quantity we have to compute it can you compute it ok let us just do  it right so let us assume that this hij that am dealing with is actually h two two ok fine now  what is a i plus one m actually which are the elements there a three one and a three two i am  assuming that i only have two units in the output layer ok  so my m is equal to two now is this fine this is how the next layer is related to the  current hidden layer plus biases ok now what am i interested in one of these guys ok  let me take one of these guys so can you tell me what is a three one first row multiplied by  the first column there is only one right plus b two one   student refer time one0oneone  sorry  student refer time one0onetwo  b three one  now let me just clarify something what is this in terms of variables i j k m what  is this this is i this is j this is k this is m this is i plus one right ok this is one of the ms  that i am dealing with now i want the derivative of this with respect to hij in fact i  want it with respect to h two two where this is i and this is j is this clear what is this  derivative w three one two everything everyone fine with this now help me find this what is  this ijkm and i plus one what is this this is coming from the m how many of you see  this because that is the unit that you are connecting to and this is j so what is the  formula how many as many as the number of neurons in the next layer a bias will be  connected to all the neurons in that layer right everyone gets that right there are only two  units  so there will be only two guys ok so what is the formula for this w i plus one mj everyone  comfortable with that ok fine you can just go back and look at this and it should be  cleared right so whenever you are dealing with vectors and matrices right if you are  really good at it you can imagine the entries and figure out what is happening if you are  not good at it do not be lazy just work it out right you just need to write down this  product and at the end remember everything is always element wise and you are never  dealing with a vector or matrix now just dealing with the individual components of them  so you should always be able to compute these derivatives or partial derivatives with  respect to the individual components and that is exactly what i did here right if you  just work it out if you just write it out then you will always get it if you cannot but  eventually try to get to a point where you can just visualize it but if you cannot at least  try to work it out    so this is what it will look like ok now consider these two vectors one is this vector what  does this vector look like this is a collection of all the partial derivatives so this is just  a collection of all the partial derivatives nothing new we have already seen this now  what is this vector actually in fact i have started with the matrix and i am saying look  at this vector what does this mean this i plus one is just the layer in which the matrix is  right so that index we do not really care about for a matrix what we care about is the i  comma j index ok now what does this dot comma j mean all the i’s belonging to j that  means the dash column j’th column everyone gets this this is all the i’s or all the entries  belonging to the j’th column  so it is effectively just the j’th column so it is one comma j two comma j up to k comma j  right so these are two valid vectors now tell me what is this quantity going to be this is  the dash between two vectors dot product dot product between two vectors is a  student refer time onethree4three  is a summation over element wise thing ok i have said enough now try to connect this is  a very simple maths the column that you will ever get in your life try to connect this to  something which is already there in the slide how many of you think the answer is this  this into this plus this into this plus this into this and just write it as a formula you will  get this everyone sees that ok so now i have a compact way of writing one of these  entries    one of these guys i have a compact way of writing this it happens to be the dot product  between two vectors one of them is the gradient but do i know this already do i know this  quantity already in this special case yes because i plus one is equal to l and that i have  already computed this of course i know right because these are the weights that i am  dealing with where do i go from here this dot yeah it means anything from that  column so that means the entire column  student refer time one448  ah no these are weights right so this is a weight matrix it has columns and rows i am  talking about the j’th column so i fixed the value of j i am talking about the j’th  column but i am not telling your given i’th entry there am just telling you all the entries  there that just means the j’th column you can take this offline ok this is very simple i  will take it offline ah now where do i go from here  student refer time one5one6  i plus one  student refer time one5two0  ok no in this specific case are we done  student refer time one5two7  where are we right now with respect to one unit where do we want to go the entire  thing so what is the quantity that i am interested in gradient with respect to always say  with respect to h i right  student refer time one544  where i is two in this case this special case ok what is that going to be collection of all  these guys that you have already computed ok now simplify this what is this first  column of the matrix multiplied by the same vector the second column of the matrix  multiplied by this vector the nth column of the matrix multiplied by this vector this  reminds you of something very very difficult this is a very very complicated matrix  multiplication right  first row of the matrix multiplied by a column the second row of the matrix multiplied  by column how many if you get this right so this is can you tell me what this is wi  plus one transpose  student refer time one65two  perfect right so now you see that this entire quantity we can compute in one go by  using a matrix vector multiplication right so that is what i meant when i was saying  that we should not be doing these unusual computations but we able to compute that at  one row right so now we can just do this matrix vector multiplication and get this entire  quantity ok now what is still missing in this module  so what is the special case that i have assumed i told you that i already know these  quantities but only if i plus one is equal to l i need to tell you this in the generic case ok  so we are almost there except that i do not know this when i is not equal to l or i is less  than equal to l minus one ok that is the case that i am looking for      so that is again very simple again what will i do i will compute it with respect to ok  what is this this is the guy that i am interested in the generic i not the l’th one right  the generic i this is what the vector looks like the gradient vector looks like i want each  of these guys ok now i will take one of those and i will write it as this ok what am i  doing am saying that i already have the entries up to here ok at a very general level  even here i could have said the same thing remember that i had said that the output layer  you can always write as hl right  so even at the output layer i could say this chain rule always holds how many of you  agree with that i want to go from the loss function to one of the lighter blue guys so  am saying that i can go through the intermediary dark blue guys that is all i am saying i  have just compressed this entire path into up to the dark blue guy remember i had said  earlier that i will be compressing this chains now how many of these quantities do you  know the first one is what we computed on the previous refer time one85two the  second one looks very difficult sorry  so h ij is nothing but sigmoid of a ij or any nonlinearity of the a ij so i can just write  this derivative as i will just write it as sigma prime ok    or g prime is this fine now i have it with respect to one unit what will i do go to the  gradient fit it all these values now simplify this what is this a vector right what is  this another vector there is a one to one correspondence between them so you have two  vectors and you are doing a one to one multiplication what is this  student refer time one94three  how many of you say dot product dot product is always a what is the output here  student vector  can it be a dot product can it be a dot product no please empathic no ok so what is it  going to be an element wise multiplication and this is how you denote that ok so what  is this called you had a multiproduct right so this is every element of one vector  multiplied by the corresponding element of the other vector ok so now again the entire  vector we can compute at one row right i am not i am when i am teaching this i am telling  you how to compute one element and then go to the gradient but when you are going to  implement this we are just going to compute the gradient at one go  
task2/super_cleaned_audios/lesson14.wav,773.0607,we will now go to the next module which is the perceptron learning algorithm  we now see a more principled approach of learning these weights and threshold but before that we will just again revisit our movie example and make it slightly more complicated  now here what the situation is that we are given a list of m movies and a class associated with each movie indicating whether we like the movie or not so now we have given some data of the past m movies that we have seen and whether we like this movie or not and now instead of these three variables we have these n different variables based on which we are making decisions and notice that some of these variables are real they are not boolean anymore the rating could be any real number between zero to one ok and now based on this data what do we want is the perceptron to do actually  so i have given you some data these factors i have also given you the label one and zero so if the perceptron if i tell you my perceptron has now learnt properly what would you expected it to do perfect match so whenever i feed it one of these movies it should give me the same label as was there in my data and again there are some movies for which i have a label one which are positive and some movies which i have a label zero  so i am once again looking to separate the positives from the negatives so it should adjust the weights in such a way that i should be able to separate so that is the learning problem that we are interested in  so now with that i will give you the algorithm  this is the perceptron learning algorithm  we have certain positive inputs which had the label one we have certain negative inputs which had the label zero and now i don’t know what the weights are and i have no prior knowledge of what the weights are going to be i need to learn them from the data  so  what i am going to do is i am just going to initialize these weights randomly as i am also going to pick up some random values for this so this should be small n so this should be small n and now here is the algorithm while not convergence do something  so before i tell you what to do can you tell me what is meant by convergence when will you say that it has converged when it is not making any more errors on the training data right or its predictions are not changing on the training data  so  that is the definition of convergence now here is the algorithm i pick up a random from point from my data which could either be positive or negative so it comes from the union of positive negative basically all the data that i have i pick up a random point from there  if the point is positive right and this is the condition which happens what does this tell me if the point was positive what did i actually want greater than zero but the condition is less than zero that means i have made an error so i have made an error then i will just add x to w i see a lot of thoughtful nodding and i hope you are understanding what is happening let us see so what is w actually a dimensional  n dimension n plus one right because w naught is also inside there  so  actually there should be w naught also here right and what is x again n dimensional right and that is why this addition is valid so let us understand that w and x both are n dimensional now let us look at the other if condition can you guess what the other if condition is if x belongs to n and  summation  is greater than equal to zero then so that means you have completely understood how this algorithm works well that is so now consider two vectors w and x so remember what we are trying to prove is or get an intuition not prove actually get an intuition for why this works ok so we will consider two vectors w and x and this is what my vectors look like very similar to the case that we are considering wzero to wn and one to n so this again x naught is just one now this condition that i have been talking about is nothing but the dot product how many of you have gone through the prerequisites for todays lecture ok good so it is just a dot product now we can just read write the perceptron rule as this instead of the dot product i mean instead of using that summation thing we can just say that it is a dot product  now we are interested in finding the line w transpose x equal to zero so that is our decision boundary which divides the input into two halves now every point on this line satisfies the equation w transpose x equal to zero what does that mean actually so just a simple example is that if i have the line xone plus xtwo equal to zero then all the points which lie on the line satisfy this equation so you could have one minus one two minus two and so on but two two is cannot be a point on this line at every point lying on this line satisfies this equation so every point lying on this line actually satisfies the equation w transpose x equal to zero  so can you tell me what is the angle between w and any point on this line how many say how many of you say perpendicular why dot product is zero so if the dot product is zero they are orthogonal so that means if i take this line then my vector w is orthogonal to this it is orthogonal to this point or this point to this point to every point on the line which is just the same as saying that the vector is perpendicular to the line itself right as simple as that so the angle is 9zero degrees because the dot product gives you the cos alpha and that is zero right and since it is perpendicular as i said to every point of the line it is just perpendicular to the line itself  so this is what the geometric interpretation looks like this is our decision boundary w transpose x and the vector w is actually orthogonal to this line and that is exactly the intuition that we have built so far now let us consider some points which are supposed to lie in the positive half space of this line that means these are the points for which the output is actually one now can you tell me what is the angle between any of these points and w or you guys are actually trying to tell me the angle we have got some measuring stuff no so i will give you three options i e equal to 9zero greater than 9zero and less than 9zero  less than 9zero it is obvious from the figure now if i take any point which lies in the negative half space what is the angle going to be between them it is greater than 9zero again obvious and it also follows from the fact that cos alpha is w transpose x by something and we know that for the positive points w transpose x is greater than equal to zero that means cos alpha would be greater than equal to zero that means the angle alpha would be less than 9zero degrees and for the negative points w transpose x is actually less than zero that means cos alpha would be less than zero that means alpha would be greater than 9zero degrees  so it actually follows from the formula itself but it is also clear from the figure so keeping this picture in mind let us revisit the algorithm so this is the algorithm  now let us look at the first condition which was this now if x belongs to p and w transpose x is less than zero then means that the angle between x and the current w is actually greater than 9zero degrees but what do we want it to be less than 9zero degrees and our solution to do this is but we still do not know why this works now anyone knows why this works so let us see why this works  so what is the new cos alpha going to be it is going to be proportional to this it is going to be proportional to this i will just substitute what w new is fine that means if cos alpha new is going to be greater than cos alpha what is alpha new going to be it will be less than and that is exactly what we wanted this angle was actually greater than 9zero degrees so you want to slowly move it such that it becomes less than 9zero degrees it is not going to get solved in one iteration and that is why till convergence  so  we will keep doing this i will keep picking xs again and again till it reaches convergence that means till we are satisfied with that condition  let us look at the other condition x belongs to n and w transpose x was greater than equal to zero then it means that the angle alpha is actually less than 9zero degrees and we want it to be the opposite i will just quickly skim over this w minus this x ok i forgot to mention that this is actually a positive quantity i mean that is why that result holds that means  cos  alpha  new  is  going  to  be  less  than  cos  alpha  and  this  slight  bit  of mathematical in correctness i am doing here but that does not affect the final result  so i will just gloss over that and you can go home and figure it out but still it does not take away from the final intuition and interpretation so now the new cos alpha is going to be less than the original cos alpha that means the angle is going to be greater and that exactly what we wanted  so we will now see this algorithm in action for a toy data set  so this is the toy data set we have and we have initialized w to a random value and that turns out to be this i just picked up some random value for w and ended up with this particular configuration for w  now we observe that currently w transpose x is less than zero for all the positive points and it is actually greater than equal to zero for all the negative points if you do not understand w transpose x it is just that the all the positive angle points actually have a greater than 9zero degree angle and all the negative points actually have a less than 9zero degree angle so this is exactly opposite of the situation that we want and now from here on we want to actually run the perceptron algorithm right and try to fix this w how does it work remember we randomly pick a point so say we pick the point pone do we need to apply a correction yes why because it is a positive point and the condition is violated so now we add w equal to w plus x and we get this new w so notice that we have a new w we again repeat this we again pick a new point and this time we have picked ptwo do we need a correction yes at least from the figure it looks like the angle is greater than 9zero so we will again do a correction we will add w is equal to w plus p this x is actually sorry ptwo and this is where we end up  now again we pick a point randomly none do we need a correction so this is what our w is this line here and none so we need a correction now what is the correction going to be it will be minus and then the w changes  now we pick another point n3 do we need a correction no at least on the figure it seems like the angle is greater than 9zero and we continue this  for ntwo we do not need a correction now for p3 again we do not need a correction  the angle looks less than 9zero sorry actually it is we need a correction the angle is slightly greater than 9zero and this is our correction and now we keep cycling  now as i keep cycling over the points i realize that i no longer need any correction  it should be obvious from the figure that for this particular value of w now all my positive points are making an angle less than 9zero and all my negative points are actually making an angle greater than 9zero that means  by definition now my algorithm has converged so i can just stop it so i can just make one pass over the data if nothing changes i will just say it has converged now does anyone see a problem with this it will never converge in some cases so can someone tell me why we are considering only cases where the data is linearly separable that we already assumed so what you are trying to tell me is that you are going over these points cyclically so let me just rephrase and put words in your mouth that what you are trying to tell me actually is that i take a point i adjust w but  now for the next point i maybe go back to the same w because that point asked me to move it again and i keep doing this again and again and basically end up nowhere that is why this will never converge that is exactly what you are trying to tell me  now that is exactly what i am forcing you to tell me  so  that is not the case this algorithm will converge  
task2/super_cleaned_audios/lesson103.wav,465.6056,through time bptt vanishing and exploding gradients truncated bptt  in this lecture we will talk about sequence learning problems and in particular some  neural network architectures which deal with sequences so recurrent neural networks is  what we are going to see so we will start with the first module which is on sequence  learning problem  so what are sequence learning problem so so far we have dealt with two types of  networks one is feedforward neural networks and the other is convolution neural  networks and both these networks the input was always of a fixed size  so what do i mean by that is if you take a convolution neural network you are feeding  thirty-two  thirty-two images to it or two hundred and twenty-seven  two hundred and twenty-seven images to it and this size will always fixed all your  training images all your test images were always scaled or cropped to this particular  size ok similarly when we used feedforward neural networks so one example was  wordtwovec the size of the input was always fixed we had this input of size two v right or k  v in general if you are looking at the k word window right   so this input was not varying from one training instance to another training instance or  one training instance to the test instance or anything and secondly each input to the  network was independent to the previous or future inputs so i pass an image of an apple  i get the prediction apple then i pass some other image to the network and i get a  different prediction it does not matter whether my previous image was a apple or a car  or a mango or whatever it just reads each of these inputs independently there is no  dependence between the inputs and the size of the inputs is fixed     but in many applications the input is not of a fixed size so and also successive inputs  may not be independent of each other so let us understand this with the example of auto  completion that all of us are used to while typing smss or whatsapp or other things   so given the first character d i want to predict the next character which is e then once i  have predicted e i want to predict the next character again and so on till i get the full  word ok this is what my task is     so let us notice a few things first successive inputs are no longer independent if i  know that the previous input was d and the correct input is e then i know that only a few  things are possible right in particular if you know that the previous input was a z and  the correct input is a e then most likely the next is going to be a b right but if you  ignore the previous input which is z then after e there are many things which can appear  right so the inputs are no longer independent of each other   and the second thing is the length of the input is not fixed because words could be of  arbitrary sizes i am trying to type the word deep that is four letters or if i am trying to type  the learn which is five letters machine which is seven letters and so on right so the input size  is no longer fixed and the inputs are now dependent on each other right there is some  dependence between so now this is very different from what we saw in convolutional  neural networks and feedforward neural networks so how do we deal with this   and the third thing here is that each network now i am calling this as a network and i  will just clarify some notations also soon each network is actually performing the same  task it is taking as input a character and it is producing as output one character and now  just remember that these networks i have drawn them vertically you are used to seeing  them as this so this is input this is your hidden layer and this is your output so this is  the green part this is the blue part and this is the orange input and this is the fully  connected layer right   so each of these boxes is actually this network ok i have just drawn it more concisely  because i need to draw many such networks so everyone gets that just remember this  mind that each of these orange blue green structures is a fully connected network like  this     so these problems are known as sequence learning problems where you have a sequence  of inputs and then you need to produce some outputs and each input actually  corresponds to one time step so this is the input at time step one time step two time step three  time step four and so on so let us at some more examples of such sequence learning  problems     so one classic example is the task of predicting the part of speech tag of every word in a  sentence right so i am given a sentence man is a social animal and for every word i  want to predict whether it is a noun or an adverb or an adjective or a verb or any other  part of speech type right and this is how it happens   now notice that once we see an adjective in this case social we are almost sure that the  next what is going to be a noun or at least we are sure that the next word cannot be an  article or most likely it will not be a verb right there is a very high prior that the next  word is going to be a noun so that is why these inputs are actually dependent on each  other   so the current output not only depends on the current input it is also actually depends on  the previous input right unlike the case of convolutional neural networks where i  feeded an apple it is no dependence on whether the previous input that i pass to the  network was an apple or a car or what not and the size of the input is not fixed because  these sentences could be of arbitrary lengths i could have sentences as small as three to four  words or as long as twofive to three0 words right so average wikipedia sentence for example  is twofive words roughly twofive words   and notice that and this case we are interested in producing an output at every time step  because for every input i want an output and each network again this orange blue  green structure is performing the same task its taking as input a word and its producing  an output what is producing as an output part of speech tag    so here the two examples that we saw we were having an input and every time step and  an output at every time step but there could also be cases we are interested in producing  the output only at sometime steps or at the final time step so let us consider task of  predicting the polarity of a movie review right sentiment analysis   so i am given a movie review and after i have read the entire review i should give a  prediction right otherwise it would be incomplete i cannot actually look at only this  word and give a prediction does not make sense i does not also make sense to make a  prediction here at this point because there could have been the movie was boring but i  still loved it or but the action was amazing or something like that it because it could have  already flipped after that so i need to look at the entire sentence and make a prediction  but you are not interested and prediction as these intermediate time steps   even in this case you can actually assume that every network is performing the same task  its taking a word as an input and it is producing some output it just that till the end you  do not care about you outputs you care about the output only produced at the final step  you do not care about what the outputs are at this time step right that is one way of  looking at it   so again at every time step we have the same network but you are only interested in  some time steps of the network ok     finally it is not always necessary that sequences are composed of only words what  other kinds of sequences are you familiar popular sequences speech is one video is  another  so a video could be treated as a sequence of images and now you could have a video  where some someone is performing surya namaskar and as you can understand that i  need to look at the entire sequence and only then be able to make a prediction right if i  stop at this point if i only consider this this is only namaskar no surya namaskar right  so you have to look at the entire sequence and then decide what the output is and you do  not care about the intermediate outputs i do not care what is the prediction till this point  this of course is again some aasan but i do not care about that i care about the full  sequence that i am dealing with it is just to motivate that sequences can be of all types   and i apologize to the speech people i do not really understand much of speech  processing so i never give speech examples but video is something i understand so i  can give examples on that   
task2/super_cleaned_audios/lesson117.wav,1176.7809,so we will go on to the next one which is hierarchical attention so again something  very popular in nowadays become very common for various things so again not very  difficult idea to understand  so let us first look at the motivation for this and then we will look at the solution so  consider a dialog right today everyone is interested building chatbots every second  start up wants to build their own chatbot and every second startup out of that they wants  to build for the agriculture domain or the banking domain or the healthcare domain so  here is what a typical dialog looks like right this is of course not for any profound  purpose this is but you can see this is an important dialog right very relevant and very  important so this is what a dialog looks like  so let us try to break it down into the kind of entities that we deal with so can you tell  me about a dialog what is a dialog it is a dash think in terms of things that we have  discussed so far   student refer time one hundred and seven  sequence good right again the safest answer is sequence from now on no it is only  for one lecture it is a is it a just a sequence or sequence of sequences right  so it contains a sequence of utterances so each of these lines here is an utterance and  each utterance in turn is a sequence of words right ok so what we have here is a  sequence of sequence as input and this is very common in many many applications right  so can you think of an encoder for such a sequence of sequence rnn of rnn’s good  that is the answer right    so we think of a two level hierarchical rnn encoder so first leveller will encode the  utterances ok let me ask you few questions is there is a mistake in a diagram should  this be connected yes no maybe do not care ok second question i will write some  parameters here right what is our notation w u this is u right and this is w right is it  fine if i have a dialog which contains one hundred utterances what will happen that is a  practical problem  but more conceptually what is wrong here what is each rnn trying to do i encode a  sentence encode an utterance so why should it be different for the first utterance  second utterance third utterance and so on right all these rnn’s should be the same  does that make sense the u one is equal to u two is equal to u three and w one is equal to w two is  equal to w three ok is that fine everyone agrees with that so now can you tell me if there  is a correction should be there or not  conceptually what is each rnn doing encoding   student refer time 0twofour7   one sentence right then why should it be connected to the previous sentence but then if  you do not know all these sentences then how will you predict the utterance the next  response rather what is missing here what kind of a what was the title of this module  so what is missing where is the hierarchy right    so what we will have is this right so each of these green guys is presentation for one  utterance in your input in fact the red red guys sorry the red guys are the  representations for the utterances in your dialog and then the green guy is a sequence of  utterances right so remember you have the sequence of sequence of words  so the red guys are the sequence of words and the green guys are the sequence of  utterances does that make sense ok how many of you get this please raise your  hands ok good so and now what would the decoder be you have a hierarchical  encoder what could the decoder be what is the decoder have to do it has to produce a  everyone   student sequence of words  sequence of words so what kind of a decoder will you use just an rnn not a  hierarchical rnn the input is hierarchical why should not the output be hierarchical  how it would look unbalanced right the diagram will not look very neat right if you  refer time 0three57  do you need a hierarchical and decoder no right i mean just a simple decoder because  the decoders has to produce a sequence of words so it will take something from the  encoder what will it take from the encoder in the normal encoder decoder paradigm not  the attention based paradigm what will be the input to the decoder  the last dash vector your options are state that is very safe answer here the last state  vector what will that means your options are orange blue green and red   student green  green the last green vector that is what the input is going to be right so this is what is  going to look like this is option one what is option two feed it to every time step that is  what option two is going to be ok so that is you have your hierarchical encoder decoder  network    what is missing here attention ok so let us look at another example consider the task  of document classification or summarisation what is the difference between two in  classification what the what were the decoder be feed forward neural network with the  softmax what would be decoder be in the case of summarisation  students rnn  rnn good what is the document sequence of sequence not sequence of sequence of  sequence it can be a sequence of sequence of sequence also right i did not think of that  then it could be a sequence of sequence of sequence of sequence ok let us look at the  not so funny case which is sequence of sequence what is the sequence of sequence of  it is the sequence of sentences which in turn is a sequence of words which in turn is a  sequence of character we will not go there we will just keep it till words so it is a  sequence of sequence so again you need some kind of a hierarchical encoder here right  so will encode each sentence then you will treat the sentence sequence as a sequence  encode that and then you will pass it to the classifier  now think of this problem right now if we want to do document classification how  would you go about it actually you want to classify whether this is a politics or sports  or health or whatever refer time five hundred and fifty-seven i think in terms of attention what would you  do actually first we will find the important words in the sentence to find the important  words you will have to read all the  students refer time 06one0  if you want to find the important words you will have to read all the sentences so what  will you do first find the  students word in sentences  word in sentences and then important words within the sentence so what kind of an  attention mechanism do you need  student hierarchy  hierarchical right      so let us look at this so first let us look at the our data model paradigm so what is the  data given to you i given a document and the class table for the document and your  first thing is the word level encoder which looks like this can i will not explain what this  is i will just expect you to know what this is why do i have two indices for h what is i  and what is j i is the dash id sentence id and so it is the jth word or the ith sentence right  that is what i am encoding and how many sentences am i encoding the number of  sentences in the document ok  and what is the second encoder so diagram is absolutely clear but the equations are  not the diagram is absolutely clear right there is no just need to map the red blue  orange green guys to the equations ok so let me ask you this what is hij no in the  diagram blue red orange  student refer time 07twothree  what is wij how did you write the rnn equation time step t is equal to rnn of t  minus one and the input at time step t right what is the input at time step t here  student word  word right probably this was not a good choice maybe we should make it xij w i think  we might get a confused with the weight’s we should not but they are so fine so wij is  actually the input word what is h i j minus one now tell me what colour is w i j it is like  an iq test at which colour map it w i j maps to which colour  students orange  orange good and h i j  students blue  blue but what about the red that is which colour i mean sorry not which variable  students refer time 08one5   hi ti that is the last state of every sequence right t i is a length of the ith sentence right  ok now what about hitwo the green guys right and what is this htwok   students refer time 08two9   the last green guy this guy ok is it fine    and then the decoder is just a softmax we do not need to go with that and loss and  everything is fine so this again whatever it is we should always be comfortable and  writing the end to end equations from x to y right and you can write it in this case    now let us make it a bit interesting how would you model attention in such a  hierarchical encoder decoder model how many attention functions would you need  two one for attention over sentences the other for attention over   students works  works ok can you think of these equations not a very big stretch from what we have  done already right i mean at level one it should be straight forward at level two just ignore  level one  how many if you can imagine the equations it is not very hard i am not joking i am i  mean just think about it and the level one should be straight forward because that is just  the same as ok so first we need to attend to the most important words in a sentence and  then we need to attend to the most important sentence in a document ok    let us have see how to model this so we have document again the same input then  you have the word level rnn ok now what be the word level attention equation look  like    i am looking for the attention equation for words what are the indices going to be j i j  t ok what is i j t that is i have put as superscript in w this is the word level attention  so at the tth time step i want the importance of the  student refer time one hundredfour  jth good right what would that equation depend on   students the word   the word should be straightforward right it should depend on oh but oh ok sorry this is  only for this guy right ok so you have focusing on one of these so you trying to find  the importance of these three words right which are there in the first sentence  so you have computed h i j that means you have computed all the representation for  each of these word that means you have computed the first three blue vectors that you  see ok and then you are computing the attention no so this is oh so instead of having  it here this is how we have been writing at right  student refer time one0four9  ok i so sorry i should have check this so read this as u i j is equal to or let me just  explain it so remember this is a vector and we wanted to do this operation to make it a  scalar right so u w is that parameter which was getting multiplied earlier so we had  this attention equation as u w transpose tan h of something right so now that u w has  been removed from here in equation two and has been added as exponent to equation to the  alpha equation does is that ok  how many of you completely confused please raise your hands how many of you  understand this completely once can the sum the one ok aa shit what did i do ok let us  just see if i can still salvage this ok let us go one by one so what is let me just delete  some of these things let us try to write it on our own right so this is what we are trying  to do  so i will i am ignoring the sentence id rights so this is sentence one two and three so let us just  focus on one sentence and the same equations we hold for the other sentences also ok so  first of all the attention weight would depend on what it would depend what are we  trying to pay attention to   student words  words so it should have word in the input right what else can you at have in the input  student previous  previous state unfortunately for this problem do we have a previous state   student refer time onetwotwo7  no we just doing one prediction right there is no rnn here we just the feed forward  network do you have any s t minus one in the output that we have put here so this was  the importance of the jth word at the tth time step so j belongs to the input and t belongs  to the decoder right in this case does decoder have multiple time steps there is only  one time step of the decoder right that is the problem which we have run into  but let us assume instead of classification we are trying to do summarisation that means  we are given this document and we were trying to generate summary of this and let us  say the summary was the following ok so this is the summary that you are trying to  generate from this document ok and now this summary has three or four time steps if you  count exclamation as the last time step now how is it going to be what is the decoder  going to be in that case rnn right and the decoder will have some k time steps ok  now at every time step at a given time step t what am i trying to do   in next assignment try to develop a better eraser for this ok    so we want to compute when you compute the attention for a word the jth word in  particular at that tth time step right and we have for a minute understood that we do not  have a feed forward network at the decoder we have a recurrent neural network at the  decoder because we are trying to generate a summary ok we are trying to generate a  sequence at the output so at the tth time step we are interested in understanding which  of the document was to pay attention to ok so now that is going to be a function of  what and i finded a bit irritating for the want of a better word that at least one input to  this function should be straight forward right what is it  student the word  the word that you are trying to learn how much attention to pay to right so that should  be very straightforward so that should be w one j because i am considering the first  sentence right now ok is that fine for the first sentence i am trying to find out which  are the words which are important what is the second input that you could put in it  should depend on the index t right so it should be the previous state of the decoder ok  and then of course i will have a this is again actually not alpha but e right and then  you get how do you get alpha from there how do you get the alphas  student softmax  softmax is that fine ok so alpha will be some softmax of the e’s is that ok fine so  this is for the word level now the equations that are written on the slide are slightly so  the equation has slices slightly differently written so let me just go back and write our  own equation and so we want to write an equation for this what was our equation    ignore the equations on the slide it was something like this v transpose tan h of w  student s t  stone uwonej   student b  b ok now imagine that your decoder is a feed forward neural network what will be  missing in that case there is no s t minus one there is no previous state of a decoder  because we just want to make a prediction once by paying attention to all the important  words and sentences in your document right so which part will go away wstone ok fine  now the other thing that you are doing is alpha was actually or other alpha i j is actually  exponent of e i j divided by summation of other k sorry alpha j t e j t e k t is that fine  it is just the softmax equation is that ok right now the only thing that you see different  and these two equations here is first you do not have the w s t minus one because the  decoder only has one time step and second we have taken out this v transpose from here  and instead we have added it here is that ok does that make sense this is just  different way of writing it so again you write the attention equation for the words now  ok now what about the sentence now first of all earlier what were we using for the  green guy what was the representation for the green vector it was the what was the  green vector  in the absence of attention what was the green vector h t i right the last time step of  sentence one is it fine everyone with me please raise your hands if you are with me ok  now what would it be it would be a dash sum of w vectors a weighted sum attention  weighted sum right so that is exactly what this equation is capturing so what did you  saying is there is representation of sentence i is a weighted sum of the representations of  all the words in that sentence is it ok   so that we will get a representation for s one s two up to s capital k all the sentences that  you have now what do you want to do for the second level what do we want we want  to compute the importance of that sentence for the tth time step right so let us call that  beta so i am interested in beta if we need to really read out this i said again alpha is  being used in both the places right  so you want to find out the importance of the jth sentence at the tth time step what is  it going to be a function of one is a sentence representation what is the sentence  representation given by   student s j  s j and what else the decoder state at the previous time step right does the decoder  have a previous time step state here no so it will just depend on s j and that is exactly  what this equation is capturing right and again the same trick that i have added this  extra parameter to the exponent is that fine and the final representation being fed to  the feed forward network is a weighted sum of the sentence representations right so this  again has to be sisi ok   i really sorry about this but i am pretty sure that once we correct the slides and then you  go back and look at it should be clear right it just two sets of equation one set of  equation sorry os sorry that is correct sorry so this the idea is there are two sets of  attention mechanisms for each you will have your own set of equations the basic form  if you can work out what the f attention would depend on the actual form would depend  from would differ from paper to paper or the toolbox to toolbox that does not matter so  much you just need to know that you have these as the input you are going to add some  parameters to every input that means you are going to do linear transformation and  then you just need to make sure that alphas eventually turn out to be scalars right that is  why you will have this additional vector getting multiplied at one point  
task2/super_cleaned_audios/lesson116.wav,648.9841,and so now in this lecture we will go on to the next module which is talking about  attention over images so let us first motivate why is it so different and what could be  done there right  so the question is how do we model an attention mechanism for images    so in the case of text we have a representation for every location of the input sequence  right so every location in the input sequence in the case of text was a word and then  you are looking at this problem of transliteration every location was a character and  whether it is a character or a word for everything it was discrete  so we could just know that this is the important time step t and then we know that along  all the inputs at different time steps you want to pay attention to certain time steps right  so that the definition they was very straightforward right    now for images what do we do so for images we typically take the representation from  a cnn right it could be fc seven or any of the convolution layers or max pooling layers  right now there is no concept of time step there right because the entire image is given  to you at one go  so now how do you decide where to pay attention to but if you think about it does  makes sense at least the motivation is very clear so for example for this figure if i am  trying to generate the description as man throwing a frisbee in a garden or in a park or  something like that right  so when i am generating a word man i would want to focus only on the man and not  focus on any other part of the image similarly when i am generating the word frisbee i  would like to focus on this may be when i am saying throwing i would like to focus on  his hand action or something like that and in a park i would like to focus on the  background and so on so it does make sense that each word in the description is  complete covering from coming from a different space in the image or different position  in the image  but the representation that we use say the fc seven representation that doesnot contain any  location information it just a flat and vector that we had so now how do we do this  how do we get attention on locations is is a motivation and the problem clear and  motivation is straight forward the problem is that we are using fc seven representation is  just a flat vector remember that was the fully connected vector and does not have any  location based encoding  so if for example if the fully connected vector is of size five hundred and twelve i cannot say that the first twelve  or first twenty-four of these five hundred and twelve dimensions correspond to this set of pixels the next twenty-four  corresponds to this set of pixels and so on right so that is the problem how do i what  do i attend to how do i decide where to attend to because that is what i am saying that  the vector the elements of the vector or the dimension of the vectors do not have any  semantic right that is not that the first dimension corresponds to first location  what you want an attention on this locations in the image right but the first dimension  in the vector fc seven vector does not correspond to any specific location in the image you  know that was a fully connected vector right so it corresponds to everything in the  image  so what something simpler than that why do i say something simpler than that object  detection is itself is a in itself is a another convolution neural network which does this  and so on right and we saw this past or seen in past class seen in problems  now let us solve the problem at hand the problem at hand is that i want i will just  rephrase a problem definition so that the answer becomes obvious i want a  representation which allows to give which allows me to get some location information  no but the fully connected layer if you back track it was fully connected by definition  right the answer is really straight forward the problem only arise at the fully connected  layer right because that is fully connected but what about the outputs on the  convolution layers do they have position information   student yeah sir yes    we saw that suppose this is vgg whatever it is sixteen i guess and this is what i am saying  as a problem because this was fully connected so you do not know that this dimension  corresponds to location one location two and so on but if you look at the convolution layers  we know that everything here actually goes back to some location in the image and if i  learn to pay attention to this guy maybe i am paying attention to some equivalent portion  in the image does that make sense right   now can you build on this intuition and tell me i want this as a solution right so in the  case of words these are the word vectors that i had at every location and i was learning to  pay attention to them let us learning these alphas for each of these now what is the  corresponding diagram for images what is each of these box is going to be so that we  learn the attention weights what do you mean by pass  student refer time four hundred and forty-nine  no so maybe i am not understanding your answer what is the equivalent of this this  box which i have highlighted there between the image and some attention weight so  you are directly going to operate on the image remember attention weights are never  given to us no one will mark that man is this frisbee is this and park is this or  whatever there is no supervision same size as a convolution what now what is the size  of a convolution  let us take the last one right so what do you mean by size you mean five hundred and twelve or you  mean one4 or you mean the other one4 that is the size of the output five hundred and twelve cross one4 cross one4  so what do you mean by the size five hundred and twelve or one4 or one4 or five hundred and twelve cross one4 cross one4 channel  does not make sense because channels capture i mean you do not want to focus on the  red part or the green part or the blue part in some cases you might that is correct ok   student refer time five hundred and thirty-nine  these are all partially correct answers going in the right direction let just think a bit  more and see so probably between these two they gave some part of the answer now  can you think of it so you want a representation for the image first of all of us are clear  that we do not want to work with the raw image right that is all of us are clear with that  the second part is we are going to work with the convolution neural network we want  to pick up a representation for the convolution neural network which gives us location  information right  and we agree that the fully connected layer does not give us the convolution layers give  this ok now i am asking you to focus on one of the convolution layers which is five hundred and twelve  cross one4 cross one4 so let us see from there how we will be try to get these attentions ok  so the output of the fiveth convolution layer or five c this is i think this whole thing is five and  this is five a five b and five c right these this is how the code or the general architecture is  numbered  so this guy has one4 cross one4 locations right the five hundred and twelve cross one4 cross one4 output so it is five hundred and twelve  channels but the number of locations is one4 cross one4 and we have seen that each of these  one4 cross one4 locations corresponds to certain portion in the image right    now for each of these one4 cross one4 locations that means i have one96 such locations and  for each of this how many dimensional representation do i have i want everyone to say  this  student five hundred and twelve  five hundred and twelve because we are reading it from the figure no what you are taking is the one  taking this pixel can you see what i am highlighting i am taking it across the depth  right so that is why five hundred and twelve dimensional representation of one pixel in your output volume  and how many such pixels do you have  student five hundred and twelve  one96 and each of these pixels corresponds to some real location in your image that  means it has space information right ok so now these are the one96 locations that you  have now this looks very much similar to that diagram that we had for words so now  you can think about that you have one96 items in your sequence and now what will you  do we will learn to pay   student refer time 0seven49  so what would that look like    so at every time step we will have an equation for alphas right let us try to write an  abstract equation right first of all give me the indices of alpha what does alpha  compute the importance of the of the jth location at the pth time step fine is that ok  what should this be a function of second part is kind of obvious so let us call these as  hone to hone96 so these are the j’s or the t’s j’s or the t’s  student j’s  j’s so what would the second parameter be  student h j  h j and what is the first parameter be what is the decoder in this case we are trying to  generate a caption that means we are trying to generate a sequence that means what  will be the decoder be  student rnn  rnn so what should i depend on st or stone  student stone  stone s t is the current thing right that we do not know yet so it will depend on st minus one  comma h j of course you can make it depended on several other things also but at the  minimum you will see these two things right because you are trying to understand the  importance of these guys so these better participate in the function and you are trying  to compute the importance at current time step so you better know what has happened  till time step t  one it is not very different from the attention equation that we had written  in fact it is a same actually  and what was one form of this attention that we had seen does anyone remember that  we had carefully analysed the parameters and the dimensions of that form what should  the output of this function be scalar vector matrix scalar right  so what is the form that we had seen for this function vt tan h of something you  should get comfortable in writing these equations right because that is what you will do  if you are proposing your models and so on right so you will see okay in the previous  model this depended on the following two quantities i think it should depend on four  more quantities so i will write a new equation just think about it there are two inputs s t  minus one and h j what will be doing with each of these inputs do a introduce some  student parameters  parameters so what will you do  student w stone  w stoneplus  student plus u  v into sorry u into  student h j  plus some bias right so you just comfortable with this right i mean that is all i mean  whatever we have seen so first of all remember that the attention is a feed forward  neural network the moment i tell you that you should know that it will have a linear  transformation followed by a nonlinearity right so that should have been very clear  that it would have a linear transformation followed by a nonlinearity ok  and this is the nonlinearity and then you have this other constraint that you want the  output to be a scalar that is why you had this guy which was a vector multiplied by a  vector which gives you a scalar everyone is comfortable with this right so we see at  the attention over images is not very different from attention over sequences it is more  or less the same once you figure out what is the correct representation to you so that you  get the space information after that it is straightforward right ok so that ends the  module   
task2/super_cleaned_audios/lesson102.wav,361.0646,with that we go on to the last module which is fooling deep convolutional neural  networks  so turns out that using this idea of optimization where we are able to actually change  the image to suit our needs right and these needs were one was we wanted to change  the image so that it fires for a particular class the other was deep dream where we  wanted to change the image so that it is starts seeing patterns which were otherwise not  observed in the image and the other was d part where we trained the image or we  optimized over the image so that we could produce some artistic images and these are  the different optimization problems that we have seen  but the same idea can actually also be used to full convolutional neural networks and i  have already hinted at this earlier so let us see how to do that    so now suppose we feed an image to a convnet and i know this is the bus image right  but now what i do is this is a trained convolutional neural network and what i do is  instead of setting the cross entropy loss to maximize bus i will set up the cross entropy  loss to maximize ostrich and then i will back propagate through the network i will not  modify any of these weights or parameters and i am only change the image right  so what i am trying to do is i know that this is the bus image but now i am setting the  objective that it should fire for the ostrich class so now i am going to back propagate  and change this image so that the blog the likelihood of the ostrich class increases you  get this set up its very straight forward ok and turns out that if you do this with very  minimal changes to the image you can actually fool the convolutional neural network  ok    so this is the change right you have the original image the second image is actually the  amount of change you made and the third image is the original image plus this change  now to the human eye there is no distinction here right you would all of first would still  think this is a bus and in fact i do not even see that there is a noise in the third bus that  you see same for some other class they have taken some bird or something like that and  added some noise to it and a temple and in all of these cases the network actually  predicts that the modified image is an ostrich right or some very random class from the  original class so why is this happening and before asking that question let me just finish  and it need not be that you start with an original image and then try to modify it    actually you can start with a blank image and do the same experiment where you  modify the image minimally so that p of robin becomes one or close to one and you will get  some very arbitrary noisy looking images which no in which to at least you and me do  not look like a cheetah or robin or armadillo but the network thinks that these are the  classes that these images belong to  now this is definitely a risky how many of you appreciate that it is bad ok now and  a network is not just predicting it is predicting it with a very high confidence right nine hundred and ninety-six  percent confidence so why is this happening can even think of a reason for that  no but ok in that case i would have been fine if there are one000 classes it should have  given oneone000 probability to all the classes right but this is like worse than random  classifier right it is saying with ninety-nine percent confidence that this is a ostrich or whatever  class that is so why is this happening and the interesting thing is that this in some  sense ties back to the universal approximation theory or at least some ideas with that  can you think of why this is happening ok  so let us try to see a very intuitive explanation for this so on     so this explanation is due to andree karpathi we need to put the acknowledgments this  slide does not have any acknowledgments actually so remember that images are  extremely high dimensional objects right they are two hundred and twenty-seventwo hundred and twenty-seven which is a very high  dimensional object high dimensional space and no matter how much training data you  have you see a only a small sample of this high dimensional space right because its real  numbers two hundred and twenty-seventwo hundred and twenty-seven just imagine the number of possibilities out there no matter you have  one million samples one0 million samples for training this is much smaller than the actual  number of samples which exist in this space of these only a few are images right  so now think of all two hundred and twenty-seven  two hundred and twenty-seven matrices that you can make and how many of them are  actually going to be natural images the probability of natural images is very small most  of these are random things right they are just matrices which do not make any sense  which actually look like these images that you see here right   now using the training images we fit some decision boundaries and this is the decision  boundaries that we fit right that this is class one the rest of the green part is class two and so  on and in fact we are doing these decision boundaries for some one000 classes while  doing so we actually end up taking decisions for a large number of points that we have  not seen we have not seen any points in this space but i have made a decision for them  that all of them belong to the green class i have not seen any point in this space but i  have ended up taking a decision for them that all of them belong to the red class right  so in particular what i have done is i saw a cheetah class image from a cheetah class i  saw a few images from the cheetah class and i drew some boundary around it to say that  this is the cheetah class but my boundary also contains images like this because this is  a very high dimensional space and in that boundary a lot of points actually fall in and  some of these points are these random points which have no relation to cheetah right  but i have been so aggressive in fitting to the training data that i have drawn these  boundaries which also include a lot of these points and now all i need to do starting with  these rend random images is that go somewhere inside this boundary and then i am all  set right it will start detecting it as cheetah because the boundaries have been drawn by  the classifier how many if you get this explanation good so that is the intuitive  explanation for why this happens so this is where we will end the discussion on  convolutional neural networks   
task2/super_cleaned_audios/lesson15.wav,895.5868,in this module we will talk about the proof of convergence for the perceptron or the learning algorithm that we saw in the previous module so we have some faith and intuition that it actually works we just need to formally prove it that it actually converges so that is what we are going to do in this module so before that a very few very simple definitions so if you have two sets of points p and n in an n dimensional space and we call say that these points are absolutely linearly separable if there exists some n plus one real numbers which has wzero to wn such that every point which belongs to p right p is the case where the output is one then these set of weights satisfy this condition and every point which lies in the negative set the set of weights satisfy this condition so nothing very different from what has we have been saying so far it is just formally defining it now our proposition is that if the set p and n are finite and there is a fixed number of points in that which was the case in the toy example that we were doing and which will be the case in most examples that we do and linearly separable the perceptron learning algorithm updates the weight vector ok before i go there ok let me not give you the definition and let me ask you the definition so now i have given this definition the first definition and given this part of the proposition can you tell me what do i need to prove if i need to prove that the algorithm converges that is one way of looking at it but  what was happening in that wrong argument which was i was making that it continuously kept toggling that means i am not making a finite number of updates right i have to keep changing again and again and this process continues in a loop so that is how i am going to define convergence that the perceptron learning algorithm updates a weight vector of finite number of times it only needs to update it finite number of times and it will reach a configuration such that now it is able to separate the p from the n ok that is what the proof of convergence means so in other words if you are going to pick up these vectors randomly from the set p and n cyclically as we were doing in the toy example then a weight vector wt is found after a finite number of steps which will separate these two steps these two sets so that is what we are trying to prove so that is the definition of converge does it make sense  so proof is on the next slide and it is going to take me around five to onezero minutes to prove it so just stay focused all right so here is a few set up right so i am going to before i go to the actual proof i am going to make a set up so that it becomes easier for us to prove it so the first thing that i am going to say is that if there is a point which belongs in negative set then the negative of that point belongs in the positive set and that is very clear because if the point belongs in the negative set then w transpose x is less than zero but then w transpose minus x would be greater than equal to zero right so i  take the negative of the point i can just put it in the positive set so instead of considering these two different things p and n i am just going to consider one p prime which is an union of p and all the n points negative ok will the set up clear if this is a setup then what is the condition that i need to ensure for every point in p dash student refer time zerothreefive7 w transpose p should be greater than equal to zero right so i do not care about the negative case i have just made everything positive now and it is i am not done anything wrong here it is just a simple trick ok and now this is how the algorithm will look in this setup these are the inputs with label one inputs with label zero n minus contains a negation of all the points in n and p prime is a union of these now again i start by initializing w randomly while convergence i will do something i will pick a random p from p prime now what is the if condition less than zero do i need the other if condition no right because everything is now positive ok and the other small thing that i am going to do is i am going to normalize p ok so that again does not mean because we are talking in terms of angles and i am not changing the direction of the vector i am just shrinking it right so i am just or maybe scaling it also i am just making it unit norm so that does not change anything so it is still everything still holds and in particular you can see here so if this condition was true this condition will also be true ok so so far just i am done some simple tricks to make things easier for me later on so now p has been normalized now remember that this data is linearly separable that is what we started the proposition if p and n are linearly separable then the perceptron learning algorithm will converge so now  if p and n are linearly separable irrespective of whether we have the perceptron learning algorithm or not what do we  know there exists a w star which is the solution vector right there exists at least one w star which is the solution vector right such that it will separate the p points from the n points so this vector which we do not  know but we just  know that it exists so you can refer to it so we will call this w star fine now we start the proof   so w star is some optimal solution which we  know exists but we do not  know what it is right now suppose you had a time step t so remember that this algorithm is going on while convergence so you have time step one two three you are picking up points so we are at a time step t at which you pick up a random point pi and you find that the condition is actually violated so this should actually be less than zero if i know the condition is violated so now what will you have to do w is equal to wone so i will just call it the new w wt plus one is equal to the old w plus pi ok now what i am going to do is i am going to consider the angle beta between w star and wt plus one i do not know what w star is but we can still assume it exists and make some calculations based on that so what is the angle between w star and wt plus one its beta and what is the cost of that angle this  and remember that we do not have w star here because we had assumed that it is the normalized vector so we do not need that but this is actually equal to one ok so now if i just take the numerator w star in dot product wt plus one now i am going to expand wt as wt plus pi fair that is exactly what i did on the previous step now now what is pi actually it is so what you had is you had these pone ptwo pthree my hand writing is really horrible and up to pn right so i have just picked one of these pi’s ok now what i am going to define is now suppose this is my these are my pi’s so these are all the vectors that i have now suppose i have this w star suppose this was the w star that i am interested now for each of these i could compute w star pone w star ptwo and so on up to w star pn and i could sort them now what i am doing is that for whichever of these points w star pi is the minimum i  am going to call that value as delta suppose w star p one is the smallest quantity out of w star pone w star ptwo w star pn and i am calling that quantity delta so i have this quantity here and my delta is the minimum of all the possible values that it can take it can make w star pone ptwo up to pn so delta is the minimum quantity so here i have an equality  now are you ok with this this is the minimum quantity right so any pi that i put in here it is always going to be greater than or worst case equal to delta now again this wtwo itself i could write it as wt minus one plus pj because that also would have come up from some update in the previous step ok again this is there which i could call it as delta and still retain the greater than equal to here ok fine so let us see where are we heading with this now notice that we do not make a correction at every time step when i was running that toy algorithm i was not making a correction at every time step we were only making a correction at those time steps for which the condition was violated so now if i am at t’th time step maybe i have made only k which is less than or equal to t corrections at max i would have made t corrections but it could have been less than that also so now every time we make a correction we are adding a value delta to this so at the time step t what would happen i had started off from w naught i have reached time safety and i have made a case that i have not made t updates i have made k less than equal to t updates so how many deltas would get added k delta so i could say that with respect to w naught where i had started from this is what this quantity is ok is that fine anyone has a problem with this so far what are we shown we started with this this condition was true again not less than equal to and hence we made the correction and this was the point that we picked up at the t th step and thence we made that correction and we also showed that the numerator is actually greater than equal to this quantity we showed it by induction fine now let us look at the denominator and particularly let us look at the denominator squared ok is a step right this is actually wt plus one dot product wt plus one but  wt plus one can be written as wt plus p i this bracket needs to disappear right is that ok fine now what is what is this quantity that is less than equal to zero so now can you guess what is the next thing that i am going to write  that is correct yeah it is a negative quantity so that is going to be less than equal to this so that is fine and what about pi square or this term  because this is less than right that is why correct is this fine ok now what is pi square one now can you guess what i am going to do by induction so what is wt square again just this wt plus one square was wt square plus one wt square is going to be wt minus one square plus one right and how many such ones will get added k of those right starting from w naught ok so what have we shown the numerator is greater than equal to this the denominator is less than this ok now if i put them together i actually get that cos beta is going to be greater than equal to the numerator over the denominator ok now what is this quantity proportional to k k square k cube square root of k k by two student square root of k square root of k right you have i mean roughly speaking you have a k here you have a square root of k here so i could roughly speaking say that it is proportional to square root of k so as k grows what will happen to cos beta it will grow and that is fine right it can keep growing student refer time onethreethreeone only until one right so cos beta is going to be proportional to k what is k the number of updates that you make now if i were to take that degenerate case which you guys were hinting at where that it will keep changing again and again what will happen to k it will keep going to infinity can that happen no because cos beta will blow up right and that is not allowed so k has to be finite so that cos beta stays within its limits right hence are we done how many if you think we are done how many if you are satisfy that we are done so yeah so this says that we can only have a finite number of such k updates that we make and after that the algorithm will converge so we have a proof of convergence now coming back to our questions this is where we had started at one point what about non boolean inputs so perceptron allows that we took imdb rating and critics rating as an input do we always need to hand code the threshold no in our perceptron learning algorithm are all inputs equal no we now assign weights to input what about functions which are not linearly separable we still do not  know so that is where we are headed now not possible with a single perceptron but we will see how to handle this  
task2/super_cleaned_audios/lesson29.wav,723.1413,before we move on to the next module a quick summary of what we have done so far  so we introduced feed forward neural networks and we wanted to learn the parameters  right from the last layer to the first layer and we figured out that what we can do is that  we can just use the gradient descent algorithm as it is except that we have this small  problem that we have so many parameters now and located at differ different points in  the network right some at the initial layer some at the final year and you want to  compute the derivatives or the partial derivatives with respect to all of these  if you can do that put them all in this large matrix then we can just use gradient descent  as it is so that is what we figured out and then we wanted to find out the gradients  with respect to or the partial derivatives with respect to all these parameters so then we  realize that this can be done using chain rule because there is a path from your output  which is the loss function to any of these weights so we just need to follow that path and  apply this smart this chain rule smartly and just sum up the derivatives across all the  paths that lead to that weight so in that process we started from the output layer we just  treated it a bit special because the output function is special and this is the last layer  so we just first computed the gradient with respect to the output layers then we figured  out how to compute the gradients with respect to any of the hidden layers and now if  you are at a particular hidden layer now the weights that feed into this layer we could or  we have not reached there  so now the next thing that we need to do is that we have computed the gradients with  respect to any of these hidden layers and now we want to find the gradients with respect  to the parameters which is the weights and the biases so it is the do you all remember  this or it is all long history or the story is back right fine so now we are at the last  point which is computing gradients with respect to parameters      so again this is the overall picture we were in this chain rule and we have come all the  way to the last point where we are ready to now compute these quantities so now start  by recalling that a k is equal to b k plus w k h k minus one right this is our activation  formula pre activation formula right so i am talking about these light blue guys ok  which is clear in image  and now i what have i done so far i have been able to come up with a formula to write  the gradient of the loss function with respect to any of these light green guys right that  is what where we ended last time right where we are able to compute the gradients with  respect to the sorry light blue guys ok and now i want to compute the gradient with  respect to any of these parameters or any of these parameters  so any parameter it does not matter am at some i’th activation layer pre activation  layer and i just want to compute the gradients with respect to the weights which feed  into this layer and that is what we are interested in so we are just taking any layer k  and you want to find the gradient with respect to the weights there now can you tell me  so can you tell me what is what is the thing that am going to do here or what is the  recipe that we have been following  i need to move what is the recipe that we have been following apart from yelling at  people who come late we find the element wise partial derivatives first and then put  them all together to get the gradient ok what is the element here what is what am i  looking for right now i want to compute this fill this blank what goes here  student w  w any of these w is right and in particular say w k that is what i am looking for so  what is the first thing that i am going to attack  student wkij  good w k i j and once i have this for one of these guys i just know a generic formula  with respect to i j and k and i can just put it into a gradient vector ok is that fine ok so  now can you ok now from here to here if i want to reach from here to here so this is  what i am interested in right now how is the chain rule going to look on look like  based on whatever you have already seen till where have you already reached you  already know this quantity right now if i want this how am i going to write it  student refer time four hundred and fifty-nine  i will find up to the light blue guys which is this i already know how to compute it and  then from the light blue guys i will go to the this is fine right so this is the quantity  that i am looking for ok now what is one element of this guy dou a k by is it fine ok  what is the dimension of this actually is it a scalar a vector a matrix matrix or a tensor  what is the tensor what is it is it a matrix what are the dimensions what does this  derivative mean or this gradient mean i change one element of w k how much does  one element of a k change how many elements are there in ak n how many elements  are there in w k n cross n so how many partial derivatives which i have n cross n  cross n what is this  student tensor  a tensor right so this is going to be a tensor ok so when i say one element of this i  mean this ok so this is one element of this gradient ok now can you tell me the  formula for this what is this quantity hk minus  student one refer time six hundred and twenty-seven  hk minus one or hk minus one j or  student refer time 06threeone  everyone gets this hk minus onei how many of you get this     so let us do it right so you have akone aktwo akthree that is your ak vector ok you have  bkone bktwo bkthree plus wkone one yeah i know again this is one of those silly things but if  everyone does not raise their hands and compelled to do this so h k minus one one hk minus  one two hk minus one three ok so let us take one of these guys right so a k one can you tell me the  formula for that  student refer time 07three0  plus first row ok one two this one three now can you tell me this quantity so what is i here one ok  so i want this by w k i j right so i is one so i can take any of the j so let me take j equal  to two so what is it going to be this will go off this is constant this is constant only this  term remains and the derivative is hk minus one two which is j right so that is what the  formula says so i have a formula for one of these guys ok and that is a generic  formula so always remember if you cannot figure out what it is just write it down in  scalar terms just add up all the terms and you will get the formula right so now this is  what the chain rule is going to be     so this is what it is going to be this is one element of that tensor this is how that  entire thing is going to look i have just flattened it out and put it here    now let us take a simple example of wk belonging to r cross three cross three everyone is fine  so far right or anyone who everyone is fine please raise your hands i mean fine i mean  not in life but with the lecture fine so this is what it looks like right for a three cross three  matrix  now let us see we already found out that this guy is equal to hk minus one comma j right  so this is what this matrix looks like nothing rocket science here right so each of these  quantities is actually can be written in this form where i appropriately substitute i k and  j and i know that this quantity can be further written as this quantity right that this is  our clear right so i have written it as this   now can you simplify this i do use a lot of this ok can you simplify it is it looks  similar to something that you did on the assignment does this look like matrix which  has some very regular patterns yeah i can see someone doing this and this everyone  gets it    so let us see so this the first column the second term in the product is all same  throughout all the rows right what i mean is all these guys are similar same thing  happens in the second row the third row right ah that is sorry the second column and  the third column what about the rows these are all equal right so what does this look  like actually the outer product of two vectors everyone gets this raise your hands ok  good  so i do not need to do an example so it is fine right this is an outer product of these two  vectors one happens to the quantity to be the quantity that we already knew right and  the other happens to be a quantity that we can figure out i mean we already know this  what is we know how to compute the hidden representations right the hk’s we can  compute    so fine so finally we come to the biases this is what one entry looks like this is  exactly the sum which i had written out now i take the derivative with respect to b k i of  the loss function so i could write it into as this chain rule where the first quantity is  something i already know i have computed the gradient with respect to the pre activation  layers what about the second quantity anonymous roar is what i was expecting  student one  one ok fine we can now write the gradient with respect to the bias what would it be  what is this what is this it is just the gradient with respect to the pre activation layer  right simple so now we are done with all the gradients that we were interested in  
task2/super_cleaned_audios/lesson8.wav,252.909,so this was all happening where deep learning now started showing a lot of promise in a  lot of fields nlp vision speech and again this deep reinforcement learning and so on  which led to this complete madness starting from two thousand and thirteen well almost for every application the traditional methods were then overwritten or kind of beaten by deep neural network based system so something like language modelling which has been around since probably 1950s or so  now the reining algorithm or the better algorithm for language modelling is now something which is based on deep neural networks then similarly for speech recognition lot of work a lot of probabilistic lot of work based on probabilistic models was done in this or in the speech area or the speech literature for the past thirty forty years and now all of that has been overcome by deep neural network based solutions same for machine translation a lot of interest in this field a lot of companies now have their machine translation systems based on deep neural networks as opposed to the earlier phrase based statistical machine translations or the probabilistic models which were used earlier similarly for conversation modelling dialogue a lot of new work started in dialogue post a deep learning era where people now realize that if you have a lot of sequences of conversations you could actually try to train a deep neural network to learn from this sequence and have conversations with humans of course you are nowhere close to human level conversations we are very very far off from them but in limited domains these bots are showing some success now same for question answering where you are given a question and you want to answer it either from a knowledge graph or from a document or from a image and so on and in the field of computer vision things like object detection most of the reigning systems or the best performing systems nowadays are deep neural network based systems a lot of advances are being made on these systems over in the last few years same for visual tracking where you want to track the same person in a video or image captioning where you want to generate captions for images for example people upload a lot of images on facebook and if you want to automatically caption them or imagine you are on a reselling site right something like olx where you upload your furniture and you do not provide a description from that but can the machine already automatically generate a description for it so it is easier for the human to read what that product is and so on so similarly video captioning i given a video anyone to caption the main activity which is happening in that video all of these problems are being solved using deep learning based solutions using a combination of something known as feed forward neural networks or convolutional neural networks or recurrent neural networks and so on visual question answering you are given an image and a question and you want to answer that question video question answering answering questions from videos video summarizations if you are given a large video and you want to generate a trailer a sort of a trailer for that video contains which kind is the most important frame for that video even these systems are based on deep learning then this was all about classification recognition and so on but  now people started getting more ambitious that can we humans are very good at creativity so can we use machines to be creative right to generate images so now  if i have seen a lot of celebrity faces can i generate new celebrity faces or if i have seen a lot of bedroom images and i am if a fireman architect now can i generate new bedroom images can i can we train a machine to generate new bed bedroom images so a lot of phenomenal progress or work has happened in this field in the last four five years starting with things like generative adversarial networks variational autoencoders and so on and people  are now  starting  to  seriously invest  into  creativity  that  how  to  make machines creative again we are far off from where the desired output but there is still significant progress happening in this field generating audio so that was about generating images you can generate music also and this is again about generating images and so on 
task2/super_cleaned_audios/lesson24.wav,375.2253,now we will move on to the next module where we want to learn the parameters of  feed forward neural networks and we first start with some intuition and then  mathematical details  so we have introduced feed forward neural networks and we are now interested in  finding an algorithm which can allow us to learn the weights of this network    so recall our gradient descent algorithm this is how it looked ok i had initialized those  two parameters w naught b naught and then i was iteratively doing this in a loop at every  step i was moving in a direction opposite to the gradient at that step  now can i write this a bit more compactly we can write using vectors     so are you ok if i write it this way so these two was actually nothing but vector at every  point so i can just write it this way so theta is the vector containing w and b ok or  theta is the vector of all the parameters my network had it just so happened that  network had only two parameters so see where am going with this how many of you see  where am going with this  good so where delta theta t right just to remind you it was this the partial collection of  all the partial derivatives with respect to all the parameters in this toy example all was  equal to two right we just had two parameters now you see where am going with this ok so  now in this feed forward neural network instead of theta equal to w comma b what do  we have theta is equal to so many parameters ok so what would grad of theta t now  be partial derivatives with respect to  student refer time one hundred and fifty-eight  all the weights but there is a problem here right this is the matrix how do you take the  partial derivative with respect to the matrix who asked you to use the matrix how you  take the partial derivatives with respect to matrix so what i am interested in this right  the question i know there is some loss function which is a function of theta one of the  elements of theta has this matrix w one which belongs to r n cross n right and now i want  the derivative with respect to w so see what i am trying to do this is scalar and we take  the derivative of that with respect to a matrix what is all that the derivative with respect  to  student refer time 0two37  every element of the matrix ok    so we can still use the same algorithm except that del this grad of hat of so now i could  just say that theta two hat i mean initialized all parameters and theta naught right  compute the gradient with respect to all of them and then do this update right i could  just instead of putting them in matrices i could just think of them as a large vector just  had initially i had just had w comma b now this vector is even more large in fact i will  show you actually how it is    so this is the grad with respect to theta looks very nasty now this is how nasty looks  right so you have this weight matrix w one you have the derivatives with respect to first  element of w one all the way up to the last element last element so with respect to all the  n cross n elements of w one what is the next entry going to be w twooneone to  student refer time 033two  wtwonn next after wloneone ok and then after this ok  student refer time 034one  what is remaining biases right so you have boneone to bonen this slight error here but  intentionally this actual is k because k is not equal to n right the last layer has only k  parameters whereas so that it looks ok is this clear so is this are all the partial  derivative that we need right you do not need to worry about taking a partial derivative  with respect to our matrix it just boils down to taking the partial derivative with respect  to all elements of the matrix  so earlier you just had two parameters now you have these n cross n plus n cross n upto l  right so l into n cross n plus l into n that many number of parameters is what you  have you get the calculation right or rather you have l minus one layers each of which  has n cross n parameters right and l minus one layers which also have the biases so these  are the w’s these are the b’s then the output layer one layer which has n cross k  parameters and k cross one bias so these are all the number of parameters that you  have and this is exactly what this size of this matrix is right it has all these parameters  and you need to compute the partial derivative with respect to each of these parameters     so this is what grad theta is composed of it is composed of the partial derivatives with  respect to all the parameters of your network ok so now if someone gives you each of  these quantities same oracle give you each of these quantities then can you apply  gradient descent right you can use the exactly the same algorithm that you are using  earlier just the sizes of earlier vectors changes  how many of you are convinced that now you can use that gradient descent there is not  a trick question how many of you convinced how many of you not convinced  assuming that someone has given you these quantities right i know that it is hard to  compute we will see how to compute that but let us assume someone has given you this  then you can use gradient descent that is what the case i made in the previous slide right  that you could initialize with all the parameters compute the gradients with respect to all  the parameters and just do this update fine so now we need to answer two questions first  is this is the key question    because we are taking derivative of what loss functions so we need to know what the  loss functions that is the crucial question right and then we are taking derivatives with  respect to all these elements so whatever i was told you that assume that oracle gives  you now you have to do the hard work and actually find it out right so if you can  answer these two questions then we are done we have an algorithm for learning the  parameters of feed forward neural networks we all agree that if you have these two  elements then we have done  so here i will end this module  
task2/super_cleaned_audios/lesson30.wav,317.2556,so we move on to the next module and now we will write pseudo code to for back  propagation   so we have all the pieces of the puzzle we have the gradients with respect to the output  layer that was the special layer because the output activation function is different they  are the gradients with respect to all the hidden layers that means i have the gradients  with respect to the activations as well as the pre activations  so in the h’s as well as the a’s and i also have the gradients with respect to the weights  and the biases and this is all index agnostic right that means i am just using k as the  index everywhere i have a generic formula which applies at any layer for the weights as  well as the activations and the pre activations right ok now we can put all this together  into a full learning algorithm so let us see what the pseudo code looks like    so we have this t equal to zero well run this for some max iterations we initialize all the  parameters to some quantity will randomly initialize them ok now for these max  iterations can you tell me what is the first thing that i will do so there will be two  functions here ok tell me what those two functions would be  student forward  forward propagation and then backward propagation right so you do a forward  propagation and you compute all these activations pre activations output layer loss  everything and then you do this backward propagation where you feed all these things  which you have computed these are the quantities which you have computed you will  pass this to your backward propagation algorithm it would not look so nasty as this it  will not take so many parameters you could write it smartly and then you will just do  the parameter update  so what will the back propagation give you actually all the gradients all the partial  derivatives right and then once you have the partial derivatives you know how to  compute the update law so now let us look at these two functions more carefully the  forward propagation and the backward propagation    so forward propagation is simple for all the hidden layers that means from layer one to  layer l minus one what will i do give me the code a k is equal to good then ok and what it  what is h of zero you are starting the loop from one right so you will need h of zero that is x  and then you will have a special treatment for the output layer and your final output will  be whatever output function you use ok this makes sense you can write this in python  you will have to write this in python      now we have computed all the h’s and the a’s what have we computed all the a’s all the  h’s and all and the y right now you want to do back propagation so back propagation  the loop will be from i equal to one to n minus one good so the first thing i will compute is  the gradient with respect to the output layer see even here the output layer was outside  the loop the same thing would happen here also in the back propagation also first you  will compute the gradient with respect to the output layer and this is the formula  if you remember from last class right that is the formula which i have substitute here  and note that f of x is known to you because you computed that in the forward pass and e  of y one hot vector which with a correct label said to one and you know what the correct label  is because we have given you the refer time zerothreeone4 data right ok then what would the  loop be l to one or l minus one let us see first you compute the gradients with respect to  parameters it is l  so because we are using k minus one then you compute the gradients with respect to the  layer below computes gradients with respect to the pre activation right this is exactly  how you will proceed this is clear to everyone the same three components that we have  used you might be a bit confused about the ordering in which we have put them because  we computed the gradients with respect to pre activation first and then the weights but  once you go back you will realize because it is the way we have indexed it because this  is already outside  so this has already been computed so you can already compute the gradients with  respect to the weights of the outermost layer is that fine so this is straightforward you  can go back and check this ok now anything remaining or you have everything can you  just take a minute and see if you can visualize the python code and we will just assume  that you are done the assignment you can read you will have multiple these vectors and  matrices and so on and you are just doing a lot of matrix operations using refer time  zero4zero6 or refer time zero4zero8 or whatever you prefer right  now what is missing here input is missing ok input we have given right the ominous  data set has been given is there something that yours i have still not shown you how to  compute oh i did not update the parameters here is it no the parameter update will  happen in the outer loop right so those forward prop back prop and then update the  parameters right so the main algorithm was forward prop back prop update the  parameters when we saw forward prob an obvious seeing backward prop so what is  missing onezerozerozero iterations something in the last line before end of course do you know  how to compute this  
task2/super_cleaned_audios/lesson5.wav,125.9927,"so this is what the progression was right that in two thousand and six people started or the study by hinton and others led to the survival and then people started realizing the deep neural networks and actually we use for lot of practical applications and actually beat a lot of existing systems but there are still some problems and we still need to make the system more robust faster and even scale higher accuracies and so on so in parallelly while there was lot of success happening from two thousand and twelve to two thousand and sixteen or even two thousand and ten to two thousand and sixteen in parallel there will also a lot of research to find better optimization algorithms which could lead to better convergence better accuracies and again some of the older ideas which were proposed way back in one thousand, nine hundred and eighty-three now this is again something that we will do in the course so most of the things that i am talking about we are going to cover in the course so we are going to talk about the imagenet challenge we are going to talk about all those networks the winning networks that i had listed there alex net zf net google net and so on we are going to talk about nesterov gradient descent which is listed on the slide and many other better optimization methods which were proposed starting from two thousand and eleven so there was this parallel resource happening while people were getting a lot of success using traditional neural networks they are also interested in making them better and robust and lead for lead to faster convergence and better accuracies and so on so this led to a lot of interest in coming up with better optimization algorithms and there was a series of these proposed starting from two thousand and eleven so adagrad is again something that we will do in the course rms prop adam eve and many more so many new algorithms i have been proposed and in parallel a lot of other regularization techniques or  weight  initialization  strategies  have  also  been  proposed  for  example  batch normalization or xavier initialization and so on so  these are all things which were aimed at making neural networks perform even better or faster and even reach better solutions or better accuracies and so on this all that we are going to see in the course at some point or the other "
task2/super_cleaned_audios/lesson18.wav,707.4677,representation power of feedforward neural networks we are in lecture three of cs7zeroone5 and today we are going to cover the following modules we are going to talk about sigmoid neurons gradient descent feedforward neural networks representation power of feedforward neural networks so let us start so here are some acknowledgments so for one of the modules i have borrowed ideas from the videos of ryan harris on “visualize back propagation“ they are available on youtube you can have a look if you want for module three5 i have borrowed ideas from this excellent book which is available online it is the url as mentioned in the footnote  and i am sure i would have been influenced in borrowed ideas from other places and i apologize if i am not acknowledge them probably properly if you think there are some other sources from which i have taken ideas and let me know i will put them in the acknowledgments so with that we will start with module threeone which is on sigmoid neurons so the story i had is that it is enough about boolean functions now we have done a lot of boolean functions but now we want to move on to arbitrary functions of the form y is equal to f of x where x could belong to rn and y could belong to r so what do i mean by this so let me just explain this with the help of an example so i will again go back to our oil mining example oil drilling example where we are given a particular location say in the ocean and we are interested in finding how much oil could i drill from this place and that is what i would base my decision alright whether i want to actually invest in this location or not and then what we are saying is that this could depend on several factors so we could have xone xtwo xthree up to xn right where this could be the salinity of the water at that location so this could be a real number this could be the density of the water it is average density this could be the pressure on the surface of the ocean bed and so on and so forth so each of these values independently belongs to the set of real numbers so each of this is a real number and we have n of these so together they belong to rn so i can read that i have n such real numbers and i could just put them in a vector and say that i have a input x which belongs to r raised to n so we have this x which we can say belongs to rn and in this particular case we want to predict y we want to take this as an input and predictor y and what is y in this case you want to predict the quantity of oil that we could mine so what does ry belong to again a set of real numbers and it could be some gallons or litres or kill of water so this again belongs to r so these are the kind of functions that we are interested in now we want a function which takes us from i am having this x which belongs to rn right it is a vector of dimension n and takes us to a value belonging to r so you clearly see that this is different from the case when we had n variables each of this was just boolean so these were only zero one inputs now we have real inputs and these are the kind of functions that we are interested in now can we have a network which can represent such functions now what do i mean by represent such functions we already spoke about this when we were doing boolean functions so what do we mean by representing the function we mean that if i am given a lot of training data right so i am given these xone xtwo each of these belongs to rn and i am also given the corresponding labels now i want a network which should be able to give me the same predictions as is are there in my training data so it should be able to take any of these x’s as input and it should give me the same y i corresponding to it and i am saying approximately which means i am with some error rate whether if it is within some to with as long as it is close to the actual value i am fine with it so that is what i mean by a network which can represent such functions is that working definition of representing clear right so that is a very similar to the definition that we were used for boolean functions we had said that we should be exactly be able to get the truth table the network should be able to represent the truth table exactly so that is very similar to the definition that i am using here and then before we do this right before we come up with a network which can do this for arbitrary functions we have to graduate from perceptron’s to something known as sigmoid neurons so please remember this overall context that we dealt with a lot of boolean functions we analyze them carefully and we saw that we could come up with these networks which could represent arbitrary boolean functions and they could represent them exactly as long as we have one hidden layer of course the catch was that that hidden layer could grow exponentially now we want to graduate from boolean to real functions that means you have a real input of n variables and one or more outputs and you should be able to represent this exactly so that is where the transition is where so that is the story that we are looking for so let us start so recall that a perceptron will fire if the weighted sum of it is inputs is greater than the threshold just recall that fine so now i claim that the thresholding logic which is used by a perceptron is actually very harsh now what do i mean by that let us see so let us return to a problem of deciding whether we like or dislike a movie that is the same problem that we have been dealing with and now consider that we base our decisions only on one input which is the critics rating which lies between zero to one and this is what my model looks like it takes the input as the critics rating i have learned some weight for it and my threshold is zero5 what does this mean it means that if for a given movie the rating is zero5one will it predict like or dislike like so then i should go and watch the movie what about a movie for which the critics rating is zeroforty-nine dislike so now you see what i mean by harsh so both these values are very close to each other but for one i say i like it for the other i say that i would not like it so it is not how we make decisions right you would have probably said something equal for both the movies you would have not given such a drastic decision so why is this happening so you might say oh this is a characteristic of a problem that you have picked up maybe that is the critics  rating which is between zero to one or something but i want to convince you that this is not a characteristic of the problem that i have picked up but this is something to do with the perceptron function itself so this is what the perceptron function looks like so this sum of all the inputs the weighted sum of all the inputs i am calling it by a quantity z and this is what i am going to plot on the this axis so this is my z axis now what does the perceptron say that when this value of z becomes greater than w naught or minus of w naught it will fire and when it is less than minus of w naught it will not fire that is what it says so this is a characteristic of the perceptron function itself it is going to have this sharp decision boundary that whenever your sum crosses this threshold you will say one and whenever your sum does not cross this threshold you will say zero so in this toy example over the movie critics it just happened that this was zero5 and so it was saying yes for zero5one and it was saying no for zeroforty-nine so this will happen for any problem that you pick up so to counter this we introduce something known as sigmoid neurons and this is just a smoother function or a smoother version of the step function you see that how many if you know what a sigmoid function what is the formula for a sigmoid function quite a few good and here is one such sigmoid function which is called the logistic function so remember that sigmoid is a family of functions these are functions which have this s shaped logistic function which i have shown here is one such function and the other function that we will see in this course is something known as the tanh function so let me just get into a bit more detail with this logistic function i just want you to understand it properly so this quantity here remember we were writing it as w transpose x which was summation i equal to zero to n wi xi remember this so now i am just going to consider this to be one over one plus e raise to minus w transpose x now i am going to ask you some questions and try answering those what happens when w transpose x tends to infinity what happens to the sigmoid function one and that is exactly what is happening here as this tends to infinity as this keeps growing so remember this axis is z which is the same as w transpose x right this is w transpose x so as it tends to infinity your sigmoid goes to one what happens if w transpose x is minus infinity zero and that is exactly what is happening here and what happens when w transpose x is equal to zero half so this is that value corresponding to half is that clear so that is how a sigmoid function behaves fine now we no longer see a sharp transition it is a very smooth function and the sigmoid function lies between the values produced by the sigmoid function rate what is the range that they lie between zero to one what is another quantity of interest that you know which lies between zero to one probability so that is one advantage of sigmoid functions so now you can interpret the value given by a sigmoid function as a probability so what does it mean in our movie example again so it just tells me in those two cases that with 5zero one percent probability i like the movie or with forty-nine percent probability i like the movie so now this is not very drastic or very harsh right i am not saying yes or no i am not committing myself i am just giving you a number which is proportional to how much i like the movie so it can be interpreted as a probability now here’s the overall picture it so this is the difference between the perceptron function and the sigmoid function so notice that here we had this if else condition right which was leading to that sharp boundary now here we do not have that defence condition we just have a function which is a smooth function and here is another picture so this is not smooth not continuous and not differentiable everyone agrees with that it is not smooth here right it is not differentiable here whereas this is smooth continuous and differentiable and the contents that we covered today it will be very important to deal with functions which are smooth continuous and differentiable so for lot of this course calculus is going to be the hero of the course lot of the things that we do will be based on calculus and in calculus always if you have smooth and continuous and differentiable functions they are always good so that is why we want to deal with such functions  
task2/super_cleaned_audios/lesson19.wav,919.9421,we will start module two which brings us to a typical supervised machine learning setup this is a very important module please pay attention so now we have a sigmoid neuron we have taken care of the fact that the perceptron was a very harsh function so we have a smooth function so things are fine now what next where do we go from here what is my next topic going to be yes a lot of you are giving the right answers we need to learn these weights it does not help just to define the function this function depends on certain weights and now i need to give you an algorithm which will help you to learn these weights now remember when i talked about perceptrons before giving you an algorithm what did i revisit what did i talk about the error surfaces and then i had motivated from there that our goal is to find a set of weights which give us close to give us zero error in that case or in general’s speaking generally they should give us a minimum error they should help us to minimize the error rate so i need to set up that similar story here so we will again revisit the concept of error   so now in the case of perceptron i had shown you this figure which they were this data is not linearly separable which is obvious and i told you that perceptron cannot handle this data but what do i mean by it cannot handle this data it cannot give zero error but what would happen if i run the perceptron algorithm on this take a guess what does the perceptron algorithm do  fine and i could convergences my condition i could make that condition a bit loose what is a valid convergence condition that you would lose here use here till almost all my points are separated properly so instead of aiming for onezerozero percent separation i could have a threshold which says as long as 9zero percent of the points are separated i am fine with it that looks like a reasonable thing to do so now if i run the perceptron algorithm with that condition what do you think will i get as a decision boundary everyone has a picture in mind ok let us see does this match what you had in mind roughly of course there many things possible but it will basically pass through this now what is happening here what is the problem there are three blue points which are wrongly classified and three red points which are wrongly classified but in most real world applications we will find that this line is not too bad you could live with this error this is probably three out of threezero on both sides which is roughly onezero percent error unless you are using it when some mission critical applications  or in health care where it is a life and death situation or something in most cases you could live with this right so if you are trying to predict whether people will vote for a particular party if you make this kind of error it would be largely ok unless it is a very close election but largely it would be ok so we could live with this kind of errors in most cases so from now on we are not going to be too optimistic and if you are going to say that there would be some error but my job is to find the weights such that my error is minimizedi want the minimum possible error that i could get is that fine so again whatever weights we want to learn we are going to be driven by some error function and we would want to minimize that error function so  this  brings  us  to  a  typical  machine  learning  setup  which  has  the  following components so this perhaps is the most important slide in the course and i will say this at least for onezerozero other slides in the course but at least for now this is the most important so you are given some data xi yi and you are given n such elements right so let me just elaborate on this and give me i will give you some instances of this let me give you some instances of this right so one thing we say i already told you was this so this is my x and this is my y so one example which i gave was about movies so this was genre actor and critics rating and so on this is one instantiation of this problem i could also give you another instantiation which was i just told you oil right so this is how much oil can i get and here my factors were salinity density and so on there were many other factors so this was my x again x belongs to rn where n is some number integer and another example could be say fraud detection so i have a customer i am a bank i have a customer who has bought some credit card and i want to predict whether he or she would commit a fraud and i would look at factors like what is his occupation maybe salary maybe family size and so on there could be various factors which i could look at so here again this becomes an x ok and you could think of various such examples right where you are given an x and you are given a y ok so this is the data that you have now what is machine learning where does machine learning fit into this so we know that there is some relation which exists between y and x in each of these cases all of us are convinced that there is some relation so whether a person would commit a fraud would depend on these factors it is reasonable to assume that it is not a very wild assumption whether you would find oil at a location would depend on some of these factors and it is related and similarly for the movie case so there exists some true relation between x and y such that if i plug this value of x into the relation it would give me the value of y there exist a true relation this true relation could be governed by various things right it could be governed by physical laws example in the oil mining case it could be even governed by biological laws again the marine life in that location and so on it could be governed by economic law’s it could be covered by psychology right we do not know why a person cheats what is his function that he is using when he cheats and so on right so these could depend on various factors but we all agree that some function exists hence we get these values for this particular input for every input we get a certain value so there is some function which takes us from the input to the output we do not know what this function is we never know in practice it is a very very complex function is all that we know we do not know this exact function if you knew this exact function then there is no problem to solve we just use that function and you can predict how much oil and all of us can become billionaires so that is not the case we do not know what this function is so then what do we do in machine learn we make an assumption ok we make an assumption that there is some function which takes x to y and this function is governed by some parameters and this is our approximation of how the real world works and now under this assumption we want to predict the parameters of this model given the data now let us take a very simple case where we could assume that y is equal to wx plus b i am taking this in the scalar single dimensional case now how would you estimate the values of w and b oh come on if i give you two data points you can estimate the value or should i write it that would jog your memory right this is how we all learn right so m and c you can estimate if i give you two data points so that is the simplest case now we will make similar  assumptions but more complex  functions  and just as  we could estimate m and c from the data we would expect to estimate ws also from the data so that is what the machine learning setup is so let us see so the model when we talk about a machine learning model it is our approximation of the relation between x and y and we are free to make any such approximation so i could say that this is what i think is the relation between y and x and which is governed by some parameters w do you know what is this function have you seen this before no not sigmoid which model is this logistic regression ok but i could also have made a different assumption i could have made this assumption what do i get linear regression ok please note that this error on the slide ok and i could make some other assumption i could assume that y is actually a quadratic function of x i am free to make any assumptions the only thing i need to ensure is there is some parameter involved what is wrong with making this assumption if this is valid is this also valid if not why there are no parameters so no not for any x we will get the we will it will depend still depend on the value of x if i plug in different values of x i will still get a different output there is nothing to learn what do i do with all the data that i have there is absolutely nothing i can use it to learn i have just said that y is equal to one over one plus e raised to minus x i can ignore all the data that you had given me whenever you give me a new x i will just plug it into this formula and tell you the answer and that is bound to be wrong because i have not adjusted this formula now once i put in the ws it gives me this degree of freedom where i can now adjust the formula i can learn the ws in such a way that my predicted y’s are very close to the actual y’s so that is why we need always need a parametric form of course there is nonparametric learning also but i am just saying in this supervised setup we are thinking of models whether you have parameters so you have the data you have the model the model always has some parameters in all of the above cases w is a parameter right either the small w which is a vector or the capital w which is a matrix right so notice that this is a matrix this is one cross n n cross n and n cross one now how do we learn these parameters that is the question that we need to answer how do we learn these parameters we are convinced about two things that we never know the true function so we come up with an approximate function and we have to insert some parameters in that function so far good now i have to be able to learn these parameters  now for learning these parameters we have something known as an learning algorithm so did you see any learning algorithm so far perceptron learning algorithm right so you already saw the perceptron learning algorithm and it was able to learn the weights for a perceptron there are various such algorithms today we are going to learn one such algorithm which is gradient descent now any kind of learning what is it driven by learning is driven by errors objective function and so the analogy which i like to give is suppose you are trying to learn trigonometry you have a chapter that is your training data the chapter has a lot of formulae that is your training data now what is your objective there are two objectives actually i will tell you the easy objective the training time objective is that once you read to the chapter a few times at least whatever formulae are given in the chapter you should be able to produce the correct output for that so if i ask you what is sine square theta plus cos square theta you should be able to answers them and you should be able to give me the correct answer so in other words you are trying to minimize the error on the training data whatever training data you have which is the chapter content you want to make zero errors in anything which is given in the chapter that is your training error of course there is also something as known as a test error because after you have learned the chapter i will give you an independent set of exercises which might contain questions which are not seen in the textbook earlier so you would have seen sin square theta plus cos square theta but now i could ask you some other formula which you should be able to give me answers if you have learned properly right so now right now we are just talking about the training error that means getting all the formula in the chapter correctly and our chapter is actually the training data which is given to you this is what we are reading so this always going to be driven by an objective function and our goal is just as we wanted to minimize the errors that we make on the formula given in the chapter we want to minimize the errors on the training data is this set up absolutely clear to everyone anyone who does not understand this has any doubts so this is something this is the same framework that we will use again in lecture one8 one9 twozero and so on to explain some more complex models so you have to absolutely make sure that you understand this it is not very difficult but just make sure you understand this fine so let us concrete at this a bit more and we will consider our movie example and try to fit that into this framework so what is our training data there they are given movie comma like dislike and when i say movie i am just using a shortcut it is actually all the details of the movie genre actor director critics rating and so on that is our input and our output is the like dislike value what is a model that i chose what is a model that i chose i do not know what is my true relation between when i like a movie or not but i made this approximation that this is how y depends on x and i made sure i introduced some parameters there i could have chosen some other functions also but i chose this now the parameter is w this should be bold w the learning algorithm that we are going to use is gradient descent which we will be seeing soon and what is an objective function here can you tell me a formula so we have been talking about it in terms of english that i should be able to get predictions which are as close to the true prediction can you put it into a formula y i minus  y i hat where hat is the prediction this is the prediction so that is y i minus y i hat that should be minimized is that fine whole square of that so why do you square it so that is correct so for all the training points n training points i want to minimize the square difference between y i the true prediction and the prediction sorry the true value and the predicted value is that fine and why do i use squares differentiable is one the other thing the positive errors and the negative errors should not cancel so it would be happen that on some movies i make a positive error of zero5 right that means the actual label should have been zero5 and i gave one on some movies i make a negative error of zero5 and these would cancel each other and i will get the false impression that i am making zero errors but once i square the values the negative values also become positive  so this cannot happen right so that is why we always use the squared error function and also this is differentiable which is more important now the learning algorithm should try to minimize this particular quantity ok so this is a typical machine learning setup almost any supervised learning problem that you see you could cast it in this framework change the y hat function appropriately change the parameters appropriately maybe use a different learning algorithm depending on the problem that you are trying to tackle and you should be able to fit it into the same thing is that fine ok at least for this course everything that we do we will largely be able to fit it into this framework 
task2/super_cleaned_audios/lesson4.wav,170.2721,"i will talk about the history of convolutional neural networks and i call this part of history as cats and it will become obvious why i call it so so around one thousand, nine hundred and fifty-nine hubel and wiesel did this famous experiment they are still i think you could see some videos of it on youtube where there is this cat and there was a screen in front of it and on the screen there were these lines being displayed at different locations and in different orientations so  slanted horizontal vertical and so on and there are some electrodes fitted to the cat and they were measuring trying to measure that which parts of brain actually respond to different visual stimuli let us say if you show it stimulus at a certain location does the different part of the brain fire and so on so and one of the things of outcomes of the study was that that different neurons in brain fire to only different types of stimuli it is not that all neurons in brain always fire to any kind of visual stimuli that you give to them  so this is essentially roughly the idea behind convolutional neural networks starting from something known as neocognitron which was proposed way back in one thousand, nine hundred and eighty you could think of it as a very primitive convolutional neural network i am sure that most of you have now read about or heard about convolutional neural networks but something very similar to it was proposed way back in one thousand, nine hundred and eighty and what we know as the modern convolutional neural networks maybe i think yan li kun is someone who proposed them way back in one thousand, nine hundred and eighty-nine and he was interested in using them for the task of handwritten digit recognition and this was again in the context of postal delivery services so lot of pin codes get written or phone numbers get written on the postcards and there was a requirement to read them automatically so that they can be the letters or postcards can be separated into different categories according to the postcard according to the postal code and so on right so or the pin code so that is where this interest was there and one thousand, nine hundred and eighty-nine was when this convolutional neural networks were first proposed or used for this task and then over the years several improvements were done to that and in one thousand, nine hundred and ninety-eight this now how famous data set the mnist data set which is used for teaching deep neural networks courses or even for initial experiments with various neural network based networks this is one of the popular data sets which is used in this field and this was again released way back in one thousand, nine hundred and ninety-eight and even today even for my course i use it for various assignments and so on so it is interesting that an algorithm which was inspired by an experiment on cats is today used to detect cats in videos of course among other various other things is just i am just jokingly saying this "
task2/super_cleaned_audios/lesson31.wav,71.3432,we have that activation function and we were taking the derivative of the activation with  respect to pre activation and i just pushed it under the rug by saying we will write it as  g dash so i need to show you what g dash is  what how to compute g dash so this is suppose g is the logistic function ok so that  means what is z actually it is one of those a’s right so this is the activation that you  are going to feed it right and then you are taking the element wise sorry z is actually the  pre activation that you feed it and then g is the activation function so i will do element  wise activation function now what is the derivative of this so i will just i will not do  this derivation  it is there and you end up with a very neat formula which is g of z into one minus g of z  so now that bit is also taken care of is there any more spoon feeding that i can do you  are ready for the assignment now i will do one more bit you will also have used a tanh  function so this is the derivative of the tanh function it again boils down to a very neat  formula which is one minus g of zd whole square so we will end this lecture  
task2/super_cleaned_audios/lesson25.wav,1568.4211,we go on to the next module where we will be talking about output functions and loss  functions   the question that we are going to focus on is how to choose the loss function but i will  show you that it is tightly coupled with the choice of the output function also remember  that we had said that we have a special o function as the output function i have not told  you what that o is and now that is what we are going to define    now the choice will be loss function actually depends on the problem at hand and that  is exactly the question which had come up right that in some cases it is to have sigmoid  as the output function because your values are between zero to one but whatever there are  cases where your output is not between zero to one right so it definitely depends on the  choice of the on the problem that you are trying to solve so we will illustrate this with  the help of two examples and these two examples will cover a broad range of problems that  you will encounter or if you are working in machine learning right  so the first problem is again you are given the input as movie you are using a neural  network with l minus one hidden layers and an output layer y hat right so this is sorry  this is a true one so you have an output layer and the output layer is going to predict the  imdb rating the critics rating and the rotten tomatoes rating  is that fine ok so what kind of problem is this people have done machine learning  this is a regression problem and notice that the output values that you want to predict  are not bounded it by zero and one they are still bounded by one to onezero but in general you could  imagine that there could be problems so there are no bounds at all right it could be a  very large number is that clear now here yi belongs to r three  so remember in all these cases we were assuming that we just want to predict one value  but nothing stops you from predicting multiple values at the same time so your output  is now three dimensional you are taking an n dimensional input and trying to predict three  values from it ok  fine the loss function should capture how much yi had deviates from  yi ok so this is a valid or maybe we corrected on this way ok so this is the formula  which was supposed to be in there right so you take you have predicted three values and  you know the true three values you just take the difference between these right is that  clear the first element of the predicted value minus first value of the actual value and  so on for all the three values that you want to predict    now you have a loss function but what should be the output function in this case can  it be the logistic function yes no it will be bounded between zero to one and you know that  your output cannot be bounded between zero to one ok so in such cases then what is a good  output function to use one option is to scale it so i will keep that aside why do that  it is unnatural and you are actually clamping it and then trying to scale it right so can  you do something more natural in that just use a sum which is linear function right so  what we could do is you could have o as a linear function  so what that means is again remember that this is a of l ok and i know all the  computations that have happened so far a linear transformation nonlinear linear non linear and then again linear so i have computed a of l from that i want to compute the  final output right so i could just have it as a linear function of the input which is a of l  in this case  does it make sense how many of you feel it makes sense ok why because now it is  no longer bounded right you could this linear transformation your weights could be  adjusted in any way to get a value whatever you wanted whether you wanted between one  to onezero or one to onezerozero or one to onezerozerozero these weights could be adjusted to do that right so at  least you are not bounding it and it is free to learn what is the range from the data it  should be able to run but how should you adjust these ws  so that you get the desired range now tell me why would it not happen that you learn  ws you start predicting values like onezerozerozero onezerozerozerozero and so on in this particular case where  your input is bounded by one to onezero sorry your output is bounded between one to onezero why  would it happen i this is my argument and you prove me wrong right i would say that  if you have chosen a linear transformation which is not bounded i then network could  learn weights which start producing a rating of onezerozerozerozero twozerozerozerozero and so on because it is not  bounded  but you know that that is wrong because the ratings can only be between one to onezero so why  would that not happen because you are minimizing this loss function right so if you  start predicting values like onezerozerozerozero when your actual rating was nine then you have a onezerozerozerozero  minus ninezero whole squared loss that is a very high loss so it will start moving you away  from that configuration right so the training is always guided by the objective function  so if your training happens well it will try to prevent this  now suppose let us take a simple thing rate that you are given a our same ball example  for probability so you are given an urn which has balls of three colors say black white and  yellow    and you have to put the balls in that so you know that the true probability distribution  is actually zerothree5 zerotwo5 and zerofour for red black and white ok this is the true probability  distribution you have put say thousands of balls in urn now what you do is you just  allow me to peep into the urn or you allow me to take some samples from there you tell  me take these onezerozero samples and you ask me tell me what this probability is right so  this is the true probability that you know is true right because you know it because you  have estimated  now you just give me a small sample from there and ask me to estimate it and based on  that i actually estimate this ok so there was a true probably distribution and an  estimated probably distribution now i want to find out how wrong i went right  afterwards you tell me the answer you tell me that this is what the true was and this is  what you predicted  now i want a way of computing how wrong i was right so how do i do that you  already know this and these are two vectors what can i do you could just do the this is  valid anything wrong with this in principle no you could just treat these as any two  vectors you have a true value you have a predicted value you just take the squared error  difference between them right but you know this is a probability distribution right  you should be able to do something better than this you know this is a special quantity  this is not just any number that you are predicting you are trying to predict a  distribution so you should be able to do something better than that right so that is  what we want to see how to do something better than this that is what our quest is   now again why we are at this right i also want to make a because this is something  people do not immediately understand so i just want to make a case for something else  so i will just do that ok now suppose there is this ipl ok and there are four teams in the  semifinal let us call them a b c and d ok now i was not in town after the semifinal  so i just know the results up to semifinal and then the finals also happen and one of  these teams wins let us call it the b team right the b team wins can you express this in  terms of probability can you express this in terms of distribution what do you mean  my zero and one b has won  so it is a certain event because it has one now so what is going to be the distribution zero one  zero zero right so this event happens with onezerozero percent probability ok now the same case  can you ok so now let us do the same thing that is as i said i was not in town right and  you asked me tell me which team would win that is i know these four teams have qualified  in the semifinals and i know who the players are and so on  and with my limited knowledge of cricket i will predict something right so say i predict  this ok can you again tell me how wrong i was you know what the true label is and  you know what i predicted you can tell me how wrong i was ok so the case which i  am trying to make is that even if the event is certain you can still write it as a probability  distribution where all the mass is allocated to the correct output can you relate this to a  classification problem when you see training data you have already observed it  suppose there were four classes possible  apple orange mango and banana if you have seen it is apple and if you ask you what  is the distribution what will you tell me zero one zero zero you will express it as this one hot  vector where all the probability mass is concentrated on the guy which is correct right  so even certain events which happen with certainty you can write them as a distribution  rate where all the masses are located on the true label so that is how all classification  problems when you are dealing with multiple class classification problems it is often the  case that you will write it as this  that your true label is given to you in this format there were four possible events four  possible classes or k cost possible classes out of which only one is correct and then you  make a prediction and you want to now find out how different was your prediction from  the true label you are trying to get the set of how this relates to a classification problem  and this is that is why this is of interest to us ok  so this so we will see this soon now the next thing that we need is how many of you  know what is entropy forget about cross just entropy ok that is why i have left two slides  intentionally blank ok so so now let us see where i go with entropy ok how many of  you know what is expectation please fine so again the same thing now i knew that  this was the distribution which i think i am into gambling am not i am into gambling  and i try to bet on these teams  and i bet some amount on each of these can you tell me what is the expected reward  that i will get so what am i saying wait suppose this is the case that if team a wins i  get onezerok rupees or my net profit is onezerok rupees if team b wins my net profit is twozerok  rupees and c and d so on right you get the setup for every even there is an associated  value with it this is the value of event a winning b winning c winning d winning  so the net profit in each of these case so what is my expected net profit no give me a  formula sigma overall events right how many events do i have here four right so  rather i should say i equal to abcd right probability of i multiplied by the value  associated with that event so this is how you compute expectation ok everyone gets  this  so now suppose say am doing this right there are suppose four symbols i do not know  what i am teaching ok so and i am trying to communicate this from a source to a  destination ok and now suppose these are the four symbols that i give and if these one of  these symbols is say with probability one and if i transmit it what is the information that  this guy gets so this is assumed that a is that sun is going to rise today if i tell you this  when you are sleeping in the night what will you tell me so basically are not gaining  any information well it is a certain event you know this is going to happen right  now one of these events suppose i am going to say that this there is going to be a  cyclone tomorrow morning what is the probability of a cyclone happening in chennai  almost one but still it is a very rare event so if i tell you something which is very rare  that message has a very high information content right so if event which has a very  high probability has a very low information content and an event which has a very low  probability has a very high information content right so you can measure the  information content of an event   so so the point is that what you can have is that the information content of an event you  can write it as how many of you get this how many of you have seen this before all of  you have seen this right so this is the value associated with an event ok now can you  tell me what is the expected information content for every event now i have given you  the value associated with that even so what is the expected information content  summation p of i into information content of i and this like and this is of course log  right so it would be so what is this called this is called the entropy  now what is cross entropy how many distributions are you dealing with here one which  is the p distribution which tells you how likely these messages were and based on that  you are trying to calculate the entropy of this situation right so now what is cross  entropy you have a true distribution say you have a predicted distribution ok this is  what you predicted so that means according to your predictions the information  content of every event is going to be log of qi because that is what you predicted right  but what are the actual properties which with these which these events are going to occur  pi’s right so then the expectation has to be computed over pi’s right  so then what you will have is summation pi log qi so this is what you estimated the  information content to be but the actual events are going to happen with this probability  right so this is your value associated with the event and this is the actual probability of  the event  right so this quantity is known as the cross entropy is it clear and this is a  way of measuring when would this be in when would this be minimized when both are  same that means if your prediction is very close to your true distribution this quantity  will be low minimized actually  so that is what we wanted actually you wanted to predict some distributions in all of  these cases and you wanted a measure which tells you that this prediction was good and  what is the definition of good it is as close to the correct value so cross entropy gives  you a measure of telling how close a predicted distribution is to a true distribution  so now instead of using the squared error which was actually pi minus qi right so pi  was my true distribution and qi was my predicted distribution i can use cross entropy  which is given by this model and it does the same thing it gives me a principled way of  measuring how close my predicted distribution is to my true distribution do you get this    so now so this was for whatever we have done so far right till this point this was for  regression right now i wanted to enter into classification for which i have built this set  up of how to take the difference between two distributions so now let us consider this  problem where we have this situation and which is a classification situation that you are  given four possible classes out of which one is the correct class and this is the true data  given to you this is the true distribution all the probability mass is focused on one of  these classes  now we want to given an image classify this into one of k classes if you could again  use a squared error loss but since we are dealing with probability distributions here we  want to use something special so before we get to what the special is going to be what  do i first need to tell you in the earlier case my output was not bounded was it also  dependent was there any condition on if the imdb rating is something the critics rating  should be something else or the  rotten tomatoes rating should be something else no  now in this case is there a tightly coupled behavior between the outputs why because  they should sum to one we are trying to predict a probability distribution so the sum  should one right so i need an output function which ensures this you get this setup    now we should ensure that y hat is also a probability distribution whatever we are  predicting is also a distribution so now can i use a sigmoid function yes it will give  me values between zero to one and probabilities are between zero to one but the sum would not be  y so sigmoid is ruled out    so what we use is something known as the softmax function how many if you have  seen this before please everyone raise your hands otherwise you will get zero on the  assignment fine so what does this what does this function actually do let us look at  this function right so here you had a l which was say a l one a l two a l three right suppose  we had three classes ok so from here i actually want to go to hl or rather i going to want  to go to y hat right which is going to consider y hat one y hat two y hat three right it is going  to give me probability of each of the three classes  let us assume there are only three classes right so now what this function does is how is it  going to predict y one hat suppose these values were onezero minus twozero and threezero so what is  going to be y one hat is going to be e raised to onezero divided by e raised to onezero plus e raised to  minus twozero plus e raised to threezero so now you see how the output is comp computed from  each of these values right so why did we do this e raised to stuff why could not i have  just taken onezero plus minus twozero plus threezero divided by the sum because we have negative  values  so once we take the exponent even the negative values become positive right so that  is why we need the softmax function i hope all of you wrote this in your assignment  they did ok so you get this we have a different output function now and this output  function does it make sense it gives us a probability distribution now the summation  would be one and each of these values would be between zero to one that is exactly what we  wanted      and now that we have ensured that y and y hat both our distributions what is the  objective function that we are going to use cross entropy how many of you convinced  it is cross entropy we have two distributions now we saw that a principled way of  computing the difference between two distributions is the cross entropy so we will use the  cross entropy  now can you tell me something about this sum there is something special about this  sum what are these three true values and these are the predicted values what is so special  about this sum how many terms are there in this summation k as many as the number  of classes in this case four how many of those terms will go to zero all but one right except  for the correct class everything else will go to zero so this just boils down to the following  loss function that if l is the true class right for that class yc is going to be one it is  going to be zero for everything else that is exactly what this vector tells you only that term  will remain so were actually trying to minimize this quantity    let us see so for classification problems this is your objective function you either  minimize the negative log of y hat l or you can say you are maximizing this thing ok  now what is this quantity y hat l no it is a predicted probability of the correct event  right so this is a probably no wait this is an important question so you have y hat l  here and this is a function of i mean this optimization problem is with respect to theta is  this a well formed objective function does y hat l actually depend on theta yes it does  so theta because why i tell is a function of all these things everything here and then a  log on top of that right so it is actually a function of all your parameters so this is a  properly set objective function we are trying to minimize or maximize with respect to  theta ok and you told me that y hat l is actually the probability of the predicted  probability of the correct class ok hence this quantity is also known as the ml class  pattern recognition class log dash of the data  student all refer time twotwo5three  all good and fill in the blanks  so it is a priority of the x belonging to the l th class and then hence y hat l because it is  the probability it is the likelihood of it is called as the log likelihood of the data log  likelihood  so what have we done so far we started with a feed forward neural network we  defined the hidden layers and the input layers and the weights and the biases we kept a  provision for the output layer to be something special right then we went to two classic  problems one is regression and the other is classification in regression we wanted to  predict values of all sorts of ranges  so we decided to use a linear layer there so that there is no bound on the values that  you can predict and your objective function should take care of where the bound lies it  should not allow values which are way off from the true values right and that is why  we use the squared error function there the other problem that we looked at was  classification where we saw that the label actually can be treated as a distribution where  all the mass is focused on the true label and zero everywhere  and our job is then again to predict our distribution so we are given the true  distribution and we predict another distribution so the output again we want something  special in this case which is a distribution so to ensure that use a spatial function which  is called the who said sigmoid softmax function fine and then we got a prediction  which is a probability distribution and then how did we find what was the objective  function what is the difference between the true and the predicted the cross entropy  right so we use cross entropy as the objective function and then with some  simplification we realize that it boils down to maximize the log of the probability of the  true class or other log of the predicted probability of the true class    so now let us look at the summary so if your outputs are real values what is your  output activation going to be linear what is the loss function going to be squared  error if your output is a distribution what is the output function going to be softmax  what is this loss function squared error cross entropy right now this grid light actually  takes care of a wide range of problems that you will see right think of any examples that  have been giving you so far movie prediction or sentiment prediction or image  classification or anything all of that you can fit into this frame of it  and so if you know these two loss functions how to deal with them then you can deal with  a large class of problems that you are going to deal and for the rest of this lecture which  will happen tomorrow we are going to focus on this at this particular output function  and this particular loss function how do we compute i have a loss function what i am  going to compute now the gradient with respect to all the parameters  so this is what we are going to focus on right so we have seen the loss function in  detail we have seen that the loss function is tightly coupled with the output function  now we are all set but given this loss function how do we start computing gradients of  this loss function  
task2/super_cleaned_audios/lesson33.wav,509.3076,stochastic gd adagrad rmsprop adam  welcome to lecture five of the course on deep learning and so today we look at some  variants of gradient descent so we will just quickly do a recap of gradient descent and  then look at some variants of it or some ways of improving it which is momentum  based gradient descent nesterov of accelerated gradient descent stochastic gradient  descent adagrad rmsprop and adam  so just to set the context so we started with this gradient descent algorithm for a single  sigmoid neuron and then we saw how to extend to network of neurons with back  propagation so we realized that all we need is the gradients or the partial derivatives  with respect to all the weights and biases once we compute that we can just use the  gradient descent update rule  now today what we are going to see is are there better update rules which lead to faster  conversion or better performance in various ways so that is why we are going to look at  all these different variants or methods of improving on gradient descent so that is the  context    i will just quickly rush through so for most of the lecture i have borrowed ideas from  the videos by ryan harris on visualize back propagation and some content is based on  this course by andrej karpathy and others when i talk about some tips for learning rate  and so on so you can just look at those also so we will just quickly rush through the  first two modules which we have already done    which was we were interested in learning the weights and biases for this very toy  network with just one input and one output and we started by doing something known as  guesswork where we were just trying to adjust these weights and biases by hand        and we realized that its clearly not good and but we still try to do a very smart guess  work where we were driven by this loss function which was telling us whether this  guess the current guess is better than the previous guess or not and we just kept  following our guess work and try to reach to some solution and for this toy network it  was very easy to do that      and what we were actually doing is there is this error surface which exists which can  be plotted for all possible values of w comma b and what we were trying to do with this  guesswork is trying to find path over the error surface so that we enter into the better  regions so red is bad blue is good the darker the shade of blue the better and this of  course becomes intractable when you have many parameters and so on    so we wanted to have a better way of navigating the error surface so this is exactly  what we were doing with the guesswork algorithm    so then this better way actually we realized that we could arrive at it from a very  principled solution from starting from taylor series    and we went to this derivative where we finally came up with this rule that move in the  direction opposite to the gradient      so that is the rule that we have been sticking to since then and we also along the way  realize some of these things which we defined carefully which was what is what exactly  this quantity means which is the partial derivative with respect to w evaluated at a  particular weight comma bias configuration and because this is an iterative process you  are at a certain value of weight and bias and you need to change it from there    and we then created an algorithm out of this and when we ran this we actually derived  the full derivative and so on    and then when we finally ran this algorithm so this is where now i will slow down  so when we ran this algorithm so let us see what was happening here right so i will  just start the algorithm from the beginning  so we are now going to run this code and you tell me something that you observe ok  so i am just clicking so there is no change in the pace at which i am clicking this  right so every click of this is one time step and i am just continuously clicking this i  will start now do you observe something fl ok do you observe something  it was initially slow then suddenly picked up and then it again became slow why did  this happen the slope is small why ok how many of you completely understand why  this slow and fast moment was there please raise your hands good so that is what we  will focus on now right so we will try to see this    so we will i hope this has been fixed ok so let us take a simple function which is f of  x equal to x square plus one right this is how it will look like now in these portions of the  curve the curve is actually very steep right and in these portions the curve is a bit gentle  and of course it becomes very gentle over here right all of you can see the pen marks  properly  so now let us see what this means this steep and fast and small so let us look at a  region which is steep ok now what i am going to do is i am going to change my x by one  i move my x from one to two how much did my y change all you need to do is just  substitute in this formula right for two it evaluates to five for one it evaluates to two so when you  move from one to two your function changed from two to five ok so there is a large change in  the function for one unit change in your value of x everyone sees that  now let me do the same at a gentle portion of the curve i will do it here now when i  changed the x by one unit again one unit right it is the same change which i did earlier i  changed from zero to one how much did my y change  student one  oneok now actually what is this quantity delta y one by delta x one  student slope  it is the slope it is the derivative at that point so what are you inferring from this what  happens to the derivative when you are at steep slopes  student it is high  derivative is high because the change in y is much faster than the change in x what  happens to the derivative when you are at the gentle slopes  student smaller  smaller because the change in y is small or relatively smaller as compared to the change  in x or it could also be missing but just these two are relatively different is what i am  trying to impress upon right and so that means the derivatives at the steep slopes are  larger in magnitude whereas for the gentle slopes they are smaller in magnitude  now can you relate it to the observation that you had on the previous slide when we  were at the plateau it was a very dash slope gentle slope what would the derivatives be  student small  small now what are our updates you have w is equal to w minus the derivative right  now the derivative is small what will happen to the updates  student small  they will be small what would happen if the derivative is large  student the updates would be large  the updates would be large therefore in the gentle areas you are moving slowly and in  the steep areas you are moving fast you get this picture very clearly now this is going  to be the basis of a lot of things that we do today so it is very essential to that you  understand this perfectly all of you get this properly good    now now you might say that this was only that special point again and i always get  those questions so let us see what happens if you start from a different point    so now again the same gradient descent algorithm i am going to run but instead of  starting at this point which was my random initialization i just happened to choose a  very different random initialization which is here everyone sees that  now let us see what happens what do you expect initially fast movement because the  steep the slope is a bit steep now what would happen it will become slow because you  have entered a gentle slope region and then again fast right so and then again it will  become slow  so see in this gentle region right the changes in w are so small that all your black points  are actually indistinguishable from each other it is almost like a snakes body whereas in  these steep slopes you can see a large change in the w you can see gaps between the  values of w right so this is irrespective of where you start from gentle means slow  movement steep means fast movement that is the basis  
task2/super_cleaned_audios/lesson6.wav,353.6676,"so i  was talking about successes in image speech pattern recognition even natural language processing and so on so one interesting thing here is about sequences so i will talk about sequences now  sequences are everywhere when you are dealing with data  so  you have time series which is like say the stock market trends or any other kind of a series time series then you have speech which is again a series of phonemes or you have music you have text which is a series of words you could even have videos which are the series of images right one frame each image each frame can be considered to be an image and so on so in speech data one peculiar characteristic of speech data is that every unit in the sequence interacts with other units so words on their own may not mean much but when you put them together into a sentence they all interact with each other and give meaning to the sentence right and the same can be said about music or speech or any kind of sequence data so all these elements of the sequence actually interact with each other  so there was a need for models to capture this interaction and this is very important for natural language  processing because in natural language processing you deal with sequence of words or all your texts or sentences or documents or all sequences of words so that is very important and the same in the case of speech also  so if you take up any deep learning paper nowadays it is very likely that you will come across the term recurrent neural network or lstms which are long short term memory cells and so on  so this is also something which was proposed way back in one thousand, nine hundred and eighty-six  so a recurrent neural network is something which allows you to capture the interactions between the elements of your sequence i had said at a very layman level but of course you are going to see this in much more detail in the course and this was also not something new even though you hear about it a lot in the past three to four years the first recurrent neural network and what you see here is exactly a very similar to what we are going to cover in the course was proposed way back in jordan by jordan in one thousand, nine hundred and eighty-six   its variant was proposed by elmen in 1990so this is again not a very new idea this has existed for some time but now there are various factors because of which it has been possible to now start using them for a lot of practical applications as i said one you have a lot of compute time and the other you have a lot of data and the third is now the training has stabilized a lot because of these advances which i was talking about in terms of better optimization algorithms better regularization better weight initialization and so on  so it has become very easy to train these networks for real world problems at a large scale so that is why they have become very popular and hear about them on a regular basis but it is again something which was done way back  so from one thousand, nine hundred and ninety-nine to 199four actually people also looking at various problems will be training neural networks and recurrent neural networks and so that this problem which is known as exploding and the vanishing gradient problem which is again something that we will see in the course in reasonable detail we have this problem and it is very difficult to train recurrent neural networks for longer sequences so if you have a very long sequence or a time series you cannot really train a recurrent neural network to learn something from that   and to overcome these problems around one thousand, nine hundred and ninety-seven  long short term memory cells were proposed and this is again something that we will cover in the course and this is now almost de facto standard used for training for a lot of nlp work lstm are used as one of the building blocks and another variants  of lstms which are known as gated recurrent units and some other variants  so  this  is  also  not  something  new  even  though  they  have  become  very  popular nowadays like almost any article that you pick about to talk about any article on deep learning that pick about to talk about recurrent neural networks or lstms or gated recurrent units this is not something which is new   lstms had come way back in one thousand, nine hundred and ninety-seven  but again due to various compute and other issues which i said at that time it is not so easy to use them but by 201four because of these parallel progresses which i mentioned in terms of optimization regularization and so on people are now able to use rnns lstms for large scale sequence to sequence problems and in particular a very important discovery at this time are very important model which was proposed at this time which is attention mechanism which is used in a lot of deep neural networks nowadays which enabled to deal with a lot of sequence prediction problems  for example translation where you have given one sequence in one language and you want to generate the equivalent sequence in another language so this is known as a sequence to sequence translation problem so for that people proposed a sequence to sequence attention network and this was one of the key discoveries which then led to a lot of adaptation of or  adoption of deep neural networks for nlp  a lot of research in nlp happened which was then driven by deep neutral networks so a lot of existing algorithms which are non neural network based algorithms which are traditionally used for nlp was slowly replaced by these deep neural network based algorithms ok and again this idea of attention  itself is something  that was explored earlier also somewhere around one thousand, nine hundred and ninety-one or so and it was something known as reinforcement learning which was used for learning this attention mechanism what attention basically tells you is that if you have a large sequence and if you want to do something with this sequence what are the important entities of this sequence or elements of this sequence that you need to focus on so this is again something that we will look at in detail in the course   "
task2/super_cleaned_audios/lesson27.wav,986.2628,now we go to the next module where we will first see how to compute the gradient  with respect to the output units well that was the first guy in our chain right that is the  first person that we need to talk to  so that is the part that we are going to focus on    so this is the output and when i say i want to compute the gradient with respect to  output unit what do you actually mean what is the quantity that i am looking for i will  help you out actually what i meant by output unit is this entire thing right so i actually  meant al’s ok but it is it is a fair answer and even y hat is a fair answer ok in fact am  going to start with y hat and then go to al so i will have to start with this guy and then  come to this guy    so this is the loss this is y hat which is equal to y one hat y two hat up to yk hat so these  are the k values that we have here and we are looking at cross entropy that means we  are looking at the classification problem right so we have got a distribution over the k  classes that is what y hat looks like and we know that one of these guys is the right  class maybe say y two so the loss function is minus log of y hat two because two is the correct  class in this toy example that i am considering ok so the loss function i am just  repeating the definition right that is how the loss function is    now oh god so again this is what our y hat looks like ok now i want to compute the  gradient with respect to any of the output units right so it could be y one y two y three y four up  to yk right so this i actually can take values from one to k in this case one to two right ok  now can you tell me what is this loss ok this much is fine can you tell me what is this  derivative minus one by minus one by y hat l if y is equal to l  student refer time zerotwozero8  and zero otherwise how many of you get that cool ok so it is a very simple thing that you  can think of this as z and this is y only if z is equal to y then the derivative would exist  otherwise it is going to be zero right ok so how do i write this fe part using  student refer time zerotwotwo7  how many of you have seen indicator variables before good so this is what you are  telling me right it is going to be minus one by y hat l if i is equal to l ok and if i is not  equal to l then these two things are not related it this is a function of something else and  you are taking a derivative with respect to a different quantity   so it is a constant with respect to that quantity and the answer would be zero ok now i am  going to write this as this right so this is the same as saying so this variable actually  this is known as the indicator variable it takes on the value one if the condition in the  bracket holds otherwise it takes on the value zero so this is exactly i am writing exactly  this but in a more compact manner ok is that clear to everyone      so this is what the quantity this is the quantity that we have computed with respect to  one of the output units ok so this is what derivative partial derivative gradient how  many of you say derivative no one likes derivative partial derivative that is always  the safest choice partially fl right and gradient oh there is one brave soul who say is  gradient do not worry well fix that ok  so this is the partial derivative y because my y hat is actually a vector and i am taking  the derivative with respect to one of those guys ok now if i want the gradient with  respect to y hat what would that look like a vector which is a collection of  student refer time zerofourzeroone  partial derivatives so let us see this is the quantity that i am interested in am interested  in the gradient of the loss function with respect to the vector y hat so remember the  vector y hat is y one hat y two hat up to yk hat right so this gradient is going to be a  collection of the partial derivatives with respect to y one hat y two hat and so on  now what is each of these quantities so it is simple right so this quantity the  derivative is either going to be zero or is it going to it is going to be one by y one hat right if l  is equal to one right and that is exactly what i have done so now how many elements  here are actually going to be nonzero at a time how many of these going to be nonzero  one which one  student refer time zero5zerofour  the one corresponding to l right everything else is going to be zero so this is a dash vector  y not vector ok so now am going to write one hot vector like this what have we done ok  where el is what one hot vector such that it is l th entry is one ok that is what am going that  is how am going to define e l is that fine with everyone ok   and so you see the story how did how we went about computing this we started with a  partial derivative with respect to one of t guys right we found a formula for y i we saw that  this formula is generic enough and so now we can compute the gradient which is a  collective of all these yis where i ranges from one to k right and then we just put that in a  gradient vector  so this story is going to repeat throughout the lecture where we try to compute the  gradient with respect to one guy and then generalize oh sorry we compute the partial  derivative with respect to one guy and then generalize and try to find the gradient fine ok      so what if i what do i have so far i have this quantity what does till which part of the  diagram am i currently the dash green part dark green part i am till here i need to go  till the light green party that is collectively the output unit ok although i have divided  into two halves but when i say output unit i mean that output neuron right complete  neuron so what i am actually interested in is these quantities or more specifically ok  this is what i am interested in what is this one of those guys right this al is actually  alone up to alk right so this is one of those guys so this is going to be the gradient or  this is going to be the derivative a partial derivative sorry ok now what do how do we  proceed from here    now i will again have to compute this you already know that good but before that i  want you to answer one question right so y hat l what is y hat l it is the output  corresponding to the correct class does it depend on an arbitrary al i so in the  previous thing we saw that only when i is equal to l there is a connection in this case is  there a connection always or only when i is equal to l  student refer time zero8zerotwo  always why softmax so   student refer time zero8zerofour   denominator has all the ali’s right so this is there it is y hat l in the numerator of  course it only has this unit which corresponds to the l th probably did not choose my  variables very well so l th component of a capital l right and but in the denominator  you have the entire sum which means that every output guy here each of these dark  green guys depends on each of the dash green guys light green guys good   so that is at least settled that we always the we can always compute this partial  derivative we do not need an if else here there is nothing like l is equal to i then what  will happen it will always have this partial derivative    so we will now derive the full expression for this so this is what we are interested in is  this fine so this is a function of the form so you are taking how do i say this so this is  log of a function so first you will take the derivative with respect to log and then push  the partial derivative inside right so that would be minus one by y hat l and then the  derivative with respect to y hat l now what is y hat l the softmax function right   so it is the l’th entry of the softmax function applied to that output vector what is the  output vector al right so it is the l’th entry of the softmax or l’th entry of the function  applied to the output vector  so this was our al what is our output right so now one of these guys here is the l th  guy and one of these guys here is the l’th guy right so what you do is you take this you  apply the softmax function to it which again gives you a vector and now you are  interested in the l’th component of that vector that is what this quantity means it should  be clear now    now i will just do some simple math stuff here and we should be able to derive this is it  fine am just replaced by the actual softmax formula this is a derivative of the form u by  v right what is the formula for that yeah it perfectly right yeah so this is what it  would be right i mean it is you all know this i am not going to spend time on this  so now am just going to substitute the values here yeah it is getting a bit nasty but it is  not very difficult right so so this so this is our g of x so am taking the derivative of  that then this is this one over h of x you can just figure it out right anyway it everyone just  read this for a few seconds and let me know if this is not clear this is g this is h in this  formula right have just substituted the gs and hs in this now what is this quantity going  to be it is derivative of the form e raise to x right so it is e raise to x always  student refer time oneonethreethree  if i is equal to l right so now we have this dependence because we are looking at a  numerator but the numerator only depends on the l th entry right so now you are trying  to take the derivative of the l th entry with respect to some arbitrary i th entry so only if  l is equal to i you will get the derivative right    now what about this how many terms in the summation would remain  student refer time onetwozerotwo  one which one  student refer time onetwozerofour  where i dash is equal to i right so the i’th guy would remain the rest of it is  straightforward right this square i have just divided into two parts ok ah now let us see  can you simplify this because i cannot ok can you simplify this what is this  student softmax  softmax and which entry of the softmax  student refer time onetwothree6  l’th entry i’th entry l’th entry with the saw with the indicator variable but what is this  this is our input hidden layer output so ok now let us see what is the next step  this  is should have been y hat i but y hat is equal to f of x right so we can fix this unit so  ok fine so we have actually what do we have now we have the derivative of the loss  function with respect to the i’th unit of the output layer right and which part of the  output layer the pre activation pattern ok now what am i going to do i have a formula  which tells me how to compute this what was i actually interested in so now how am i  going to go from here to there i just put all the partial derivatives into a  student vector  vector and that vector is the  student refer time onethree58  gradient good    so we have this one formula it is ok if some of you did not get this derivation right it is  very very straightforward if you go back and look at it i am pretty sure you will get it is  nothing in this is very simple elementary stuff right except for some degree here and  there ok so now what would this look like   we should add actually l theta here this would look like a collection of all the partial  derivatives we have a generic formula what will we do now what is the first entry  minus in indicator l equal to one minus y hat one which is the variable that we are indexing  over i right not l oh god oh we are indexing or ok have i goofed up oh that is wrong is  it oh yeah that is wrong fine then this is fine we are indexing over i and then we can do  this  now can you simplify this i am looking for ok this is the element wise difference of two  student refer time one5three8  of the indicator vector and  student y hat    y hat oh hey we should change all this y hat is equal to f of x right but i just want it to  be consistent as y hat so is this fine this is a simplification fine right so we have  come a long way right you have finish this part ok we have got the gradients with  respect to the output units ok this much part is a clear to everyone moduler bit of the  math which you can go back and look at it this entire derivation is fine but you get the  concept right that we start with one unit from there grow the gradient then keep going  applying the chain rule  so we started with the dark green guys and then went to the light green guys now we  have the derivative with respect to the entire light green vector and that is what we had  started off with that we wanted the gradient with respect to the output units   
task2/super_cleaned_audios/lesson26.wav,790.4592,this lecture is on backpropagation and feed forward neural networks so we introduced  a feed forward neural networks we saw the input layer hidden layer and the output  layers and we saw that the output layer actually the output function depends on the task  at hand and we considered two main tasks one was classification the other was a  regression  for regression it made sense to use a linear layer at the output because we did not want  the outputs to be bounded they could be any range and for the classification problem  we realized that we want a special kind of output because we are looking for a  probability distribution over the output and for that we use the softmax function and in  both cases we used a different kind of a loss for the regression problem the squared  error loss made sense because we predict some values and we want to see how far we  are from those values  but for the other case the classification we realize that it is a distribution so maybe we  could use something which allows us to capture the difference between the true  distribution and the predicted distribution    and therefore we had this figure emerging which was depending on the output whether  it is real values or probabilities you will have different types of output activation  functions and different types of losses  and of these combinations today we are going to focus on softmax and cross entropy  and our aim is to actually find these gradients remember there are many of those we  have seen this large matrix which had many such partial derivatives and we want to find  that entire matrix i hopefully do it in a way that it is not a repetitive we could compute a  large number of partial derivatives at one go  so before we look at the mathematical details we just get an intuition for  backpropagation    and then we will get into the gory details of how to actually compute these gradients and  partial derivatives so this is the portion that we are in we are intended to ask these two  questions and this is where we are    so now this is what our network looks like this is clearly much more complex than that  single neuron that we had and which had only two weights w and b that was very easy to  compute the gradients there now imagine that i want to compute the gradient of the loss  function and let us assume it is a classification problem then what is the loss function  minus log of y hat  so this is the loss function and we want to compute the derivative of this with respect to  one of these weights in the network and am deliberately taking something which is  much farther away from the loss but why do you say why do i say it is much farther  away it is right at the input layer right and the loss is somewhere at the output layer  so we want to compute this gradient    now to learn sorry you want to learn this way to learn this weight we know that we can  use gradient descent we are all convinced that this gradient descent algorithm which i  have shown here as long as we put all these variables or all these parameters that we  have into theta  we can just run the gradient descent algorithm and compute them the only thing that we  will need is this partial derivative with respect to all the weights in the network and in  particular with respect to this weight that i am interested at    now so we will now see how to calculate this we will first this is only to get the  intuition so we will first think of a very simple network which is a very deep but the  thin network it has many layers but it is a very thin network here you see what i mean  by a thin network ok now this is what i am interested in can you tell me how to  compute this this looks like a chain so it is justified the user chain rule of derivatives  so what would the chain rule look like  you want to compute the derivative of this with respect to this and you have done this  in high school right so you have functions of the form of sine of cos of tan of e raised  to sine of x and this is exactly how this chain is right you have some function of x  followed by another function of x another function of x function of x function of x and  so on you just keep making a composite function of the input we actually wrote down  that function if you remember it was just one function applied after the other function   or a very composite function so you just need to apply the same idea here so we take  we go step by step so i am almost accounting for every shade of color here  so dl theta by d y hat then dy hat by d a l eleven there is only one neuron here then this  with respect to the sorry h twoone then h twoone with respect to a twoone a twoone with respect to h eleven h  eleven with respect to  a eleven and then a eleven with respect to w eleven  so i just traversed down the chain in the reverse order this is how the chain rule works  right anyone has a problem with this it is straight forward right and now what i  have done is for convenience i have just compressed the chain you see the red part and  the green part i have just compressed this weight so that and this is again something  that you have done in high school if you have this you could just write the chain as the  first and the last it so this is what you can do and i also compress this other chain ok  and am going to use these kinds of compressions later on  so what am trying to impress on you is that if i want to go from here to here right that  is what my intention is if somehow i have already travels from here to here then i can  just reuse that computation that is the idea which i am trying to impress on it i do not  need to follow the entire chain every time i can do these partial computations up to a  point have you seen this something similar idea somewhere else dynamic program is  something like that so you have just computed up to a certain point and then it is reuse  the value for further down the chain  so that is what we are going to do and same for all the weights right for each weight  the chain size would be different depending on where it lies in the network right for the  weights which are very close to the output layer the chain would be very small makes  sense ok so this is the intuition and we will see the intuition a bit more    so let us now understand this in the terms of the wide complex network that we are  using    so what actually is happening is that we are at a certain stage that means we have  some values of ws and b’s ok at the initial stage we just have these w knots and b  knots but let us assume that we have done some training and we are at a certain level  we are at wt at time step t and bt at time step t right for all the weights inverse now we  feed it a training example we do this entire compute computation what do we get at the  end we get y hat which is a function of this x that we have fed it but we also know this  true y we know the true value we know y hat  so we can compute the loss function so we compute the loss and to our surprise we see  that the loss is not zero we are getting a non zero loss that means the network has not yet  learnt properly right the weights and biases are still not in the right configuration that we  want them to be in right so now what do we do we go on this path of investigation  we want to find at who in this network is messing up things there is someone who is  causing this problem because of which i am not getting the desired output and we are  on our quest is to now find out who this guy is who is responsible for this so what  would you do where would you start the output layer    because the output layer is the guy who give you the output right so go and talk to  him and we say that hey what is wrong with you why are you not producing the desired  output right now what is the output layer going to tell you in very civil language i  will say i cannot do anything boss i mean i was just given some weights and inputs  from the previous layer and those weights and inputs were messed up  so there is nothing which i can do go and talk to them so who will it directors do it  will say that i am just as good as wl hl minus one and bl because these are the guys that i  completely depend on if these guys were ok then i would have been fine so we then  go and talk to these guys that what is wrong with you    so now they say ok fine wn and bl take the responsibility they are the nice guys they  say we are the weights we are supposed to make a we are the ones who are responsible  for the adjustments in the network so we have failed to do our job properly and i think  we should get adjusted right but then hl will resist it will say it is not my fault why  will it resist because it against again depends on the previous activation layer  so till then point as to what the ws and b’s in the previous layer right and you see  how the investigation is now proceeding where will we reach well keep going down the  network we are talking to everyone in the network we are talking to every dark green  guy every light green guy every dark blue guy every light blue guy we are also talking  to all these weights and biases and in the end what do we figure out the responsibility  lies with all the weights and all the biases they are the ones who are responsible for this  now but now we find out that this is also one of those weights which is responsible and  this is also one of these weights that is responsible but it was have been very difficult  for us to talk to them directly so then what are we going to do instead of talking to  them directly which is this we will talk to them through the chain rule so we will talk  to the output layer that is exactly how what we did maybe went to the first guy that we  knew that guy pointed out to the previous hidden layer that guy pointed us to the  previous hidden layer and then finally we get to the weights right  so this talking to is fine but where do derivatives figure out in this why are why is the  language derivatives why are we not talking in english or hindi or something else  what does the derivative tell us so talking about gradient descent like what we saw in  gradient descent but in general what does the derivative tell us if i change this a bit  how much does my loss change right  so that is how much this guy is responsible for the loss because if this is very sensitive  even adjusting a bit of this i could drastically reduce the loss right so that is what the  derivative tells us that tells us how sensitive is the loss function to the weight or any  quantity with this with respect to which am taking the derivative right that is why the  language is of derivatives right is that clear is the intuition fine to everyone    so now will convert this intuition into actual math and try to figure out how to compute  every guy along the way right and we will use this idea that we have made some  partial computations and then well use it for the rest of the chain so we have made this  much at some point we will reach where we have made this much and then you could  use it for the rest of the chain in fact we will start right from here well start with this  guy and then keep expanding the chain  so the rest of the story is going to be about computing three quantities can you tell me  which are these three quantities gradients with respect to the output units gradients with  respect to the hidden units and then gradients with respect to the parameters so these  are the three things that we need to do if we do this we have everything in the chain and we  are done and the other thing that we need to do is we cannot sit down and compute this  for every element right we want to have it in a generic fashion where instead of talking  about w one one one w one one two and so on  we should at least be able to talk about w one w two and so on so that means we have  only three matrices and three biases right at least at that level so we have to do a collective  computation instead of just computing for every guy so instead of looking at scalars  which is what we are doing when we are doing gradient descent for w naught and v  naught we were just computing the update rule for w and b we want to now do it for  vectors and matrices  so that is that is the transition that is going to happen and our focus is going to be on  what cross entropy and softmax why is that important because that is the loss function  so that is the quantity that am going to take the derivative if i change the loss function  all the gradients are going to change are all the gradients going to change only the first  guy will change in the change all this should remains still same right modulus some  conditions but largely it should remain the same right unless you change something in  between   
task2/super_cleaned_audios/lesson7.wav,81.4269,now since i mentioned rl so we will go on to the next chapter which was now becoming much more ambitious with what you can do with deep learning and people started beating humans at their own game quite literally so there was this starting with atari games in two thousand and fifteen where resources from deep mind show that you could train a deep neural network to play atari games and do much better than what humans do so that is something that they were able to show on atari games and then people started looking at other game so then there was this go and this popular tournament and which alphago which is deep reinforcement learning based agent was actually able to beat the reigning champion at that time one of the best players of go at that time then even at poker were something known as deepstack which is again a deep reinforcement learning based agent which is able to beat eleven professional poker players at this game  then other games like defense of the ancients since on which is a much more complex strategy based game where again deep reinforcement learning based agents have shown a lot of success in beating top professional players on this game 
task2/super_cleaned_audios/lesson32.wav,2607.1271,so for the next module we need something known as cross entropy so we will just try  to make some develop some intuitions for cross entropy and get to the formula for that  and then i will tell you how it relates to the problems that we deal with ok so first let  us start with something simple that what is it that we are trying to do ok so with that i  will give you an example and i will ask you a few questions and then from there we will  slightly try to go towards cross entropy so now suppose you have an urn which has  thousands of balls and these balls are of three different colors which are red black and  white so you have an urn which has three different types of and there are many such balls which you have put in it and since you have put in it you know how many red balls are there how many blue balls are there and how many white balls are there so for you it is very easy to compute the probability of each of these things so that is say our probability is twenty-five thirty-five and four now talking more formally what is happening here is that you have a random variable x which can take on the values red blue or white right and this is the probability of each of those or the random variable x taking any of these values ok so this is the setup now i am your friend and you tell me that you can peep into the zone you cannot actually count take out all the balls and count them and estimate the probability we can just take a look into this and try to give me an estimate of what these actually what are these probability values that means what is the value so what is the probability that x is equal to red or x equal to white or x equal to blue so i just take a look at it turn it around a bit and try to get some feel ok i see a lot of red balls but a fewer blue balls or white balls and so on and based on that i make my best estimate right so i will just say that maybe these probabilities are thirty-five fourfive and two right so this is actually the true distribution i will call it as p right because this is the correct one and what i have estimated i will call it as q and remember now p has you can think of p as a vector which has these three values p one p two p three because there are three possible events here and similarly q has three values q one q two and q three so in this case i clearly know that i am wrong or when i give you these values you know that you are wrong so you tell me that whatever you have estimated is wrong then i obviously ask you tell me how wrong was i so how would you give me that number that is the thing that we are interested in so the general problem that we are interested in is that there is a true probability distribution and there is an estimated probability distribution and we want to find out how bad was the estimation now can you tell me a simple way of computing this it may not be correct but still it makes sense  student squared error  you can just take the squared error so what you are essentially telling me is that you could just treat these two as any other vector right and you could take the squared error difference between these quantities so what you are telling me is this where i goes from one to three ok so this is one valid way of doing this but then we are ignoring the fact that this is a distribution and hence it has certain properties that the sum of the elements is one and so on all of them are positive and things like that so we are ignoring those kind of things we are completely ignoring the fact that we are not dealing with a normal vector but a spatial vector which happens to be a distribution so now we want to find out a more principled way of computing the difference between two distributions and in practice why are we interested in this because we will always have a true distribution and a predicted distribution so that is what we want to do we have some way of computing it but you want a better way of computing it now let me make a case for why do we care about such differences right so let me take a simple case of a classification problem and to motivate that i will start from a different example and then i will come to the classification problem suppose there is a tournament going on and there were four teams which leads the semifinals let us call them a b c d ok now you were following the tournament up to the semifinals and after that you didn’t watch the tournament and you do not know who eventually won well the tournament is over and someone has won it i actually watched the tournament and i know that b has won it now can i express this in terms of a probability distribution right so first let us look at what is the random variable here what is a random variable here the team which won right so that is my random variable and it can take one of these four values now i know that team b won because i saw the tournament and i have seen that they won so now how can i write this as a distribution what is the distribution comprised of it comprised of these probabilities assigned to each of these events and there are four such events here so how do i write this distribution so what you are telling me is i could write it as zero one zero zero so essentially they are telling me that all the probability mass is focused on one of these outcomes because that is the certain outcome that is already happened no one can change it so that is the outcome for this tournament so i know that the probability of that even is event is one and everything else is zero so in other words the probability that the random variable x takes on the value b is one and everything else is zero so what i am trying to tell you is that even for a certain event you could still write it in terms of a distribution where all the mass is focused on that event now again i will bring the same setup that i did not watch the tournament after the semifinals so now you ask me give me your prediction what which team would win or this is the prediction which i made before just after the semifinals or just before the semifinals that i think one of these teams is going to win the tournament and the chance of each of them winning is something like this so i know the teams i follow this sport and i probably know that ok b has a very strong team and they have a very good record in the past few months and so on so maybe they have a higher probability of winning so these are the numbers which i assign now again i have made an estimate was my estimate perfect when would it have been perfect if i had predicted with certainty won that b is going to win but i was not willing to bet everything on b so i said there is a very high chance it will win but there is still a chance that there could be some surprises now how wrong was i now again tell me can you tell me what is p and what is q here this is the true distribution and this is my predicted distribution and what am i interested in again the difference between them how wrong did i go and what again what is a simple way of doing this again square errors so again this is what my formula would look like so this is fine in this toy case but why do we care about in real life examples that we are going to deal with in machine learning so in watching learning will deal with a lot of problems which are classification problems and in classification problems you would again have this setup where you have a label the good thing of the label as a random variable and it could take off one of many values so i will again assume that it could take suppose you are trying to take a picture of fruits and you are trying to classify them and i could again think that i have four fruits say apple banana cherry and dragon fruit ok and this random variable can take one of these four values depending on the image that i am seeing ok now i have been given some training data so for every training data i have been given an image and i have been given the correct label so for that training data what is this distribution suppose i have been given the image of a banana what does this distribution look like zero one zero zero right again i have seen it so i know it is certainly a banana so i do not have any confusion all the probability mass is focused on that now the same image we are going to show to one of our models ok and it is going to make a prediction and will again ask it to give us a distribution the model will give us values perhaps like this ok so this is the models prediction again the model has given us a distribution and we have a true distribution and we are interested in knowing how wrong the model was so that student correct the refer time zero9two6 we can correct the parameters of the model so this is our dash function loss function right so a loss function is some notion of difference between p and q right and so far we have been dealing with a very simplistic notion of this difference which is just the squared error loss ok and we want to do something better than this right so what i have told you is that you could always have a true distribution always have a predicted distribution and you would be interested in finding the difference between them that is the one first part the second part is that even when you are given something with certainty you could still write it as a distribution such that all the mass is focused on that event which was which has happened right which was the label was banana in this case and then you could still predict this from your model and now you are interested in knowing how wrong you are model wind because that is the loss function that you will use and then you will try to update your parameters with respect to this loss function means that is the setup that we are interested in so that is so i made a case for why we need to find differences between two distributions how to do it in a more principled manner we have not seen that yet we will get to that ok so before i get that i also need to tell you something about expectations so let us written to as sports example where there were four teams and say based on pundits and that sport they have said that these are the probabilities of winning and now you are into betting and you bet place your bets on these teams and you place our bets in a way that suppose team events then you end up winning onezero k rupees if team b wins then probably will end up winning five k rupees and if team c wins probably onezero k and if one of the other team wins maybe will end up losing money or something like that now you want to know what is your expected reward so now let us see what is happening here this was a random variable which could take on one of these four values these are the probabilities of the random variable taking that value taking on the value a or taking on the value b c and d ok this is your value or the gain or the profit associated with the random variable taking one of these values so you have a random variable you have a probability associated with every value of the random variable and you have some gain or value associated with every value of the random variable now how do you calculate the expected gain or expected profit which is this there is a threezero percent chance that you will earn onezero k there is a fourzero percent chance that you will earn five k there is a twozero percent chance that you will earn onezero k and onezero percent chance that you lose threezero k so the way you will compute it is that and this is the simple expectation formula which is the probability of now the event here belongs to abcd right this is one of the four teams that will win probability of that event happening in to the value associated with that event happen this is a fair computation you get the intuition that this is how you will compute the net reward that you have so this is how you compute the expected value with respect to a particular distribution so this is the background that we need now i will just go on to the next slide and now we will talk about entropy first perhaps information content be first then entropy and then cross sector ok so now what is information content so now again let us take the same case that we have a random variable which can take on values a b c d now let us what we are trying to say is that if i know a certain thing what is the information that i have gained so you and i are talking you tell me something and i want to see whether my information was enhanced whether my knowledge was enhanced that is how we will quantify information content so if you are talking to me and you tell me that my name is mitesh the zero information content for me right because i already know that there is no surprise in that ok but if you talk to me and you tell me that today there is going to be a lunar eclipse then there is a possibility there is some information content gain for me right because that is not a event which happens every day if you just tell me you will see the moon today and you live in a region where it is not typically cloudy and there is no information gain there right so what do you see here when is the information gained high when the event which happens is a very surprising event and how do you say supplies in terms of probability student refer time onefourtwo9 it is a very low probability event right so if there is again this tournament and say d was the weakest team in the tournament and a was the strongest team in the tournament if you come and tell me that d won then i would be really surprised that some information which i had gained but if you tell me that a won then probably i already knew  it at the back of my mind  because a is clearly  the strongest team  in the tournament and there is no information gained for me so one thing that we are trying to establish that the information content i see ok the information content of an event is inversely proportional to the probability of the event there is a that is a fair intuition fine now i want am still talking in terms of vague things i am saying it is inversely proportional but i still need an exact function so that i can compute it so i want something i want a function where i plug in the probability of an event and i get the information content of the event right now i do not have that function i am just building some intuition towards that function ok but this is one requirement that i want the function to satisfy this is something that all of us agree with ok now think of two events which are independent a and b ok so a is the event whether the ac is on here or not and b is a event which tells maybe so let us consider two different random variables so x is the random variable which can take on values zero and one sorry so x is again this random variable which can take on these four values abcd whether who won in the tournament and y is this another random variable which can take on the value on and off depending on whether the a c’s on in this room or not what can you tell about these two random variables they are independent random variables so this is on or off and this is which team won the tournament now i come and tell you something about the random variable y and i come and tell you something about the random variable x ok so now i want you to tell me this what is the information content of x and y i tell you something about x and i tell you something about y and these two events are in these two random variables are independent then what can you tell me about the information content what is the condition that you would want you gain some information by knowing things about x and you gain some information by knowing things about y so what can you tell me it should be the sum right because these two are independent events so whatever information i am getting from this random variable and this random variable which together enhancing my information right it is not cancelling out anything or is there is no common intersection there right if the two events were not independent then i would not expect this to hold because knowing something about the first event only tells me something about the second event right because they are dependent so then that case the information gained would not be additive ok so now let us see i already made a case that this function which tells me the information gain is actually proportional to the probability ok so that means this is what the input is going to be right and then what is the other condition that i want this is a fair thing right i just replaced information content by a function and i know that the function should depend on the probability because that is what we have established here so we know that the function depends on the probability we still do not know what this f is exactly but i am trying to impose some conditions on f one condition of f f is that this condition should hold ok now let us look at this condition which i have underlined this is f of is this fine because it two events are independent you can write them as the joint probability as p of x into p of y this is clear to everyone right you seem to be a bit lost arvind clear ok now what is happening here i have a function f of a into b and that is actually equal to f of a plus f of b what family of functions do you know which has this characteristic log right that is why log is a good choice for this that is why information content is going to be the log of the probability but i wanted to be inversely proportional right so it will be log of one by the probability ok so that is why information content of this thing is so you see this how did we arrive at this log formula and this log can just be to any base it does not matter so all of you get how we arrive at the formula for information contained now just give me a minute i need to think of what is the next thing that i have to say ok and so we have found out the information content of one of these events happening which was the x taking on the value a now let us think of this random variable x so here actually i should have said x equal to a probability of x equal to a ok it makes sense because the random variable is x and the event is x taking on the value a how much information content is in that so if i know that x was a how much will i be surprised by it ok now let us take this event this random variable x which can take on values a b c and d as i said with each value there is a probability associated with it such that this sums to one now i did not need to draw this diagram ok i should refer time twoonezeroone so x is a random variable which can take these four values which each of these values i have a probability associated ok so these are the values these are the probability values now what do i also have i have the information content associated with each of these right and the information content actually tells me the surprise of that evening now if i ask you what is the entropy of this random variable x so remember i had this case where i was betting i am with every poor outcome i had a value associated with it i had the same situation here with every outcome have a probability and i also have a value associated with this and the value is the information content now if i ask you what is the entropy or the expected information content of this random variable then how will you compute that i am asking you for an expectation so i will compute summation i belonging to a b c d p of x equal to i information can take what is the formula for that student minus refer time twotwotwo9 minus i will just take the minus outside ok so this quantity is called the entropy of the random variable right it is the expected information content in the random variable now  if you see what would be  the expect  entropy  of a random  variable  if it  is corresponding to a certain event that means say the sun rises always in the east right so what is going to be the entropy of that zero why you will have one of the sums in that summation as one log one right and every other sum would be zero into log of something so zero into anything is going to be zero even though that quantity is not defined zero into anything is going to be zero so the total entropy is going to be zero ok so this is entropy now what is it that we are actually interested in cross entropy so we have not gone there yet ok so we need to perhaps add one more slide so far everything is clear ok so now we are interested in something known as cross entropy so there again the situation is that there is something which is the true distribution and something which is the predicted distribution now actually before going there so let me just erase this off how many of you have thought that entropy is related to the number of bits that you need to transmit something do you know why that connection exists no now again let us think of this that you are trying to transmit a message and that message is again a random variable which can take on four values a b c d so think of these as four commands that we are trying to send to someone right and then based on that command someone will take some action now in the digital case how will you transmit this encode it to bits so what is the encoding that you will use zero zero yeah we will come to that zero one one zero one one so how many bits are you actually using for every message two bits ok for every bit so maybe this is a this is b this is c and this is d so for every message you are using two bits let us see actually what when you are doing this what are you actually assuming so actually assuming that all of these are equally likely if all of these are equally likely can you tell me the information content of any of them it is going to be minus log of one by four ok that is actually equal to minus log and this is to the base two ok one by four is two raise to minus two that is equal to two so the information content is actually equal to the number of bits that you are going to use to transmit that message now let us see if this is just in this special case or in a different case also suppose this could take eight values how many bits would you use three bits right so you will have zero zero zero zero zero one ok and this would be a to h now what are we actually assuming here each of these is equally likely what is the probability one by eight what is the information content two raise to minus three that is equal to three so the number of bits that you actually use to transmit something this can you can talk of it in terms of the information content of that now suppose i want to transmit this over the long distance so i need to bit be a bit efficient in terms of number of bits that i use right so now in this one of these cases suppose it is of the following form right that let us look at the case where x can take one of four values and say let me just put the right values so i will say one by two one by four one by eight one by eight ok now what is the information content of each of these one two three three and this is the message that i am going to send so what am i doing here i am using a different number of bits depending on the probability of that event why does this make sense why is this a smart thing to do if you want to transmit something which you are going to transmit a lot of times you better use less number of bits for that and this is exactly what is happening here a was having the highest probability and you are using the lowest number of bits for that now what is the expected number of bits that i will use up if it is a i will use one if it is b i will use two if it is c three and d three so what is the expected number of bits that i will use again i have the same situation right i want you to cast it into the same situation i have the probability values and with each of these guys i have a cost or a value associated what is the cost one bit two bit three bit three bit so what is the expectation now can someone compute the expectation one7five actually let me just write it down it would be again i belonging to a b c d p of x equal to i into the number of bits that you will use so that is just equal to log of log to the base two of p of x equal to i minus one what does this quantity actually this is the entropy we just saw this a this is the entropy of the random variable and what is it telling us actually that the entropy is one7five so what is the meaning of this actually so on average you will be needing one7five bits whereas if you are assuming everything is equally likely on average you are using two bits right so you see that on average you are making some savings here right so that is what the entropy tells you if you know what the probability of these events is then you better use that to decide the number of bits that you are going to use to send each of these so now let us complicate this a bit more now we have the entropy now let us complicate it a bit more so there is some true distribution which exists there is some true distribution from which these messages are coming right but you do not know what that true distributions we never know the true distribution that is the entire problem that we have been dealing with in machine learning so what you will do is we will somehow try to predict this distribution and this then the and the recipe that you will use is the same as that i used for the example where i had an urn right so there are these thousands of onezerozerozerozero’s of messages which has going to keep coming on you do not have access to the entire stream but you have seen some thousand of those messages just as i had peeped into the urn and i had seen some balls and i had made an estimate that i think based on these messages that i have seen so far i think these are the actual probabilities so the true probabilities are say p one p two p three and p four corresponding to a b c and d i do not know what this two properties are but i can estimate them looking at some samples or basically using my domain knowledge right maybe i would know that if one of these messages is stopped and i am actually trying to talk to a computer or a computer program that maybe stop is something which are used very rarely only at the end of the program or something so you have some either some domain knowledge or based on some samples i can estimate the value of this probability and i just try to relate it to the exact example of urns where you had these onezerozerozerozero’s of balls but you could not see all of them you sampled some and estimated a probability here again there is a continuous flow of message you cannot have access to all of these because they are going to continue but i have seen some of those and based on that you estimate these probabilities now based on this estimation how many bits so now this is the estimation that we have now based on this you will decide the number of bits that you will use for each of these messages right because you have some estimate so you want to be smart you do not want to keep two bits for all of them so you will just say that i will use log qi bits for the i’th message this is fair thing because i know that the information content is proportional to the probability in fact it is exactly given by this formula minus log of qi so based on my estimated probability i am going to do this ok and this is the number of bits that i have reserved now do you see a problem with this this is my estimation but the data is actually going to come from the true distribution it is not going to follow the distribution q it is going to follow the distribution p so now what is actually happened is this right this is the situation that we are dealing with we have p which was a true distribution that is the rate at which the data will come but with each of these events the value that we have associated is now related to q because q is what i have access to i do not have access to p i just have access to q so i have associated a value based on q does this make sense i should have actually used log p one bit’s log p two bits and log p three bits but i do not know what p one p two p three are i just estimated them based on some samples so that is q one q two q three and these are the number of bits that i am using now if i have to compute the expectation how will i do it i have to use p because that is the true distribution from which the data is coming so  what  would  the  expectation  now  look  like  everyone  gets  this  the  actual probabilities are this but because i am poor at estimating them i ended up associating these values which could be wrong right because i would have overestimated the probability of one of these messages and hence i have reserved lesser bits for that or underestimated the probability of one of these events and hence reserved more bits for that or vice versa i could have assigned a wrong number of bits to them right p no so do we have access to p in the sense so someone knows that right i mean there is a again in the same case as in the label case right we have access to the true p there and we are estimating a q when we are given these images for the training data we know that the distribution is zero one zero zero if the image is b for banana student refer time threethreefour9 then it is validated right so now this is what is this quantity called this is called the cross entropy ok you get why it is called the cross entropy because now you have two different distributions involved here ok you have the q distribution based on which you made your decisions you assign values to these events based on the q distribution but the true distribution is the p distribution so the actual number of bits that you use up on average is going to be based on the true probability they try to understand that now what will happen is for event a you have assigned a certain number of bits now how many bits will get used up it depends on the actual probability of p if that message is repeated many times then that is how this summation would be computed so this is called the cross entropy but now why is this the difference between two distributions that is what we wanted given two distributions we wanted to be able to find the difference between them now am telling you that cross entropy is a way of finding that difference why is it so so what would you want this difference to what is the property that you would want this difference to have if p is equal to q then if p is equal to q then student refer time threefivezeroeight not zero maybe it should take the lowest possible value right so this function right this is actually telling you loss of p comma q right this is what this is and we are calling it as the cross entropy this function you take it is minimum value when p is equal to q right because now at that point you are not really making any loss that is the best you could have done does this function take it is minimum value when p is equal to q yes why how is that obvious but why there could be something else which is lower than the actual entropy right why how you have to we are trying to minimize something so you have to give me answer so yeah so let us do that ok so how many of you it is obvious that q is the answer i mean the answer is p is equal to q it is not ok now this is the part which i am a bit worried about but i will just do it anyways so let me see how do i put this ok so remember that we had a p and we had a q and we want to find a q such that this quantity is minimized that is what our objective is so we want to minimize this with respect to qi ok now how do you find the minimize suppose i have this problem how do i find the minimum value how do i find the value of x which minimizes this take the derivative and set it to zero ok and then in this case i will get x equal to zero is that value can i do the same thing here and suppose it was this so now this is a function of two variables again i could do the same thing i could take the partial derivatives and set them to zero and i will get the minimum value now here this is actually a function of how many variables k in general right so q one q two q three up to qk ok now can you try doing the same thing can you can you take the derivative and set it to zero this is again a sum right it is very similar to this situation it is actually let me just write it down it is p one log q one plus p two log q two up to p k log qk now i want to take the derivative with respect to one of these guys say q two what would it be p two by q two is equal to what will i do that is the derivative p two by q two i will set it to zero do i get anything what is it that i am doing wrong here there is something that i am deliberately doing wrong is this an unconstrained optimization problem there is a constraint on the variables what is the constraint so why my true optimization problem is minimize with respect to q i’s such that summation q i’s equal to one do you know an easy way of dealing with these problems how many of you know the lagrange multiplier how will i use it here what will my objective function become then summation of qi minus one lambda then minus ok how many of you understand the intuition behind this that is a good answer now let us let me try to explain why this makes sense right this is the constraint that we have to operate within this constraint what i have done is i have taken the so now if the constraint is not satisfied what will happen to this quantity if the constraint is not satisfied that means my summation is not equal to one that is what means whether the constraint is not satisfied what will happen to this quantity it will be a nonzero quantity right fine then what will happen to my overall objective and i think we have made a mistake this should be plus i should add it right should be plus no it does not oh the lambda can be ok sorry so let me assume this is plus so what i am trying to do is that this is my objective function which i am trying to minimize i have added another quantity to it if this quantity is not equal to zero then i will not be the absolute minimum i will be at the minimum plus something right but if this quantity if the constraint is satisfied then this quantity will go to zero then am actually at the minimum of the function do you get this right so this is the function that i want to minimize i have added some quantity to it now that quantity is actually related to the constraint that i do not want to violate if i violate the constraint this is going to be non negative right so whatever minimum value i achieved i will be slightly higher than that because some nonnegative value has got added to it ok is that fine but if the constraint is satisfied then i can achieve the minimum value so that is roughly the intuition behind using this lagrangian multiplier it is a very crude intuition but there is of course a lot of math behind that but i am just giving you the intuition behind this which one student refer time fouronethree9 yeah that is what you could adjust the lambda and ensure that it is not negatively ok so now now can you do the same thing can you equate this to zero can you take the derivative and equate to zero what will you get now this term will give you p i by q two as before oh sorry p two by q two as before plus lambda times yeah plus lambda times one ok fine so equal to zero so then what will you end up getting p two is equal to i think it is something wrong here now this should be minus p two by k so p two is equal to lambda times q two ok and then further actually you can show that lambda is going to be equal to one how can you show that your constant is fine so do you see how we will get lambda equal to one so what does it actually tell you then p two is equal to q two that means all in fact you can show that all p i’s are equal to q i’s that means the distribution p is equal to distribution q so this cross entropy term will be minimized when your true distribution or when your plated distribution  is  the  same  as  the  true  distribution  and  hence  it  captures  the difference between the two ok and that is exactly what we were interested in we were interested in a quantity which can allow us to capture the difference between the true difference between a true distribution and the predicted distribution so we have arrived with that quantity and that quantity is cross entropy so therefore for all our classification problems where we have this scenario that we are given the true distribution where all the masses focused on one of the labels and you are estimating a distribution where you could give non zero quantities to many of those and you want to find out how wrong your estimates were with respect to the true distribution you can use cross entropy as a measure for that right so now your loss function which you wanted to depend on the difference between p and q it can just be the cross entropy between p and q 
task2/super_cleaned_audios/lesson36.wav,677.3457,let us look at nesterov accelerated gradient descent  so now we know that momentum based gradient descent is good at these gentle regions  it moves really fast but we do not still do not like it because it has this problem of  oscillations it has this problem that it overshoots its objective its goal and then it has to  take a lot of u turns so can we do something about reducing this oscillation so the  answer is always yes so let us look at nesterov accelerated gradient descent    so the idea here is very simple look before you leap ok now remember that this was  the update rule for momentum based gradient descent ok and i will write it down again  wt plus one is equal to wt minus gamma into update t minus one minus eta into the gradient at  the current point  so you see that actually i am taking two steps one is this step and then one more step  and i could just this is one way of visualizing right that i move according to the history  and then i move a bit more according to the current gradient so everyone sees that there  is a two step movement happening here  now can you think what could have been done look before you leap so we will see  what we can do    so we know that we are going to move at least by this one and that is fixed we know  that our history is telling us to move at least by this one and then we will move a bit  more by the gradient  so now can you think about it i am at least going to move this much what if i had  some way of looking ahead and then do something at that point this is what you are  saying of course i can verify it but i am sure it will become clear once i show the  equations but i just want you to think about it a bit wait it is very simple it will become  absolutely clear once i show you the answer but just think about it a bit  so here is the answer it why not compute the gradients add this look ahead point right  so you are again adding it in two steps minus the history and then minus the current  gradient so take this value call it the look ahead point i know that i am going to move  by this much so let me not compute the gradients at the current point let me move by  this much then compute the gradients and see what happens at that point    so this is the equation right that first i move by that one step i had to make a two step  movement so i will move by that one step right then i will compute the gradient at that  position not at my current position right this was earlier gradient at point t now i have  already moved a bit so i can compute the gradient there and then move in the direction  of that gradient   so you understood this that there is a two step movement right wt minus history minus  the current gradient gradient computed at time step t ok now you know that you are  already going to move by the history right so why not just move there and then  compute a gradient at that point you are anyways made some movement you compute  the gradient at that point and then decide which is the direction to move in right  so that is what this look ahead value is i know it is still not clear to many of you and i  am very confident it will become clear in the next five minutes we will show you one more  visualization for this but this stay with stay with me for a while as long as you get the  intuition i am fine i will move ahead and then i will explain it again in a different way  this is fine ah that should become clear good that you asked that question ok so ask me  again on the blank slide that i have and then i it should be complete  so for right now let me just show you what will happen with the code and then i will  again explain it with a different way    so this is what momentum based gradient descent it ok now let us see what nested or  accelerated gradient descent will do again the code is simple you can just read it up and  i have started executing you see this blue curve coming over there fine ok and now i  keep running this now what will happen you see that all the u turns of the blue curve  are inside the u turns of the red curve  so the objective is being achieved at least empirically i have showed you that right its  taking shorter u turns what is probably not clear to all of you is why is this happening  is it clear to everyone why is this happening can everyone visualize that ok so let us  see why this is happening i will give you an alternate explanation for this    so suppose this is my error surface right on a two by and i have a single variable with  respect to which i am trying to optimize so this is my w i started off with some initial  value w naught  now what is the gradient at this point positive negative negative right because when i  am going to increase w the function is actually going to decrease right so right so the  slope is negative so where will i move this is the number line right so this line is  actually the number line because it is a single variable so where will i move positive  side of the number line or the negative side of the number positive side the derivative  is negative i am going to move in the direction opposite to the derivative so i am going  to move in the positive direction right so i will end up somewhere here is that clear  fine with everyone  so now i am somewhere here what is the derivative at this point now what is the  derivative here positive negative negative right when i am increasing w my loss  function is decreasing so my dl by dw is going to be negative this is positive this is  negative so again i will move in this direction  so what is happening a lot of negative updates are getting sorry a lot of positive  updates are getting accumulated and now because of my momentum i am not going to  move only by this derivative i am also going to move by the history right so i will end  up somewhere further  so now at this point what is the derivative again negative when i am increasing w the  function is decreasing so what is my update positive or negative positive so now  you see that a lot of positive updates are getting accumulated right my momentum is  building up so now what will happen now if i just move further then again i will get  a let me just put it here right so i am again moving largely in the positive direction  because this guy is also positive all my history was also positive so i have moved in  the positive direction  now what will happen at this point what is the derivative here no it is still its negative  sorry so again i am going to move in the positive side of the number line ok now at  this point i want you break down the movement into two points one is what my history  was telling me which was all these positive updates but of course i will not make such  a large update because i am waiting them exponentially right so but its telling me to  move in the positive direction ok and i know that the gradient at this point is negative  but i want you to ignore that for now i just want you to focus on the history if i just  move according to the history where will i end up i will end up somewhere here right  because the history is very positive so i will keep moving in the positive direction and  this is my w look ahead  now what will happen if i compute the gradient here  student positive  the gradient is  student positive  positive so where will i move  student negative  negative so you see now why momentum works because you are able to look ahead to  this point instead of what should i have actually done is i should have looked at the  gradient at this point the history is positive the gradient is also telling me to move  positive so i would have moved a large positive and i would have ended somewhere  here instead i just moved by the history i checked where i end up i end up here  now let me see whether what is the gradient at this point have i already overshoot my  overshot my objective when would i overshot my objective it has the sign of the  gradient changes right it became from negative to positive and now since its positive  because as i am increasing w the loss is also increasing so now where will i move  negative   so now what is the second step actually its again bringing me close to here so instead  of taking this large u turn i end up taking this small u turn is this clear to everyone  now how many if you still do not get it how many if you get it now good sure ok so  this is what and now we can relate it to what was happening on the figure  so let us go back right so you saw that i was making these smaller u turns because  when i was at this point right i already moved by the history i knew i would land up  somewhere here where i would need to go back right so i already accounted for that  and made a very small movement is this clear everyone gets this how the nesterov of  accelerated gradient descent works sure raise your hands    so looking ahead helps nag in correcting its course quicker than momentum based  gradient descent right so it is already looking ahead where do i land up and already  making a correction if required if not required it will again move in the right direction  right so the update is this guy plus the gradient and my update happens on the original  value not on the look ahead value  so her confusion was perhaps that i am doing w look ahead minus update where this  update again has this quantity you know that is what your confusion was but i am not  doing w look ahead i am using wt there everyone gets this so that is where ah now it  is clear that why the oscillations are smaller in the case of nag and it is able to correcting  its course quicker   
task2/super_cleaned_audios/lesson3.wav,5114.0852,"when this deep revival happened so in two thousand and six a very important study was or a very important contribution was made by hinton and salakhutdinov   sorry if i have not pronounced it properly and they found that a method for training very deep neural network effectively now again the details of these are not important we will be doing that in the course at some point but what is the important take away here is that while from one thousand, nine hundred and eighty-nine to two thousand and six we knew that there is an algorithm for training deep neural networks and they can potentially be used for solving a wide range of problems because that is what the universal approximation theorem said  but the problem was that in practice we were not being able to use it for much  it was not easy to train these networks but now with this technique there was revived interest and hope that now actually can train very deep neural networks for lot of practical problems this sparked off the interest again and then people started looking at all such of thing right that even this particular study which was done in two thousand and six  will actually be very simple to something done way back in nine thousand, one hundred and ninety-three and which again showed that you can train a very deep neural network but again due to several factors may be at that time due to the computational requirements or the data requirements or whatever i am not too sure about that it did not become so popular then but by two thousand and six probably the stage was much better for these kind of networks or techniques to succeed so then it became popular in two thousand and six   then  this  two thousand and six  to  two thousand and nine  people  started  gaining  more  and  more  insights  into  the effectiveness of this discovery made by hinton and others which is unsupervised pre training right that is what i spoke about on the previous slide unsupervised pre training  and they started getting more and more insights into how you can make deep neural networks really work so they came up with various techniques some of which we are going to study in this course so this was about how do you initialise the network better what  is  the  better  optimization  algorithm  to  use  what  is  the  better  regularization algorithm to use and so on so there were many things which were started coming out at this period two thousand and six to two thousand and nine and by two thousand and nine everyone started taking note of this and again deep neural networks of artificial neural networks started becoming popular  that is when people realised that all this all the negative things that were tied to it that you are not able to train it well and so on have slowly  people have started finding solutions to get by those and maybe we should start again focusing on the potential of deep neural networks and see if they can be used for large scale practical application so this two thousand and six to two thousand and nine was again a slow boom period were people were again trying to do a lot of work to popularize  deep neural  networks and get rid of some of the problems which existed in training them  now from two thousand and nine onwards there was this series of success is which kind of caught everyone which made everyone to stand up and take notice  right  that this is really working for a lot of practical applications starting with handwriting recognition  so around two thousand and nine these guys won handwriting recognition competition in arabic and they did way better than the competitor systems using a deep neural network and then this was a success  so this was an handwriting recognition and then there was speech so this shown that various  existing  systems  the  error  rate  of  these  system  could  be  seriously  be significantly reduced by using deep neural networks or plugging in a deep neural network component to existing systems right so this was handwriting and then speech  then again some kind of pattern recognition which was on handwritten digit recognition for mnist this is a very popular data set which had been around since ninety-eight and a new record was set on this data so this is the highest accuracy that was achieved on this data set around that time in two thousand and ten sorry and this is also the time when gpus entered the same so before that all of the stuff was being done on cpus but then people realised that very deep neural networks require a lot of computation and lot of this computation can happen very quickly on gpus as opposed to cpus  so people started using gpus for training and that drastically reduced the training and inference time  so  that was again something which sparked a lot of interest right because even though these were successful they were taking a lot of time to train but now the gpus could even take care of that and this success continued   so  people  started  gaining  or  getting  success  in  other  fields  like  visual  pattern recognition so this was a competition on recognising traffic sign boards and here again a deep neural network did way better than its other competitors   and then the most popular or one thing which made neural networks really popular was this image net challenge which was around since two thousand and eight or two thousand and nine and before two thousand and twelve when this alexnet was one of the participating systems in this competition most of the systems were non neural network based systems and this competition was basically about classifying a given image into one of thousand classes so this could be an image of a bird or a dog or a human or car truck and so on say you have to identify the right class of the main object in the image so in two thousand and twelve this alexnet which was a deep neural network or a convolutional neural network based system was able to actually outperform all the other systems by a margin of sixty-seven percent so the error for this system was sixteen percent and this is a deep neural network because it had eight layers  the next year this was improved further and something known as zf network propose which was again eight layers but it did better than alexnet the next year even a deeper network with nineteen layers was proposed which did significantly better than alexnet then google entered the scene and they proposed something which is twenty-two layers and again reduced the error then microsoft joined in and they proposed something which had one hundred and fifty-two layers and the error that you see here is actually better than what humans do  so even if a human was asked to label this image because of certain law certain noise in the image and so on even a human is bound to make more errors than thirty-six per cent that means even if you show hundred images to humans he or she is bound to may go wrong or more than three or four of these images right there is this system was able to get an error of thirty-six per cent over the large test set  so this two thousand and twelve to 20sixteen period were there was this continuous success on the image net challenge  as  well  as  successes  in  other  fields  like  natural  language  processing handwriting recognition speech and so on so this is the period where now everyone started talking about deep learning and lot of company started investing in it a lot of traditional systems which were not deep neural network based was now started people started converting them to deep neural network based system  so translation system speed systems image classification object detection and so on there were lot of success in all these fields using deep neural networks  and this particular thing that we are talking about which is image net and the success in this was driven by something known as convolutional neural networks  "
task2/super_cleaned_audios/lesson22.wav,2092.2279,"before we move on to the next modulate some small corrections from yesterday’s class  so one was this partial derivative it should have been dou w square so we already  taken one derivative with respect to w and now you are taking another derivative it is the  gradient of the gradient and similarly should this should have been dou b square and  this should have been dou w dou b    the other small thing which i wanted to say was so when i was executing this  algorithm right so i forgot to mention that just notice what is happening is the black  dot that you see the black dots that you see right and which are very close to each other  actually because you are just making small movements those are the changes in the w  comma b values and the red dots are the corresponding loss to that w comma b values  right just to clarify  so that is why you see a movement on the w b plane which is this movement and as you  keep changing that your loss function changes and it becomes better and better right that  means it goes closer to zero    so in this module we are going to talk about the representation power of a multilayer  network of sigmoid neurons right so i am going to compare these two things which are  written in the title so first tell me what was the representation power of a multilayer  network of perceptrons ok i roughly hear what you are saying and basically what you  are telling me is that a multilayer of network of perceptrons with a single hidden layer  can be used to represent any boolean function precisely right no errors that’s what we  saw with that illustrative proof where we actually constructed once its network  now what is the representation power of a multilayer network of sigmoid neurons so  multilayer network of neurons with a single hidden layer can be used to approximate ok  so just see the difference in the language so this was a represent that means exactly  this is approximate that means i will tolerate some error any continuous function  instead of boolean function to any desired precision so this was not this was precisely  with no errors this is up to any arbitrary desired precision  so what does this mean actually what is the meaning of this so there is a guarantee  that for any function ok which takes our original x from r n to r m what is the m that  we have been considering in all our examples one right we just care about one output  but it can be r m also we can always find a neural network with one hidden layer  containing enough neurons so that is the operating trace here enough neurons whose  output g of x so that means you would have a network it would take as input n x it  would produce some y hat and that is what i am calling as g of x right that g of x would  be very close to the true function f of x  so remember that we said that there is this true function f of x which gives us the true  y’s and we are trying to predict this y hat so the true why i am calling by f of x and the  y hat i am calling by g of x and you can come up with a neural network which can give  you values which can predict values which are very close to your true values does that  make sense do you see the value of this theorem what is it trying to tell me tell me  can you can you give me an interpretation of this why is this so useful do you know  what this theorem is called universal approximations here and we did that in the history  right    so this was one thousand, nine hundred and eighty-nine ok what is the significance of this why do we care about such  arbitrary functions and what does this theorem telling us actually it is of course telling  us something about the representation power of a multilayer network of sigmoid neurons  but why is this important so we will see that    so this the remainder of the lecture i have borrowed ideas from this url you should  actually read this it is a very interesting book it is available online for free very  illustrative so please take a look at it ok    so now actually what we are interested in is we are interested in knowing whether a  network of neurons can be used to represent any arbitrary function like the one shown in  the figure ok so let me put some labels on this so they understand what i am trying to  say suppose this is salinity again i go back to my oil mining example and i say that my  decisions are based only on a single variable which is salinity and this is actually how the  amount of oil varies right as the salinity increase it is a very arbitrary function it is  definitely not a linear function it is not even a quadratic function it is not an exponential  function it is just some arbitrary function but a mathematical function this is possible it  is quite likely that salinities has this influence or in oil production or maybe it does not  but i am just taking that as an example  now what do we want the network to learn if i take some data and train the network at  the end of training what do i want so if i feed at this point after training what should  happen it should give me this value right that is what training means and that means i  should be able to approximate this curve if i do that that means i have learned from the  training data so let us see    now we make an observation that such an arbitrary function can actually be  approximated by a lot of something that we call as tower functions these are all single i  mean pulse functions which you have many of these and you could have an  approximation right and you can see that this approximation is bad at many places but  still it is an approximation it largely gives you the same shape as the original curve what  would happen if i increase the number of such tower functions    student refer time zero6two9  the approximation would improve right if i keep increasing it the approximation would  go more and more better right so now just try to keep things in mind whether i write  in the theorem right you can make it arbitrarily close to the actual value that means you  can keep doing something so that your approximation becomes better and better and  you already see something of that sort this is still in the sense of a figure we need to  relate this back to a neural network but you see that as i am increasing these tower  functions i become approx arbitrarily close to the actual function    now this is what is actually happening right i have multiple such tower functions i am  adding them up all of them are shifted in time so this tower function is actually this one  this tower function is actually this one and so on right and i have not drawn the  remaining ones i am taking all of these tower functions adding them up and getting my  original function right and the more such tower functions have the better is the  approximation    now you make a few observations right all these tower functions are actually the same  what is the only difference they just shifted and their magnitude changes right but they  are all tower function right so let us think of this that if i know how to make a rectangle  then i can make any rectangle right i just need to change the size of the rectangle and  maybe shift it or oriented differently or something right so they are all similar i just  need to learn how to draw a tower right that is what my quest is  now if i take the original input salinity pass it through multiple such blocks each block  is capable of making a tower function and each of these would give me one of these  towers that i am looking for and i am looking for so many of these right if i have as  many such tower makers then i could get these towers i could just add them up and then  get the original function back and the more these i have the better is my approximation  right so i am taking as input the salinity and trying to predict the oil does this make  sense still we have not figured out a neural network way of doing this we are still  building intuitions of how to do this  now our job now is to figure out what goes in this black box that is the tower maker and  how does it connect to neural networks if you figure that out then our story is complete  then we know that a neural network can actually do this and that precisely proves the  statement which i had made that it can it can represent arbitrary functions so we will  figure this out over the next few slides    now if you take the logistic function and set w to w to a very high value what will we  get just try to think about it the answer is already written but i want you to imagine it  w covers what  student refer time zero9threeone  the slope right as i make w very high what will happen is i will get the so let us try  changing the value of w ok i just increase the value of w and see what happens to the  sigmoid curve    some error here actually there is some problem the w value should have increased and  that is how the sigmoid slope increases not the b value the b value comes later on so  actually sorry about this the w value as i keep increasing so do not think that b is  increasing think that the w is increasing it will become sharper and sharper and it will  come very close to the step function right it will not become exactly the step function  that will only become in the limit but if i keep increasing i will get very close to the step  function everyone agrees with this  now what happens if i increase the value of b it will shift everyone is confident about  that can you tell me why  student refer time onezerothree4  what will shift actually the point at which the transition happens right so what is this  point actually  student refer time onezero4three  this is the point at which i get that half value right and let us look at our function this is  a function when will i get that half value when w x plus b is  student zero  zero right so that means x equal to minus b by w that is why it is proportional to b so  as i keep increasing the value of b this will keep shifting ok is that fine everyone ok  with this    now what if i take two such modified sigmoid functions which are shifted differently  and both are very close to the step function right so here is where one threshold is here  is where the other threshold is and now i subtract this one function from the other what  will i get  you know the term  you will get a tower right is that fine everyone gets this right so these places up to  this point both are zero so zero minus zero will be zero at this for this small range this is one and this  is zero so that one minus zero and then afterwards both are one so one minus one would be zero so you  get that tower function so now i have my tower maker    now can we come up with a neural network to represent this operation i want a  sigmoid neuron i was working with a sigmoid neuron with some arbitrary weights right  so that i recover that step function can you imagine now given x i want this tower  function and that is exactly what one of the blocks was right so what i am asking you  is oh god so i am asking you to give me a neural network for this can you think of it  can you try imagining it  two neurons in the hidden layer how many of you agree with that ok can you can you  take some more time to imagine what it would be  and i have already ok right    so this w one b one if i set it appropriately i will get this step function if this w two b two i set it  appropriately i will get this step function now i needed to subtract one from the other  right so i will do plus one minus one this is just a simple addition and i will get this is that  fine everyone agrees with this this is just a adder right this is just an aggregator  everyone gets this so now i have given you the tower maker if you put enough of  these tower makers and learn the w’s appropriately what will you get  that function that we were needed so you can approximate it arbitrarily to any  precision that you want as long as you keep increasing the number of these units right  so these units actually give you one tower more of these units that if you have actually  this much this is the input ring the more such tower makers that you have the more is  the bars that you will get and then you can approximate everyone gets the intuition  behind this fine ok this all is always good in one dimension    now what will happen in two dimensions what if we have more than one input what  is the tower there do you do you guys all do all know what is the tower there  if you say no i will give you a zero on the assignment remember the last question of  the assignment did you all make a tower did you all make a twod tower did you all  copy that no so what if we have more than one inputs suppose you had again trying to  take a decision about whether we will find oil at a particular location of the ocean right  and suppose now we base it on two two right so say this is salinity this should be x one  should be x two should be y and this is pressure  now just observe about the red and blue points so the red points are where you will  not for those configurations of salinity comma pressure you will not find oil and the blue  points are for which you will find oil what is the one thing that you can tell me about  the red points and the blue points not linearly separable right but we still want to be  able to learn this is that fine a single perceptron cannot do it i will also make a case for  a single sigmoid neuron cannot do it and then i will show you that in fact first i will  show you that with a network of neurons we can do it and then i will show that with a  signal single sigmoid neuron you cannot not actually do that  so now this is again a valid problem you could have we could imagine that you will get  this kind of data where you have two factors and your function is some arbitrary function  of these two factors it is not a need linear boundary between the blue and red points  everyone sees that the blue and red points are not linearly separable you cannot draw a  plane such that all your red points lie on one side and the blue points lie on the other  side everyone sees that ok but the solution which i have plotted here that is a good  solution it makes sure that all the red points are in this region and the blue points are  outside  so it will predict a high value for these red points and a zero value everywhere for the  blue points is that obvious how many of you understand that figure ok good    so now i want to show that even in twodimensions i could come up with arbitrary i could  come up with a neural network which could actually approximate this and again what  will i look for a tower maker right i just want something which can make towers and  approximate it    so this is what a twodimensional sigmoid looks like slightly incorrect because i have what  i have done is i have actually said w two to zero so if you actually i would want you to do  this go back and plot this for w one equal to three and w two equal to three  just go back and plot this and see what you get you will not get such a smooth such a  nice looking s but you will still get something which looks like looks like a snakes  hood right so in still get that s shaped function it just that it would be bent at some  points and it be thinner at some points and broader at the other points so just go back  and see and then you will realize what is happening right    so here again what we want to figure out is from the single sigmoid i was able to take  you to a tower function right from a twodimensional sigmoid what does a tower mean  here and how do i take you to the tower so that is what i want to do so i have said w  two equal to zero and i will it will become obvious why i have done that so just understand  what the figure is doing right so this if you just look at this is like the cross cut right  so you are looking at the front view of this figure and that is just the sigmoid function  without the w two right and now since i have said w two equal to zero no matter what i set x two  to the same function will get repeated throughout that axis do you get that  so that is why this entire function is just getting repeated throughout this axis and then  you just get a similar s shape function everyone gets that how many of you do not get  that how many of you get that so this if you look at the front view this is the sigmoid  of one variable but since i have said w two to zero no matter what i change x two to the  function is going to remain the same so it will just get copied throughout the x two axis is  that fine with you  now what will happen if i increase w two sorry w one same thing right it will just keep  shifting till it becomes almost like a twod step function ok now what will happen if i  increase b shift i can do the same thing here also same logic applies here also ok    now what is the next step that i am going to do take two of these which are shifted by  some point and then subtract what will i get everyone had this figure in mind so just  see right so this portion both are zero so zero minus zero would be zero this portion this is one but  this is zero so that would be one minus zero and again in this portion both of them are one so one  minus one would be zero so you will get this kind of function would you like to live in such  a tower i am very serious yes or no no why it is open from two sides right you  cannot live in this tower so you want something which is a closed tower right so how  will you do that give me an intuition  we will do the reverse thing what will be set to zero  w one ok    and this is how it would look the orientation would change and again so notice that this  is your sigmoid function and since i have set x one to zero no matter what i change along the  x one axis the same function gets copied and you get a nice looking a sigmoid function  now again i will do the same thing i will increase the w i will get a close to a step  function i will increase the b i will move along this axis    next step take two of these subtract get what another tower function this is also not a  tower that you like to stay in so what do i do now add them sure add this tower to  the other tower    so now what will happen if i add these two will you get a tower function what will  you get  you will get a tower function with a parking floor right is that what you will get  everyone understands why this is so these portions both are zero so you get zero same  logic applies for all the four corners right is that fine now for this portion or rather this  area right so this guy is zero this guy is one so you will get a one the same logic applies for  all these four corners in the centre both are actually one so one plus one would give you two so  this is two this is level two this is level one this is level zero is that fine  so what am i done so far i have taken my x one x two passed it through some  transformations right this what are these transformations we will see but transform it  through these multiple hoops right where i adjusted a w’s and b’s and i have got some z  right and this is how that z behaves for different values of x one comma x two i will get  these different values and these values range from zero to one to two is this pictured clear i  have taken my original x one comma x two passed it to some of these transformations and  irrespective of what my x one to x two is this tells me the entire range of values that i will get  for some combination of x one comma x two i will get zero for some combinations i will get one  for some combinations i will get two and some combinations also between one and two right  so these places where it transitions is that clear is that picture clear to everyone  so now i can treat this as the output z ok now from here how do i go to a tower  threshold it how will you threshold it what do you want you only want this much  part to exist right this without the parking floor how will you do it any output which  is greater than equal to two you want to keep it any output which is less than two you want  to make it zero if i do this will i get a tower right sorry greater than equal to one any value  which is greater than equal to one you want to keep it anything which is less than one you  want to make it zero so this entire thing will get demolished how do you do this this is  an if else ok  if else if something is greater than equal to zero do something else do something else what  is that  perceptron right but we do not want to use perceptron we want to use sigmoid  neurons have you learned an approximation from a sigmoid neuron to a perceptron  very high w right you get the intuition let us see what we do on the next slide    so i take this any z which comes from here i will pass it through a sigmoid neuron  which are very high slope such that the threshold is at one anything which is greater than one  will pass out as one anything which is less than one will go to zero so everyone sees how we  took this structure and converted it to a tower we have this tower now now what do i  do with this    i lead multiple such towers and i can approximate this i could put a tower here here  here and so on i could have these multiple towers and here of course all my towers  would be of zero height right in this region right so now i can cover the entire twod space  with a lot of tower functions and approximate this exactly that is a very weird statement  approximate this exactly i mean approximate this to arbitrary precision everyone gets  this do you see why we constructed these tower functions and now we can put them  inside this cone and approximate it    now all this is fine i was making some towers there so can you now give me a  complete neural network which does this i want you to imagine that remember you are  taking what i am asking you to do is this x one x two give me this such that i get this two dimensional tower i do not know how to draw it something like this maybe whatever  so i want this twodimensional tower what is this network of perceptrons going to look  like just go back to all the operations that we did and try to imagine in your mind  no we will not use perceptron because we can always use a sigmoid neuron instead of a  perceptron with the high w i do not expect you to answer this i just want you to imagine  right we just try with a there is something known as a pen there is something on a  paper ok so here is the solution    so what is happening here you have this salinity and oh actually this is slightly wrong i  do not know why you guys saying it is correct actually at both places i need both the  inputs it is just that in one case i do not care about that input because i have said w two to  zero so i learn these weights wone wtwo b wone wtwo b of course here the network should learn  that w two is equal to zero right and then you get this one tower do not needs this to be  modified this figure is incorrect  so we need x one x two both as inputs we need to label it with w one w two equal to zero and b and  so on it so we will discuss this later anyways but you get the idea right that you take  these two inputs make one tower take the inputs again make another tower add them up  to get this function pass it through this step neuron function step sigmoid function so  that you get the tower so this is one block you will have many such blocks each of  which will learn different w’s and b’s so that they get shifted and then you will place  them all together you have an aggregator on top of this which will combine them just a  minute how many of you get this ok good  yes so that is a good question i am going to come to that right so i have very  conveniently given you a solution where i have what is the bad thing that i have done i  have hand coded these things right i have hand coded w ones w twos and b’s is that  fine in practice no i mean that is where we started off and we do not want to hand code  these right  so now you know a learning algorithm for a single sigmoid neuron now what you  have is a network of neurons right for this network of neurons i need to give you a  learning algorithm driven by the objective function that whatever output it gives would  be very close to that arbitrary function that you are trying to model  if i give you a learning algorithm then you would be convinced that if this has to be  minimized and the weight configuration which need it needs to arrive at as w two is equal  to zero then the algorithm should be able to do that right because we saw we have some  faith in these algorithms in the case of a signal sigmoid neuron that with the right  objective function it will give me a principled way of reaching that objective function in  this big network my objective function is to arbitrarily to approximate this of this true  function right  so now if i give you that as the objective that whatever outputs the network generates  so the network might generate something like this so that has to be very close to the  true output that is the objective function that i am going to use in that learning algorithm  and if that learning algorithm works which will prove then you should be able to arrive at  the necessary weights to make this approximation right is that clear and in fact there  might you might not even have to do these multiple towers in practice all i am trying to  prove is that there is one solution which exists  if there is one solution which exists i can say that locate the network can learn that is the  only claim i make i am not saying this is the only solution right same as in the case of  the boolean functions where i said that one solution exists where you have to raise to n  neurons of the hidden layer that was a sufficient solution that was not a necessary  solution for the and function we were actually able to do it with a single sigma neuron  right so just keep that in mind i am just giving you a sufficient solution  and the network could actually learn something better than this all right this is again a  very bulky solution why it scales with the number of neurones’ proportional to number  of input variables that you have so that is for a sufficient solution but you would want  something better than that all i am trying to say is that it can approximate i am just  telling you the representation power and just as we had the catch there that the hidden  layer is very large the same catch applies here also is this story clear to everyone  so i have given you a solution i have not told you how to learn the weights i have given  you a network now later on we will discuss a learning algorithm for this network and  we will have some confidence that given a particular objective function that learning  algorithm can strive to go to minimum error or minimize the quantity of that objective  function that is going to come in two lectures from now is that fine    and that was for the tower function now i could have actually directly done this right  so i wanted to approximate these functions so i could have placed a lot of these kinds  of things here and approximated it right so that instead of that very high slope sigmoid  function i could just use a normal sigmoid function also ok and again there is a error  here but i hope you get the picture it is just that you feed both the inputs to them    so for one dimensional input we needed two neurons to construct a tower for two dimensional  input how many neurons did we need i am just counting these because these are simple  aggregators right and this is one constant at the end so how many did we need actually  o of two n i mean o of i mean so for n how many would we need let us try to work that  out ok so i will ask you that in the quiz how many do we need for n dimensions    now why do we care about approximating any arbitrary function we will again try to  close the loop now we saw that we can arbitrarily we can approximate any arbitrary  function but now again i want to come back to the point why do we want to do this and  can we tie this back to the classification problem that we were dealing with    and this is the data which i had given you which was there were some points some  values of x and y sorry this should be x one and x two it is where this is pressure and salinity  or salinity tendency and this is the output which is oil  now there was this is what the function actually looks like now what would have  happened if i had used a single sigmoid neuron to try to approximate this function try to  represent this function and sigmoid neuron in two dimensions right so the two dimensional  sigma what would have happened can you give me one solution for this remember  earlier i had said that perceptron cannot handle data which is not linearly separable but  then i anyways used it for data which was not linearly separable and we got some line  such that we got some errors the red points and the blue points are not clearly separated  so i am asking you for a similar thing here i force you to use a sigmoid neuron what  would you give me    is this fine this is one of the possibilities of course it could have been oriented  differently and several things what is happening here is that for these blue points it is  acting correctly but for these red points it is not acting correctly i am assuming red is  positive and blue is negative i think that should have been the other way round but let  us assume red is positive and blue is negative again now for these red points this part is  working fine but it is misclassifying all these blue points  so all these bad locations is actually saying that you can find oil and for all these good  locations here it is saying that you cannot find oil that is what a sigmoid neuron would  do and you could have multiple solutions are possible here right but all of them would  have this problem that will make errors on some red points and some blue points right  but the true solution that we wanted is something like this again there are multiple  solutions possible right you could have anything there are you could have even finer  one side you could just have this much there many things possible this is one such  solution what the illustrative proof told you is that you can actually use a network of  perceptrons and approximate this arbitrary function which exists between the input  variables and the output variable  so if this is the function which exists between the input variables and the output  variables now you could take these multiple two dimensional tower functions and  approximate it with the catch that you might need many of these in the hidden layer but  you can still do that ok so that is why this in theorem important because now any  problem that you take right any problem that you will have in machine learning would  always want you to take an x learn a function of x which takes you to y this function will  be have some the function will have some  parameters right and now what this theorem is saying is that you could adjust these  parameters such that you can arbitrarily come close to the true function right so that is  the significance of this any machine learning problem that you can think of in the sense  of classification or regression you would find that this is useful and i am giving you a  very powerful tool to do that of course with the catch that i am not giving you any  bound on the number of neurons that you will need i am just saying use as many as you  want  "
task2/super_cleaned_audios/lesson109.wav,1837.5391,so with that motivation let us go to the next module where we will talk about long  short term memory and gated recurrent units ok  so now all this was fine in terms of ok i gave you a derivation on the board and say  that this is not required but can i give you a concrete example where rnns also need  to selectively read write and forget right only then you will be convinced that this kind  of morphing is bad in the case of rnns so i will start with that example and then once  we agree that we need selective read write and forget how do we convert this into  some mathematical equations right because conceptually it is fine but you have to  write some equations so that the rnn can do some computations where you have  selective read write and forget right so that is what we are going to do over the rest of  the lecture    so first let me start with the concrete example where you want to predict the sentiment  of a review using an rnn so this is the rnn structure we have done this in the past  that you have a sentence one word at a time is your every time step you will feed this to  the rnn and at the last time step you will make a prediction and as i said the rnn  needs a document from left to right and by the time it reaches the end the information  obtained from the first few words is completely lost right because it is a long document  and you are continuously writing to the same cell state   so you will lose the information that you had gained at the previous time step but  ideally we want to do the following we want to forget the information added by stop  words like a an the these do not contribute to the sentiment of the i can ignore these  words and still figure out the sentiment of the document  i want to selectively read the information added by previous sentiment bearing words  so when i have reach the last time step i should be able to read everything else which  had some sentiments before it and focus on those words just i want to selectively read  from these sentiment bearing words and also i want to selectively write the new  information so i have read the word performance now i want to selectively write it to  the memory whether i should write it completely or should i only write parts of it or not  that is what i need to decide so that is fair this is a typical example where rnn also  when it is dealing with long documents it needs to understand what is the important  information in the document that needs to be retained and then selectively read write and  forget ok  so i am spending a lot of time on this analogy because you need to really understand  that this is important and this is where rnn suffer right if you are using them for very  very long documents if we have document of the size onezerozerozero words which is not comm  which is not uncommon right because wikipedia pages have much more than that per  document so it is going to be very hard to encode the entire document using an rnn  not that it is going to become significantly easier with lstm or grus but to certain  extent it will become easier ok      now the next part is how do we convert this intuition into some mathematical equations  right so let us look at that so in this diagram recall that the blue colored vector is  called the state of the rnn it has a finite size so now i will just call it as s t belongs to  some r n and the state is analogous to the whiteboard and sooner or later it will get  overloaded with information and we need to take care of this right so now our wish list  is selectively read write and forget ok so let us start with that    so what we want to do is that and this is the problem definition now that we have  computed the state of the rnn this is a blue colored vector although it is not blue but  this the blue colored vector from the previous diagram where the state of the rnn was  computed i know what the state is at time step s t minus one now i want from here to here  go from here to here that means from s t minus one i want to compute the new state of  the rnn right so i had something written on the whiteboard i want to write something  new i want to change the state of the whiteboard and this is the new information that has  coming to me right the x t is the new information at time step t  and while doing this i want to make sure that i use selectively write read and forget so  these three operations have to come in somewhere in the between so that i am true or  faithful to the analogy which i have been giving right that is the this is the our problem  definition now going from s t minus one to s t and introducing these three operations along the  way that is what we are interested in doing    i will go one by one we will implement each of these three items right so we will start  with selective write so recall that in rnns this is what happens at every time step t  you take the previous time step previous cell state you take the current input do you  recognize the operation here how many of you recognize the operation here raise  your hands ok so this is nothing but the following operation and as usual i have  ignored the bias ok is that fine so that is what i am representing it as  but now so one way of looking at it is that when i am computing st i am reading or i  am taking the whole of stone so once i have computed stone i am writing this to my  whiteboard and then whole of it would be used to compute the next time step ok but  instead of doing this what i want is i want to read only selective portions of s t minus one  or rather i want to write only selective portions of stone once i have computed s t minus one  i do not want to write the whole of it because then the whole of it will be used to  compute the next cell state i do not want that i just want to selectively write some  portions of it ok  now in the strictest case since i know that s t minus one belongs to r n it is an n  dimensional vector in the strictest case what i could have done is i could have used a  binary decision that of all these n entries i am going to read some entries and ignore the  others so all the other entries i am going to set to zero fine that is the strictest thing that  you could have done now for any of these strictest things what is the soft solution so  for binary what is the soft solution binary zero to one so what is the soft solution for that  between  student refer time zerosixzero5  zero to one so read some fraction of each of these dimensions right so let us try to  understand what i am trying to do here ok so and the third bullet some of these entries  should have gone to zero right ok  so instead of doing this what we want to do is we have this vector which has n entries  this is the cell state at t minus one i do not want to write the entire vector onto the final cell  state what i want to do is i will take some fractions of it is say zerotwo of this zerothree of this zerofour  of these and then write only that do you see the operation that i am trying to do right i  want to take some fractions and write only those to the cell and as i said this is the softer  version of the hard decision which would have been zero for this one for this again zero for this  and so on right    how to do this why to do this all that is not clear i am just telling you the intuition  how and why will become clear later is that fine ok so we want to be able to take stone  and write only selective portions of it or pass only selective portions of it to s t    so whenever we compute st we do not want to write the whole of s t minus one just want  to use selective portions of that so what we do is we introduce something known as a  gate and so this gate is otone ok we take the original cell state stonedo an element wise  product with a gate which is known as the output gate and then write that product to a  new vector which is stoneok so initially this will look confusing but it will become clear  by the end of this lecture ok so is that fine this is what i am trying to do again how to  do this is not clear but this still matches the intuition which i have been trying to build  that i want to write only selective portion of the data which i already have is that fine  ok so each element of otone gets multiplied by the corresponding element of stone and it  decides what fraction is going to be copied and this o t minus one is going to be between  zero to one  but how do i compute o t minus one how does the rnn know what fraction of the cell  state to get to the next state how will it do it we need to learn something whenever  you want to learn something what do we introduce everyone  student refer time zero8twosix   parameter sorry what did you guys say back propagation back propagation will do  what it will work in the air or propagate to what  student refer time zero8three7  whenever you want to do some kind of a learning i want to learn some function what  do i introduce  student refer time zero8fourtwo parameter  parameter right so that is what we are going to do we are now going to introduce a  parametric form for otone right and remember this throughout in machine learning  whenever you want to learn something always introduce a parametric form of that  quantity and then learn the parameters of that function do you get this how many of you  get the statement ok this is what we have been saying day from right from class two or  class three right    always introduce a parametric function for your input and output and learn the  parameters of this function so that is exactly what i am going to do i am going to say  that otone is actually this function i am just giving some time to digest this so this is at  time step t  one so it depends on the input at time step t minus one it also depends on the  output at output means whatever comes out of this right so the same operation what  have happened at time step t two so whatever was the output at that state it will also  depend on this  you just take a while to digest this equation you will see at least six more equations of  this form in this lecture so if you are comfortable with one all of them would be clear  so try to connect the whole story i have stone i do not want to pass it as on pass it on as it  is to st so i am computing some intermediate value where i will only selectively write  some portions of stone and selectively write in the strictest case it should be binary but  that is not what we are interested in we introduce fractions if the fraction has to learn  binary let it learn but we will make it fractional that means we will make it between zero  to one   hence the sigmoid function right remember in one of these lectures we had said that  sigmoids are still used because in rnns and lstms remember in we had said that  sigmoids are bad use stanage refer time onezerothreetwo or use relu but we had ended with  sigmoids are still used in the case of recurrent neural networks and lstms so this is  where they are used ok how many should get that connection ok good fine  so we use sigmoids because we want the fraction to be between zero to one and we also want  some parametrization right and this is the particular form that we have chosen there are  various equations possible various things you could have done here in fact there are onezero  to one5 different variants of lstms i am covering the most popular one which uses the  following equation right so it is says that this is how you will compute the output gate  and that gate will regulate how much of the cell state should be passed on from t minus  one to the next state ok everyone clear with this ok so now if you are clear with this  give me an equation for h t minus one  student refer time oneonetwotwo  loudly everyone s t minus one is that fine right so this is the equation that we will have  so we have done selective writing and these parameters are no special they will be  learned along with the other parameters of the network ok so let us spare some thought  on that you got a certain loss at the output ok earlier you just had these parameters w  u v which were the parameters of rnn which you are adjusting to learn this loss now  in addition you also have the flexibility to adjust these parameters   so that if the lost could improve by selectively writing something then these parameters  should be updated accordingly right may be you are being over aggressive and making o  t minus one to be all ones that means you are passing everything to the next state right  now it has the chance because they have introduce parameters if it helps the overall  loss it better make these fractions more appropriate so that only selective information  is passed to the next state how many you get this intuition so that is why anytime you  introduce parameters you have more flexibility in learning whatever you intend to learn  there is remember one clear difference here right and that is where i said that while i  was giving the analogy i was really setting up things but here there is one distinction  what is the distinction that is there ideally what would i have wanted suppose i take  the example of the review ok and the review was say the movie was long but really  amazing ok now which is the word here which is actually trying to mislead so overall  sentiment is positive right everyone agrees with that but which is the word which is  misleading  student long  long right that means i need to do what with that word  student refer time onethreeoneone  forget that would right now ideally i would have wanted someone telling me retain  retain retain forget retain retain retain i would have a label for each of these words  and then i could have a loss function which tells me whether my gates were actually  athering to this decisions or not so remember my gates are learning some distribution  otone which tells me what fraction to retain and at this particular time step i would have  wanted o t minus one to be all zero’s ok i would have wanted to forget but this kind of not  just o t minus one this will become more clear when i do all the other gates also so what  i am trying to say is that you should have had some supervision which tells you which  information to retain and which information to forget but you do not have this  supervision right no one is telling whether these are the important words these are not  the important words  so that is the difference between the whiteboard analogy there you knew exactly which  step is important and which step is not important here you do not know that all you  know is that you have a final loss function which depends on plus or minus whether the  this prediction is close to positive or close to negative and what is the loss and that loss is  what is being back propagated but the difference now is that you have introduced a  model which can learn to forget some things right earlier you did not have a model  which could learn to write or read or forget selectively now you have introduced a  model this is a better modeling choice right so the same as we have had arguments that  you could do y is equal to w transpose x or you could do y is equal to deep neural  network of x right you are making different modeling choices here and with the hope  that one modeling choice is better than the other choice  so just as rnn was one modeling choice now you are using a different modeling choice  where again with the help of these gates and all you can definitely write a function of y is  a function of the input and that function is going to be lstm function which we will see  in detail so this one part of that function and while doing this you are just making a  better modeling choice which allows you to learn more parameters and along the way if  important do selective write read and forget is that clear right so you would see the  difference what would have been the ideal case and what is it that you have the ideal  case would have been explicit supervision for what to forget read and write you will  never have that but you are still making a modeling choice which allow you to do that  so if it required to model while back propagation should be able to learn these  parameters so get you are able to do that    i know i am repeating myself but it is very important that you understand this situation  how many of you get this now and as i said these parameters will be learned along with  other parameters and o t is called the output gate because it decides what to output to the  next cell state ok still you see that there is a lot of gap here we have not reached st yet  we are still at stone we have computed some intermediate value but we have not reached s  t yet and along the way we had three things selective write read and forget we have only  taking care of selective write so far ok    now let us look at selective read so what this selective read do you are going to get  new information at time step t which is x t right and now instead of this original cell  state you have used the selectively written cell state because that is what you have  written now so that is what you should use  now using a combination of these two i am going to compute some intermediate value  ok and just stare at this equation this equation form is very similar to the rnn  equation form right only thing is that instead of s t minus one i am using h t minus one and  for good reason because i know that h t minus one contains only selectively written values  from htone is that fine and x t is the new input still there is some gap here i have not  reached st yet i am still at an intermediate value so this is the new input which i have  received now what should i do with this new input selectively read this input i do not  want to take all of this input because may be the input which i have got now is a stop  word and i do not want to read all of it right do you get that    so now it captures all the information from the previous state as well as the current input  and we want to selectively read this so now what would you do to selectively read  again the same situation that you have a s the answer is already here you have s tilde  and you do not want to pass it on as it is to st this is s  st is somewhere here which you  do not know how to get to but you know that you do not want to pass on all the input  that you have read you want to selectively pass it on so what will you do now again  introduce a  student gate  gate and this gate will be called  student read gate  input gate or the read gate right ok    so now what can you give me an equation for the gate i t is equal to sigma of that is  good because sigmoid is what we need it is going to be a fractional thing let me add  the easy part w into  student htone  htone that is telling you what has happened so far and u times xt you see the same  equation same form the parameters have changed so these we will call as wi ui and vi  and they are depending on the input as well as the previous state previous temporary  stay that we had computed ok so that is exactly what your input gate is going to be and  now this operation is the selectively reading operation how many you are fine at this  point ok and then this product is going to use to be it is will help us to read selectively  from this temporary value that we have constructed or the input that we have taken ok     so far what do we have we have the following we have the previous state which was s  t minus one then we have an output gate which was o t minus one using these two we have  done selective write right we have taken the previous state and the gate and then a  selective write is that fine ok we need to check if the sigmoid should come here  because sigmoid is already there in the computation of s t minus one right oh it is not  there so this already has one sigmoid right yeah so then again a sigmoid on that is it  there ok we will figure it out just check the equation right   so there may or may not be the sigmoid the sigmoid already applied to s t minus one but  we can figure that out ok so this is the selective write portion then you compute the  current temporary state ok and just look at the similarity between these equations then  you have an input gate and using these two we have done a selective read ok so you have  taken care of selective write and selective read but you are still not reached s t i still do  not have an arrow here i still need to figure out how to compute the st finally ok so  what is the operation which is remaining now selective  student forget  forget ok so what do you think should we forget we want to find new st so let us see  what we will forget right    so the question now is that you had this s t minus one and now you have a temporary state  s t which is here how do we combine these two to get the new cell state ok so the  simplest way would be that you say that st is equal to whatever was there in stone plus  selectively reading from the current input is that fine this is the one way of doing it ok  but now what am i doing here what is the problem here i am reading i am taking stone as  it is right so what should i do i should forget some parts of stone so what should i do  for that introduce a what gate  student forget gate  forget gate right so we may not want to use stone as it is but we are want to forget so  there is at this point all of you should get some confusion if you do not then i would be  worried if you are getting some confusion good right you should all get confused at this  point why are you confused because you already did selective write and now again  you are doing a selective forget also right but there is a difference because the selective  write was then used to compute how to read the information right but now once you  have read the new information you want to see how to assimilate it back with the old  information that you had right so that is why you introduce a separate gate so think of  it as this way that you are keeping these functions separate input output and forget so  they can separately learn things ok  so whatever you want to selective write let it be a separate function these h t minus one  is not going back to s t right let us just by use so that you can compute these temporary  states so that is what is being passed to the next temporary state let i t only decide  how much of this input should be read ok and then when you want to combine these two  just use a separate gate and this exact idea which is confusing all of you why have a  separate write gate and a separate forget gate led to something known as gated  recurrent units where they merged these to gates we will get back to that ok   so at this point it is fine i am just telling you the original equations for lstm and this  was the motivation that they had so as i said there are at least one5 to twozero different variants  of lstm which use different equations they tie some of these weights so one thing  could be that forget is the same as one minus remember right or output could be same as  one minus input right you could have tied these gates instead of learning separate  parameters for that    so in the most parameterized form you have a separate parameter for all of these ok so  we introduce the forget gate again can you tell me the form for this forget gate ft is equal  to first term  student wf  wf second term uf what will be there in the second term xt and the first term ok so  this is what it will look like ok so if you remember one of these equations you will be  able to write all of these not that i am going to ask you to write them in quiz or  something but why take a chance so and then once you have completed the forget gate  instead of this equation can you tell me what is the equation and you are going to use  what is the first term going to be it is stone here what is it going to be now   student ft into  f t into  student stone  stone that fine ok    so now we have the full set of equations for lstm we have certain gates and certain  states what are the gates output gate  student input gate  input gate  student forget gate  forget gate why do you guys has this momentary amnesia like suddenly you forget  everything ok so output gate input gate and forget gate all of these are the same form  with different parameters ok what about the states which are the states that we have  completed one was st the other was ht and the third one was s t ok s t from s t we get st  and from st we compute ht ok    so in the diagram that you see here at the top tell me which are the computations  which are happening at one time step at time step t which are the computations which  are happening is it i will give you the options right is it this or is it this let us call this one  let us call this two or this three or this four which are the computations happening at one time  step and you see the order also here this should be straight forward right why  student refer time two5onesix  how many of you say four that is the one right because you start with selective reading  right and you can just go by this right these are all indexed by t right is that fine ok so  these are the computations which happen at time step t and these are exactly the  computations which were written right so we have the three gates which you need to  compute at every time step and you have the three states which you need to compute at  every time step is that fine and this s t minus one is not being computed it is just taken  from the previous time step is that fine so you have these six computations which  happened at every time step and the output final output of an lstm so when you use  tensor flow or something the output of an lstm would give you two things it will give  you ht st because these both the states that are being computed one is the running state  and another one is the current state which is being computed ok  and i choose the notation s because that is what we have been using for rnns but in  lstm in all the literature instead of s you will find it to be ct because it is called the  cell state so that is why st ok so all these equations wherever you see an s when you  are reading some standard blogs or things like that you will see c instead of s so you  just do this mapping in your head ok    so lstm actually has many variants with include different number of gates and also  different arrangement of the gates so as i was saying that you could say that input is one  minus output or input is one minus forget or things like that and also why this particular  parametric form right why not make w zero into stone instead of htone and so on so the all  points of things that you could do or all of these are valid these are all valid variants of  lstms  so there is this paper called lstm a search space odyssey so you can go and look at i  think we link it in the in the reading material right ah so you can see that there are  actually many variants of lstms but this is the most standard and default variant  which you will find in most platforms on tensor flow or pytorch refer time two7twosix  form    and there is another very popular variant of lstms which is called gated recurrent  units so we will just see gated recurrent unit so i will just give you the full set of  equations for grus so you have gates but unlike lstms you have only two gates you  have an output gate and you have an input gate you do not have the forget gate ok  so what am i going to do for the forget gate so this is what i am going to do you see  the last equation so instead of forget gate i am just saying that this is what you are  going to selectively input from the current temporary state so the rest of you rest of it  you take from the previous state right so i have just tied the input gate and the forget  gate any other changes do you see in this so earlier we had s t minus one everywhere  right now we have s t minus one itself is that fine ok so the basic idea these equations are  many and you could think of your own equations you could say that i will not really use  this input information at all or i will choose to use it differently or what not right there  are several things that you could do at a very abstract level this is what you need to  what is this   student refer time two8three8  so these parameters could then make a difference right they could adjust it accordingly  and so on right so that is what i was coming so the there are various ways of realizing  this right at the abstract level you need to understand that the original problem was  trying to store all the information from time step one to time step t capital t right which is  not feasible because of this finite size that you have so along the way we built this  intuition that it should be good to have these operations which allow you to selectively  read write and forget right how do you mathematically realize these operations there  are various choices for doing that and we saw a few choices for doing that right there are  many others you could have done but this is largely what whenever you say that i have  used an lstm most likely you are using the set of equations which i saw which we saw  on the previous slide and whenever you are using a gru these are the set of equations  that you will be using ok  and again remember this that there is no explicit supervision here it is just that we have  a better modeling choice we are just introduce more parameters so that if required these  parameters could be adjusted to do a selectively read write and forget right so it is  often it is often valuable if you are doing some task with rnns or lstms you should  visualize these gates right you should see that at time step t if you thought that it should  have forgotten everything that it has learnt so far because suppose you had this the  movie was long but i really loved it because the direction was superb and so on  now this word but actually changes everything right because it whatever was written  before it does not matter anymore right so is it really learning those kind of gates  where everything before but was forgotten right so it would be helpful to visualize  these output gates and see what kind of values they are learning what kind of things  they are remembering forgetting and selectively reading and so on right so as i said i  will just again summarize the key thing here is the intuition and then the realization in  the form of equations there are multiple choices we have seen a few of those right that  is what i will end with and in particular in grus there is no explicit forget gate and  instead of htone you use stone everywhere   
task2/super_cleaned_audios/lesson108.wav,515.8628,so we have been looking at a new kind of or a different kind of neural network which is  recurrent neural network and last class we spent some time on showing that it is how to  train these recurrent neural networks because of the specific problem of exploding and  vanishing gradients in particular we saw that if you want to propose if you want to back  propagate the gradient from time step t the final time step to an arbitrary time step k then  you have this multiplicative term in the back propagation which could explode or vanish  so today we are going to try to see if we are trying to focus on something which can  help us solve this problem to whatever extent right so that is why we will look at  lstms a long short term memory cells and gated recurrent units ok so let us start  with that   so first we will introduce the idea of selective read selective write and selective forget  and then we will try to build on this intuition and see how could you could realise it by  using lstms and gated recurrent units and whether that help in solving the vanishing  and exploding gradient problem    so recording the state of an rnn records information from all the previous time steps  that was the whole idea that you add this recurrent connections so as you keep going on  at this time step the cell is not only recording information from the current time step but  it also has some kind of an accumulated history from all the previous time steps right  but now the issue is that this state or this blue coloured vector that you see is going to be  of some finite size right we will say that it is a one hundred dimensional vector or a one hundred0  dimensional vector but whatever be the size it is going to be some finite dimension  now as you keep writing information to that cell you are morphing the information that  you had written at the earlier time steps right do you get that so now that is the  problem right because on one hand you are saying that you want to record the  information from all the previous time steps but the at the same you at the other hand  you just have a finite amount of memory to deal with so it is bound to read over ridden  and the information will get morphed so much that it is completely impossible to say  what was the original contribution at time step one or time step two once you have reach  some time step two0 thirty or so on right so that is the problem with recurrent neural  networks and we will tie it back to the problem that we had with the vanishing and  exploding gradients right    so in fact the similar problem occurs when you try to when the information flows  backwards during back propagation right it is very hard to assign the responsibility of  the error caused at time step t to arbitrary time steps before it right to very far away time  steps it is very hard and that is the vanishing gradient problem right because we have  this multiplicative term and the gradients vanish so that that is very hard to do so both  during forward propagation the information vanishes and even during backward  propagation the information vanishes right and we saw a formal argument of this while  doing vanishing gradients so this is just an illustrative diagram but we also saw that  formally the guardians do vanish under certain conditions right    now let us see an analogy for this and from here on we will build on some ideas which  will help us arrive at lstms and gated recurrent units right so the analogy is a  whiteboard so you have a whiteboard and it is always of a fixed size right i mean the  whiteboard is not infinite you just have some size for the whiteboard and you keep  writing information on that so at every time steps i keep going and writing something  on the board and i am trying to derive something or just try to make a story or anything  right i just keep trying to write information on the board  now since the whiteboard is fixed size at every time i am essentially morphing the  information which was written at the previous time step and after many time steps it  would be impossible to find out that now whatever my state of the board is how did time  step one contribute to that state right because it is written all over the board i do not  know where i started what i did and so on and its going to be very hard for me to find  and this happens right when you do these long derivations on on the whiteboard it  becomes very hard to track where did i start where was this variable defined and so on  right because it is a fixed size you cannot really i will end of deleting some terms and so  on right    so and we will make this more concrete with the help of an example right suppose i  am trying to drive an expression on the board and typically what we do is we use three  things we use selectively write on the board selectively read the already written content  and selectively forget or erase some content so let us see what i mean by these three ideas  ok    so first we look at selective write so this is my problem this is the derivation that i  want to do on the board and i have a very small whiteboard which allows me to write  only three steps ok that is the situation in which i am operating ok so i am given the  values of a b c d and i want to compute this expression ac bd  a  ad ok now the  first thing that i am going to compute is ac right that make sense because that is the first  term that i need so i will write ac equal to five and the second thing i will write is bd   threethree ok so now why did not i do the following i could have done this a equal to one c  equal to five then ac  five then b is equal to something then b into d is equal to something   so what am i doing here while writing on the board why did i write only two steps  because i know i have a finite size for the whiteboard right so i am only trying to  write information which is important right i am not writing everything because i know  that i am going to run out of memory right so that is why i did not write these  intermediate steps that a  to one c  five and then a c  five and so on right i juts wrote the  results which are important so i am selectively writing to the board because i am  dealing with a finite size memory and this is exactly what we do while writing in a note  book or a whiteboard or anywhere right we just do not write everything one because  you are lazy but second is also because we do not have enough space right so that is  about selectively writing    now selectively reading i already have something stored on the whiteboard so this is  the state of the whiteboard at this point now at the next time step i need to read some  information but i do not need to read everything that is on the whiteboard i just need to  read some information what is the information that i need to read for the next time step  bd i do not need to read ac right so i am just doing a selective read of the information  or the state which is already stored on the whiteboard  now what is happed is i have exhausted my space where i had just three steps that could be  written on the whiteboard and i have done that now what do i do for the next thing  now i need to compute ac bd  a i will have to selectively erase so what will i erase  bd right    so now as the whiteboard is full and i will have to selectively delete some information  and as this obvious in this trivial example that you can get rid of bd because you have  already encoded the information in bd  a and now the next step which is ac into bd  plus a can do on the whiteboard and this is how you keep doing at every time step you  selectively write at every stage   here again note that i am not written acbd  a would be something like i do not know  what was it five x three4  one70 right i am not using two steps i am just writing everything in a  single step because i do not have enough space right so selectively writing selectively  reading and selectively forgetting things which are there in a constant memory is  something that we do regular right    and then other ways of motivating this so you could even think of our brain as  something which can store only a finite number of facts right as we keep going or if we  will learning more and more things we can only retain a finite number of facts  and what happens inadvertently is that you erase some of the steps not consciously of  course you do not have a delete button or anything but you erase some of these things  that you forget a lot of things which had happened a year back or so and also at various  times if i ask you what was this which i have done in last class most of you forget to do  the selective read but that is what you do right you always do selective forget but that is  what we typically do in our brain also right any time when you are dealing the finite  size memory you will always have this three operations either they are explicit or implicit  but the intuition is that you end up doing this ok  so now since the rnn also has a finite state size can we do something like this  selective read write and forget so that one during forward pass even if the information  gets morphed it gets morphed in a principle manner right so even in the whiteboard  example i was morphing the information i was deleting the information written at time  step one time step two but i was being a bit smart about that i was retaining some good  information and only deleting what was not required so can we do this in analogy so  this analogy really sets it up but now the solution is not going to live up to the  expectation but it will have some it will be something in this direction ok   
task2/super_cleaned_audios/lesson23.wav,1083.621,so welcome to lecture four of cs7zeroone5 the course on deep learning today we will talk  about feed forward neural networks and back propagation so quick recap of the story  so far it so we started with mp neurons we saw there were some problems with the mp  neurons they could handle only boolean inputs and boolean outputs and threshold  needed to be hard coded so from there we moved on to perceptrons which allowed for  real inputs real outputs and sorry real inputs and binary outputs and we also learned an  algorithm for learning these weights and parameters right so we need there was no  need to hand code these parameters anymore  but then we found that for a single perceptron there is a limitation it cannot it can  only deal with functions which are linearly separable so then we went on to a multi layer network of perceptrons and we proved by illustration that it can handle any  arbitrary boolean function whether linearly separable or not the catch is that you will  need a large number of neurons in the hidden layer right then we also observed that  perceptrons have this harsh thresholding logic so which makes the decisions very  unnatural it is zerofour9 it is negative fifty-one is positive so you wanted something more  smooth  so the smoothest approximation to this step function which is the perceptron function  was a sigmoid function sigmoid is a family of functions and we saw one such function  which was logistic function and then we saw that it is very smooth now it is  continuous and differentiable  now for the sigmoid neuron on a single sigmoid neuron we saw a learning algorithm  which was gradient descent and we proved principally that it will always go in the  direction where the loss decreases right so that is what is the basis for gradient descent  and then we graduated from a single neuron to a network of neurons and made a case  that such a network of neurons with enough neurons in the hidden layer can approximate  any arbitrary function right ok so i have told you that it can approximate any arbitrary  function what does that mean and what is the thing in the network that does all this  all the tower functions and the tower functions depend on weights and biases so there  in that illustrative proof again we were adjusting the weights and biases by hand right  we knew that we wanted these very tiny tower functions and we were doing it  now from there where should we go   student refer time two hundred and thirty-nine  we need an algorithm to learn these weights and biases right so that is what back  propagation is so today i am going to formalize these feed forward neural networks  we just did it by illustration the other day i will introduce you to the terminology and  see what the input outputs are and so on and then we will look at an algorithm for  learning the weights in this feed forward neural network    let us begin so this a lot of this material is inspired by the video lectures by hugo  larochelle on back propagation he has a course on neural networks  it is available on youtube you can check it ok so let us first begin by introducing  feed forward neural network right    so what is a feed forward neural network the input to the network is an ndimensional  vector so ok that means my input belongs to rn that fine the network contains l  minus one hidden layers where do you already know what hidden layers are right we  have been defining that terminology since multi layered perceptron so you have these  hidden layers and there are l minus one of these and then it has one output layer containing  k neurons ok those are the feed forward neural network looks like what is missing  here   student refer time three hundred and fifty-seven  the weights right  so each neuron in the hidden layer ok before that each neuron in the hidden layer and  the output layer can be split into two parts right so i will call the first part as the pre  activation and the second part as the activation have you seen this plate before right  what does the pre activation do   student aggregation  aggregation and what does the activation do   student nonlinearity  nonlinearity right so we have this pre activation and activation at every layer and a i  and h i are vectors is that correct because this entire thing or rather this part is h one and  this part is a one both of these are vectors right and for this discussion am going to  assume that everything till here belongs to r n  so the input was r n and all the hidden layers also have n neutrons is that fine so  please pay a lot of attention to this couple of slides because this is going to stay with us  for the rest of the lecture and perhaps two more lectures and even for the course alright so  this is very important that you understand this the way we are defining a feed forward  neuron network    the input layer can be called as zeroth layer what i mean by that is that i could refer to  this as h zero ok there is no a zero h zero here because there is no pre activation activation  you are just given the input so i just call it as h zero ok and the last layer can be called as  h of l right whatever you get from this green part you will call it as h of l ok what  is the dimension of h of l r raised to k it belongs to r k because i have said here that  you have k neurons each corresponding to k classes ok  now we have weights between the input layer and the first hidden layer now can you  tell me this belongs to r n this also belongs to r n so what is the dimension of w one n  cross n right because it contains weights for connecting each of these inputs to each of  these hidden layers there are n here n there right so it is n cross n  and what are the dimensions of the bias n one corresponding to each of the hidden  inputs fine and this is only for up to this layer because till here i have assumed  everything is n    now what about the output layer n cross k and the biases k k dimensional ok so this  is what the network looks like but now i have to give you some function so i have  just i have shown you a diagram but what does it mean mathematically because  remember that we are always interested in writing something of the form y is equal to  function of x right and that is not well defined yet    so let us start defining that ignore the red portion for now ok i will go over it so each  of these activations right or rather the pre activations is given by b i plus w i into h i  minus one so what it means is that these activations take inputs from the previous layer  multiply by them by weights and also add the bias is that clear so let us see it right  for example if i look at a one which is this vector so that is three dimensional and assuming  it is three dimensional for simplicity  so it is a one one a one one a one two a one three right and that is equal to how do you get rid off this b  one one b one two b one three plus this matrix multiplication is this clear to everyone i know it is trivial  but am still going over it right so let us not ok and then how do you do this matrix  multiplication row was multiplied by the column so this is what you get right and in  the end i can write it as this right and this looks very similar to what we have been  seeing throughout it from a mp neuron to perceptron to sigmoid neuron and now this  case right  so it is just an aggregation of all your inputs or weighted aggregation of all your inputs  that is the case which i want it to know and that is obvious now so you understand  what these are right  so this is r n in our case we have assumed n equal to three what is this i will keep  asking till this is completely fine with everyone r n and this is  student refer time zero8threezero   n cross n and this is  student refer time zero8threefour   n cross one n cross n i mean r n sorry is it fine so everyone understands the operation  happening here it is a weighted aggregation of your inputs so every guy here is a  weighted aggregation of all the inputs ok    now after that i do h i of x is some function of a i of x ok what does this mean so  this is again a vector right i have assumed that it is three dimensional so these are the three  elements of h i so these are the three guys now these are some function of these light blue  guys ok now how does that function operate on the vector it operates element wise  not all functions on vectors are element wise but this particular function we are going  to do element wise  that means that h one one is equal to g of a one one h one two is equal to g of a two and h one three is equal to  g of a one three right where if i take g of a one three one of the functions that i could choose is the  sigmoid function so it would just be one over one plus e raised to minus here so what is  happening is i am taking this value and passing it to the sigmoid function to get oh sorry  am taking this value and passing it to the sigmoid function to get h one one taking this value  passing it to the sigmoid function to get h one two right  so the key thing to understand here that this is a element wise operation right it is not  operating on the vector that does not make sense it is operating on every element of the  vector right ok and g is called the activation function    it could be logistic tanh linear anything right so we will see some of these functions  later on ok  now the activation at layer i sorry they are supposed to be activation at the output layer  the activation at the output layer is given by the final function which is f of x is equal to  o of a of so let us see so this is a three in our case l was equal to three because we had l  minus one hidden layers and the lth layer was the output layer right so this is a l so this  is what i have computed here that light green part of the figure that you see right now  based on that i want to produce an output  so that is someone had asked me a question that why do we always choose sigmoid  because sigmoid will clamp the output to zero to one what if i want to predict the amount of  oil which will not be between zero to one right that is why for the output we will use a  special function that will call the output function and later on i will show you that it  depends on the task at hand so it is going to change with the task that we are going to  do right so we are just going to say that the final output which is h of l is equal to  some function of the pre activation at that layer is this terminology clear to everyone  how is each function operating is that clear to everyone    and we will see some examples of the output activation function right now just for  simplicity am going to remove the x’s from the brackets right so instead of calling  everything ai of x hi minus of x and so on i will just call them ai hi and so and so that  just simplifies things but we know that everything is a function of x because x is the  input and that passes through some functions and we get the final output right so this is  the notations that we are going to use is the dimension of everything that you see every  variable that you see here completely clear to everyone  dimension of ai bi w hi x everything is clear ok and the output layer has a slightly  different dimension than the other layers because there we have k classes as opposed to  n neurons everywhere else ok fine now i need to put this in the paradigm that we saw  for supervised machine learning what were the five components there data  student model  model  student parameters  parameters  student learning  learning algorithm  student refer time onetwotwo9   objective function right ok everyone remembers that ok  so i said that we will do deep neural networks and we are trying to write this y hat as a  function of x but then what i gave you is just a diagram from which this is not clear  whether y hat is actually a function of x how many of you think y hat is actually a  function of x very few ok    so let us see what exactly is our model assumption here right so the question let me  repeat the question just to be clear so i said that they are given some data we do not  know the true relation between y and x we make an assumption that y is related to x  using some function f right and it is has some parameters and then we like to try to learn  the parameters of that function so what is the function here   so what is your model what have you assumed as the model can you write y as a  function of x if yes what is that function how many of you have the answer i think  you have your answer ok i think i cannot wait more so i will give you the answer  then it will become very obvious ok so this is how y is a function of x right so let us  see what is happening i took the original x which was this i transformed it added b one  that was the dash at layer one   student refer time onethree56  no this thing  student refer time onefourzerozero  preactivation at layer one i passed it through the activation function right ok  now again let us be clear about the dimensions what is the dimension of this   student n  n what is the dimension of this n cross n so what is the dimension of this product   student refer time onefourtwoone  n what about this so what is the product the final dimension of this  r n now you are passing it through a function g that function is operating element wise  so what is the output dimension  student r n   r n so this is again r n ok now this  student refer time onefourfourtwo   so now you see the whole story right so now this n cross n guy multiplies with this n  guy again you get a vector again pass it through a nonlinearity was it so hard it is  obvious now right you just take an x just note down all the transformations that you  have done that is what a function does right it passes it through the through first a  linear transformation this is a linear transformation then a nonlinear transformation  then again linear nonlinear and so on  so just see how far we have come from where we started off right we started off with  simple things like w transpose x right that was the perceptron model where we were  taking decisions based on w transpose x and we were saying y is equal to one if this  quantity is greater than something y is equal to zero if this quantity is greater than  something right that is why we started off with we made it slightly more complicated by  doing this this was sigmoid neuron  now from there where have we gone to this right so we have increased the  complexity of the network with great complicity complexity comes great  student refer time one5four6   no power right we have already seen the representation power of deep neural  networks right so it comes from this complexity that you have you have a lot of linear  and nonlinear transformations right that adds to the complexity of the network it has  more parameters at each linear transformation you have some parameters and you are  also using a lot of nonlinearity so that is the reason why deep neural networks are so  powerful right do you get that ok so just to impress again right  so any machine learning algorithm that you have you should be able to write it in this  form right that y is a function of x with some parameters and then your job boils down  to learning these parameters right it just happens that here y is a very complex function  of the inputs is that clear ok so i am not deviated from the original story i am still  being able to write y as a function of x with some parameters ok what are the  parameters   student refer time one6fourtwo  all the w’s all the b’s right so w one to w l and b one to b l    and the algorithm that we are going to see today for learning these parameters is called  gradient descent but we will use it with back propagation where back propagation will  help us to compute gradients it is ok it does not it does not make sense at this point  that is what the lecture is supposed to be about right so and what is an objective  function   student refer time one7zero7  loss function so i could just go with this loss function right ok there is an error here  i thought we corrected this there is a summation so actually these are vectors right  so this does not make sense so you should have summation j equal to one to k yij minus  yij does that make sense so this is the vector y hat ok for the i th example it will be  called as y hat high i which will have k elements right so y hat i one y hat i two up to y hat i  k right  so that is what my predictions are and i will have the corresponding true vector also i  am trying to take the difference between them which is going to be an element wise  difference everyone understands the error in the slide how many of you do not get it  how many of you get it if you do not get it please raise your hands it is a minor thing  i can correct it and how does deep neural networks fit into these this paradigm  
task2/super_cleaned_audios/lesson2.wav,2644.0044,"we will start talking about artificial intelligence and this is titled as from the spring to the winter of ai so i am going to talk about when was this boom in ai started or when is that people started thinking and talking about ai seriously and what eventually happened to the initial boom and so so  let  us start  with  one thousand, nine hundred and forty-three  whereas  i  saying  that  there  was  a  lot  of  interest  in understanding how does a human brain work and then come up with a computational or  a  mathematical  model  of  that  so  mcculloch  and  pitts  one  of  them  was  a neuroscientist and the other one was a logician no computer scientists or anything at that point of time  and they came up with this extremely simplified model that just as a brain takes a input from lot of factors so now suppose you want to decide whether you want to go out for a movie or not so you would probably think about do you really have any exams coming up that could be our factor xone you could think about is a weather good to go out is it deep learning raining would it be difficult to go out at this point would there be a lot of traffic is it a very popular movie and hence tickets may not be available and so on  so being kind of presses all this information you might also look at things like the reviews of the movie or the imdb rating of the movie and so on and based on all these complex inputs it applies some function and then takes a decision yes or no that i want to probably go for a movie  so this is an overly simplified model of how the brain works is and what this model says is that you take inputs from various sources and based on that you come up with the binary decision right so this is what they proposed in one thousand, nine hundred and forty-three so now we have come to an artificial neuron so this is not a biological neuron this is how you would implement it as a machine right so that was in one thousand, nine hundred and forty-three  then along and then this kind of led to a lot of boom in our interest in artificial intelligence and so on and i guess around one thousand, nine hundred and fifty-six in a conference the term artificial intelligence  was  a formally  coined  and  within  a  one or  two years  from  there  frank rosenberg  came  up  with  this  perceptron  model  of  doing  computations  or  what perceptron model of what an artificial neuron could be and we will talk about this in detail later on the course and not tell you what these things are as of now just think of the a new model was proposed and this is what he had to say about this model right so he said that the perceptron may eventually be able to learn make decisions and translate languages do you find anything odd about this statement yeah so learn and make decisions make sense but why translate languages why is so specific why such a specific interest in languages so that you have to connect back to history so this is also the period of the cold war and there was always always a lot of interest there was lot of research and translation was actually fuelled by the world war and evens that happened after that where these countries which were at loggerheads with each other  they wanted to understand what the other country is doing but they did not speak each other’s language that is why there was a lot of interest from espionage point of view or from spying and so on to be able to translate languages and hence that specific require and lot of this research would have been funded from agencies which are interested in these things right and the defence or war or something  so and this work was largely done for the navy and this is an this is an extract from the article written in new york times way back in one957 or fifty-eight where it was mentioned that the embryo often this perceptron is an embryo of an electronic computer that the navy expects will be able to walk talk see write reproduce itself and be conscious of it is existence  so i am not quoting something from two0one7 or oneeight this is way back in one957 fifty-eight why i am that is why i like the history part of it so recently there is a lot of boom or a lot of hype around ai that ai will take over a lot of things will take our jobs it might eventually we might be colonized by ai agents and so on  so i just want to emphasize that i do not know whether that will happen or not but this is not something new we have been talking about the promise of ai as far back since one957 one9fifty-eight right this not something new that people are talking about now it is always been there and to what extent this promise will be fulfilled is yet to be seen  and of course as compared to one957fifty-eight we have made a lot of progress in other fields which have enabled ai to be much more successful than it was earlier for example we have much better compute power now we have lots of data now and all thanks to the internet and other things that you can actually crawl tons and tons of data and then try to learn something from a data or try to make the machine learn something from it so we have made a lot of progress in other aspects where which ai is now at a position where it can really make a difference but just wanted to say that these are not things which i have not been said in the past it has always been the it has always been considered to be very promising and perhaps a bit hyped also so that is about one957fifty-eight  then now what we talk about what is all the for the past eight to one0 years at least when we talk about ai talking about deep learning and that is what this course  is about largely about deep learning i am not saying that other and what deep learning is largely about if i want to tell you in a very layman nutshell term is it is about a large number of artificial neurons connected to each other in layers and functioning towards achieving certain goal so this is like a schematic of what a deep neural network or a feed forward neural network would look like now this is again not something new which is up in the last eight to one0 years although people have started discussing it a lot in the last eight to one0 years look at it way back in  one9656eight opposed something which looked very much like a modern deep neural network or a modern feed forward neural network  and in many circles he is considered to be one of the founding fathers of modern deep learning about the springtime for ai and what i mean by that that everyone was showing interest in that the government was funding a lot of research in ai and people really various applications health care defence and so on  and then around one969 an interesting paper came out by these two gentlemen minsky and papert which essentially outlined some limitations of the perceptron model and we will talk about these limitations later on in the course in the second or third lecture but for now i will not get into a details of that but what it is said that it is possible that a perceptron cannot handle some very simple functions also  so you are trying to make the perceptron learn some very complex functions because the way we decide how to watch a movie is a very complex function of the inputs that we considered but even a simple function like xor or is something which a perceptron cannot be used to model that is what this paper essentially showed and this led to severe criticism for ai and then people started losing interest in ai and lot of government funding actually subsided after one969  all the way to one9eight6 actually this was the ai winter of connectionism so there was very little interest in connectionist ai so there are two types of ai one is symbolic ai and the other is connectionist ai so whatever we are going to study in this course about neural networks and all that probably falls in connectionist ai paradigm and there was no interest in this and people i mean hard to get funding and so on for these one7 to oneeight years  and that was largely triggered by this study that was done by minsky and papert and interestingly they were also often misquoted and what they had actually said in that papers so they had said a single perceptron cannot do it they in fact said that a multi layer network of perceptrons can do it but no one focused on the second part that a multilayer network of perceptron people started pushing the idea that a perceptron cannot do it and hence we should not be investigating it and so on right so that is what happened for a long time and this known as the winter the first winter  then around one9eight6 actually came this algorithm which is known as back propagation again this is an algorithm which we are going to cover in a lot of detail in the course in the 4th or 5th lecture and this algorithm actually enables to train a deep neural network right so deep network of neurons is something that you can train using this algorithm  now this algorithm was actually popularized by at rumelhart and others in one9eight6 but it is not completely discovered by them this was also around in various other fields so it was there in i think in systems analysis or something like that it was being used for other purposes in a different context and so on and rumelhart other and others in one9eight6 were the first to kind of popularize it in the context of deep neural networks  and this was a very important discovery because even today all the neural network so most of them are trained using back propagation right and of course there have been several other advances but the core remains the same that you use back propagation to train a deep neural network right so something this was discovered almost thirty years back is still primarily used for training deep neural networks that is why this was a very important paper or breakthrough at that time  and around the same time  so  again interestingly so back propagation is used in conjunction with something known as gradient descent which was again discovered way back in oneeight47 by cauchy and he was interested in using this to compute the orbit of heavenly bodies that is something that people care about at that time today of course we use it for various other purposes one of them being discovering cats and videos or even for medical imaging or for describing whether certain have of cancer is being depicted in a xray or things like that there is a lot of other purposes for which deep neural networks enhance and hence back propagation gradient descent and other things are being used for it but again these are not very modern discoveries these are dated way back thirty years and even gradient descent is almost one50 years and so on so that is what i wanted to emphasize  and around the same time in one9ninety or one9eight9 there is this another interesting theorem which was proved which is known as the universal approximation theorem and this is again something that we will cover in the course in the third lecture or something like where we will talk about the power of a deep neural network so again the importance of this or why this theorem was important will become clear later and when we cover it in detail but for now it is important to understand that what this theorem said is that if you have a deep neural network you could basically model all types of functions continuous functions to any desired precision  so what it means in very layman terms is that if the way you make decisions using a bunch of inputs is a very very complex function of the input then you can have a neural network which will be able to learn this function right in many laymen terms that is what it means  and if i have to hype it up a bit or i have to say it in a very enthused and excited manner i would say that basically it says that deed neural networks can be used for solving all kinds of machine learning problems and that is roughly what it says but with a pinch of salt and a lot of caveats but that is what it means at least in the context of this course so this is all around one9eight9 and despite this happening some important discoveries towards the late end of eight0’s which was back propagation universal approximation theorem people were still not being able to use deep neural networks for really solving large practical problems and a few challenges there was of course the compute power at that time was not at a level where it could support deep neural networks  we do not have enough data for training deep neural networks and also in terms of techniques while back propagation is a sound technique it is to fail when you have really deep neural network so when people try it training a very deep neural network they found that the training does not really converge the system does not really learn anything and so on and there were certain issues with using back propagation off the shelf at that time because of which it was not very successful so again despite these slight boom around eight6 to ninety where some important discoveries were made and even follow up in 9two ninety-three and so on there is still not a real big hype around deep neural networks or artificial neural networks and at time again a slump a slow winter right up till two006  "
task2/super_cleaned_audios/lesson37.wav,809.6227,now we look at stochastic and mini batch versions of these algorithms  so we will digress a bit actually we should have ended up somewhere else but i was  just going to digress a bit    so this is the original gradient descent code that we had and i have highlighted  something in this red box  so notice that the algorithm actually goes over the entire data once before making an  update it has going over this entire for loop which is over all the data points of course  in this toy example i had only two data points but in i practice i will have many many  data points i go over all the data points compute the derivatives and then make this one  update  student refer time fifty-seven  because that is the right thing to do ok this was the exact formula that we painfully  derived right that the gradient with respect to the loss function right which we had the  summation i equal to one to n remember and the true derivative was a sum of the  derivatives with respect to all the data points that is what we analytically derived and  hence we are doing that it was that is the right thing to do not for any other purpose ok  that is what it should always be right so that is the right thing to do because this is a  true gradient and we actually derived it    and hence this was not an approximation so all the theoretical guarantees hold if i do  this i know that now this is the true gradient or the true derivative and if i move in the  direction opposite to the gradient everything falls in place because i proved it using  taylor series   but what is the flip side of this this is the right thing to do but what is the flip side if  you have millions of point we will go over all these million points and make this one  update now imagine the consequence when you are in a plateau region right even that  momentum or whatever your movement in the plateau is going to be relatively smaller  right you are going over these million points and making that tiny delta update right  so imagine how much time it will take your algorithm to converge you get the problem  so the algorithm will take a million calculations and then make one tiny update to your  w ok this is going to be very slow can we do something better always right so let us  take a look at stochastic gradient descent fine    so i have done a very subtle change to the code what is it do not tell me indentation but  that is what i have done so you can tell me that so what is happening now for every  data point i am making an update to my w values  now the algorithm updates the parameters for every single data point if you have a  million data points how many updates will be make in one pass over the data a million  for every data point will make an update right so that slowness factor in what is known  as batch gradient descent right batch gradient descent is when you look at the entire data  and then make one update    what is the flip side what does this module titled stochastic gradient descent so what  is the flip side these are not the true gradients the true gradient is summation over all  the points now this is no longer the true grading this is just a point estimator this is just  a approximation of the gradient right and stochastic because we are calculating the  gradient based on a single data point right it is a sampling one data point and computing  the gradient that this is what the entire population looks like right  this is almost similar to tossing the coin once and saying that this is what the probability  of heads is if it lands at heads then the probability is one otherwise its zero right you see  the error you see the problem with that right as opposed to tossing the coin a thousand  times and then deciding the probability is just tossing it once so this is always going to  be a erroneous right this this is going to be bad  so now there is no guarantee that each step will decrease the loss why because the  guarantees were only when you are doing the right thing which was to compute the  gradients over all the data points now there is no theoretical guarantees right because it  is all stochastic now so it is possible that in a particular data point your loss might  increase also the overall loss on the data with respect to that point it might decrease  but the overall loss right  so now let us see this algorithm in action and i want you to make certain observations  about this so this is the code that i am going to run now so let us see    so i will start and you have to observe and let me know and this is really becoming an  eye test for all of you but that is good so for nothing interesting to observe or already  maybe  remember i am running gradient descent this is not momentum not nesterov this is  gradient descent ok i have already given you the answers what do you observe  student    i can still pretend an answer a let us do that we see many oscillations why why do we  see the oscillations are these oscillations the same as the oscillations that we see in  momentum no these are different everyone gets that right why are there oscillations  what is each click here correspond to one data point right so what is happening here  because we are making greedy decisions right we are looking at one point this point  says to decrease the loss with respect to me move in this direction and we blindly move  in that direction  now we look at the next point it says oh no no wait you need to move in this direction  so we again move in that direction so all these points are actually trying to just make  things better for themselves they are not thinking about what is happening to all the  other points in my data right so all these points are actually competing with each other  so some decision which i took with respect to where to move which was locally  favourable for one of these points may not be good for the other point right hence i  keep these tiny oscillations which i make these are the stochastic noise that you are  seeing now  now can we reduce the oscillations by improving the stochastic estimates always yes  fine so let us see what do i mean by that    so we look at a mini batch version of this so what i am going to do is instead of so  this code is actually for mini batch stochastic gradient descent it is a very minor  alteration on the stochastic gradient descent i will just let you stare at it for a minute or  so what i am doing here is i am instead of doing it for every point i am waiting for a  certain number of points and then making the update right that is what i am doing here  now for this i have kept k equal to two what does that mean i look at two points compute  the derivatives with respect to them and then make an update for two points at a time  what do you expect no what do you expect with respect to this code    so let us see we will try to run this now and you will start seeing a red curve here and  make some observations about this so this is the red curve yeah its visible oops i do  not read any of those  student refer time zero8zero7  if you need to fix this right these bullet us should come only after the curve has finished  this journey ok do not read any of that ah so what do you see about the red curve it is  completely contained inside the black curve that means its oscillations are smaller than  the black curve right does that make sense why this is happening because now you are  not listening to just one point you are listening to two points and then at least you are  doing something better right instead of just taking one  so what is the analogy with respect to our coin toss experiment you are tossing the  coin twice and then deciding what is the probability are heads or tails right so it is  always going to be slightly better than tossing it only once right and now what would  happen in the limit if i keep increasing this you will end up with a batch gradient  descent where you look at the entire data  so looking at only one data point is bad because it is very noisy looking at the entire  data is bad because it is very time consuming so you need to do something in between  which is mini batch gradient descent ok and typically you look at values of one6 3two sixty-four  but it also depends on the amount of data you have and if you have a billion points you  might actually want to look because if you have a billion points and you have a batch  size of sixty-four you will take one billion by sixty-four times to finish the data once  so you might want to keep a larger batch size at that point right but just ignore that but  you will try different batch sizes and see which one works better so in the assignment i  will be asking you in to experiment with bad sizes yes ok no sorry wrong question i  will be asking them to implement stochastic and mini batch also or only vanilla  student mini batch  mini batch fine that is fine ok so you will see this in your assignment so everyone  sees what was the difference between stochastic and mini batch you have better  estimates now and therefore this red curve is contained inside the black curve fine    so you have some things to remember one epoch get used to this terminology one epoch  is one pass over the entire data one step is one update to the parameters n is equal to the  number of data points and b is equal to the mini batch size now you have to fill in the  second column in vanilla or the batch gradient descent what is the number of steps that  you take in one epoch  student one   one in stochastic gradient descent  student n  n n  studentn  in mini batch gradient descent  student n by b  n by b everyone gets that so get used to this ok so this epoch step batch size all this  is something that you will see regularly when you are reading papers on deep learning  fine    so similarly we can have the stochastic versions of momentum based gradient descent  and nesterov accelerated gradient descent    so these are just the codes it is very easy to see what is happening here again basically  this is just an indentation right so if you look at the difference between the two codes i  have just indented it inside that means i am making these updates for every data point  right and same thing you could do for nesterov also    now let us see ah this guess what is it this is the gradient descent stochastic gradient  descent now let us see if you have really understood nag and momentum based gradient  descent one of these curves here corresponds to stochastic nag the other one  corresponds to stochastic momentum tell me which one is which  student blue pill   blue pill red pill blue is  student refer time onetwozeroone  how many of you say that ok i am confused ok how many of you say that blue is  momentum how many if you say that red is momentum oh there is so many you do  not have an opinion  student sir not clear   not clear i will buy that so ok so look at this who is taking longer u turns  momentum or nag momentum roughly which guy is taking the larger u turns  student red guy   red guy right i mean roughly speaking there is only one point to judge by this here  because here they are almost same and that could happen in practice right because this is  now noisy so the red curve corresponds to  student momentum  momentum because it is taking a larger u turn we saw that momentum takes larger u  turns and the blue curve is corresponding to nag ok so no i remember this was an error  on the slide yeah so this has to be red and this has to be blue so ok so the momentum  is actually red and the nag is blue because it is taking a shorter u turn and the reason you  do not see it very clearly is because both of these are running in this stochastic mode  but you still see the relative advantage of them that nag still takes shorter u turns both of  them are faster still faster than vanilla gradient descent    you see that black curve at the top and both of these are faster than them both of them  all three have run for the same number of iterations after 6zero steps you see what happens  to stochastic gradient descent and what happens to nag and momentum basically gradient  descent and of course you can have the mini batch versions of momentum and nag  also   
task2/super_cleaned_audios/lesson21.wav,1838.8665,in this module we will talk about gradient descent  so what we want to do is find a more efficient and principled way of navigating the  error surface     and the goal is to find a better way of doing this    so let us start by setting up things we will define some notations and some parameters  and so on and from there on we will try to come to the algorithm ok so my parameters  in this case were w comma b what i am going to do is i am going to put them into an  array or a vector right and call that vector as theta so theta is the vector of parameters  and theta belongs to r r what rtwo right there are two parameters here so it is a two  dimensional vector    now what i want is again what i will do is i do not know what the value of w comma b  is so i started with a random guess so that is always going to be my starting point i  will always start with a random guess and from there on move on to good values now  once i have started with a random guess i want you to tell me some changes that i can  make to w and b so that i land up in better situations right that means i land up in  situations where the error is less is that fine  so that change in w and b i am going to call it as delta w and delta b and that again is a  vector which is storing these two values so this is the picture right i want to take theta  and i want to add a small change to it so this is my theta this vector is actually theta  right this is the theta vector i want to add a small change to it which is again a vector  this is delta w comma delta b such that i will get a new value for theta new so theta  new would be what actually theta new is equal to w new comma b new is that fine that  is what theta new means    now what has happened is actually when i have added delta theta to theta i have moved  in the direction of delta theta i have come from here to here now i am going to be a bit  conservative and i am going to say that while i am ok in moving in the direction of delta  theta i do not want to make a giant stride what i will do is i will just move by a small  quantity in that direction  so this delta theta is this large magnitude so all i am saying is that i will not i move in  that direction i am fine with that but i do not want to make a giant stride i will just take  a small stride in that direction so eta is a scalar which actually scales down delta theta  so now if i am going to take only a small step in that direction instead of this large  change i will just get a smaller change theta new so red the red vector is actually going  to be the movement which i make that is the new value of theta so theta new is equal  to the original theta plus a small step in the direction of delta theta  so everything is clear you are done we are done with gradient descent what is  missing what is delta theta right i am telling you i want to move in a certain direction  but what is the right delta theta to use how many of you know the answer to this what  is the answer move in the direction  opposite to the gradient why where does that answer come from not the ml class  folks how many of you know why we need to move in the direction opposite to the  gradient why ok we will see ok so that is the question that we need to answer if i  give you an answer to this question then what is it i am doing i am giving you a  principled way such that you start from a random value of theta move in certain direction  and you will ensure that your loss has decreased and then you have to keep doing this  right so that is the set up and the answer to this comes from taylor series    so now what i am going to do is i am going to give you the right direction delta theta  fine and for ease of notation i am going to call it as u so remember what this delta theta  is what is it change in w comma change in b so it is a vector in rtwo remember that  ok i am just going to call it as u now this is what taylor series tells me what it tells me  is that if i am at a certain value of theta and if i want to change that value a bit then what  is going to be the new value of the loss function or any function for that point and this is  the formula for that ok now what is let us see what are some quantities here what is  this quantity scalar vector matrix  scalar this  vector we just did that right it is a vector what about this  what is this quantity actually  gradient what is the gradient what is the gradient no you are telling me how to use  the gradient i am asking you what is the gradient you are giving me absolutely correct  and absolutely useless definitions  that is a very good answer ok so now what i am going to do is i am going to digress a  bit and i am going to tell you something about derivatives partial derivatives and  gradients and then we will come back to this ok so now suppose you have a function l  this is l in my handwriting this function of w and say this function is w square ok  now what is what is this called a derivative of the function with respect to w this is  the derivative and you know this is twow ok now suppose i have a function b square  now what is this quantity  is a partial derivative of the function with respect to w why partial  because it is considering b as a constant and taking the derivative with respect to only  one of the variables right this happens to be and what is this quantity oh sorry so is w  comma b right this is the partial derivative with respect to b ok  now can you tell me what is a gradient the gradient is nothing but it is just these two  partial derivatives taken together and put into a vector right now suppose i had a  function which depended on hundred variables what would the gradient be size of the  gradient  rtenzero it would lie it would be a hundred dimensional case ok so now can you tell me  with this evidence in knowledge but this primer can you tell me what this is this is a  gradient vector  which is right there in front of you in a red ink  this is what it is right fine everyone ok with that so actually the right way to write  this and probably we need to correct in the slides would be theta so remember that theta  is equal to w comma b so this is the derivative of l theta with respect to theta which is  nothing but the collection of the partial derivatives with respect to the components of  theta is it fine so everybody understands what is a derivative partial derivative and  gradient ok fine so now the gradient is a vector in this case fine ok    so now what is this quantity it is a  no it is what is this  the dot product between these two vectors ok fine now one last thing and many more  things actually so what is this square of the gradient  this is not the square of the gradient what is this hessian fine everyone knows the  textbook what said can you tell me what does it is a scalar vector matrix  matrix what is the size of this matrix  two by two what are the elements of this matrix  second order partial derivatives right so it is the gradient of the gradient right is that  fine so what does that mean you had this gradient this is the gradient now you want  to take the gradient of this again with respect to w comma b right that is what this  means it is a gradient of the gradient right so what that means is we will take the  gradient of the first quantity again with respect to w so that would be dou square by  dou w what would this quantity be   what would this be  is that fine and you can fill in this quantity right so now it is clear what the hessian  is it is the derivative of the derivative and it would be a matrix ok is that clear to  everyone so i have a habit of doing a lot of these basic stuff i know that the top twozero  percent of the class gets really pissed off when i do this but as a philosophy i teach for  the bottom thirty percent of the class  so i do not mind that and the other thing is i use slides so i do not write a lot of math  so i can cover a lot of material despite doing all this basic stuff right so i am going to  stick to that what i am trying to say is that write this in the feedback that you do not like  this basic stuff but it is just that i am going to ignore that feedback i mean just being  honest right so i like doing this because it just takes me ten minutes to do this and for  the rest of the class i do not have to look at blank faces afterwards right so it really  helps me a lot fine so is that all clear all the quantities here are clear  so now so this is the gradient this is the hessian and now eta remember what did we  say about eta  it is a small quantity and what do we do with small quantities always in maths  we ignore them so once we take their powers you are always ignore them whether it  is correct or not who cares i mean someone has told it it is good to ignore so we will  ignore it right so now all these higher order terms we can ignore right that means i  will only consider this fine  so let us again look at what the setup is the setup is that i have some value of theta i  want to move away from that value such that what do you say about this loss compared  to this loss i will call this the new loss and i will call this the old loss what is the  relation between them  the new loss should be  less than so if i or someone gives you a u i am not getting ok someone gives you this u  then what does what when would you say it is a good u    if this condition holds everyone agrees with that right so i have found a good direction  to move in if this condition holds now this condition actually implies that this condition  should hold right this is l theta plus eta u right so if i just do minus here i get this  right so this quantity which should be less than equal to zero implies that this quantity  should be less than equal to zero and remember eta is a positive constant ok why cannot it  be negative  why because you wanted to take a small step in that direction if we make it negative  we will do what  we will reverse the direction we do not want that as of now right so eta is that for a  positive quantity so that means this quantity should be less than zero is it fine with  everyone    so so far after all this story what we are left with is this condition should hold for the u  that i am trying to choose so that i can be sure that i have chosen the correct u right and  the definition of correct u is that the loss at the previous step the loss of the new step  should be less than the loss at the previous step is that fine so that is what we have  arrived at  now what is the range of this quantity that is why i asked you what is this this is a  dot product i will leave it at that so now you tell me what is the range of this people  from the ml class cannot answer did i cover this in the ml class no ok fine what  is the range of this not a very hard question  plus or minus  student refer time one4two5 mod of u refer time one4two6  very good how many of you understood that answer he said plus or minus mod of u  into mod of gradient the gradient vector right why is it so easy  let beta be the angle between u t and this between sorry it should not be u transpose  between u and the gradient then we know that this condition holds cos beta is given by  this quantity and we know that cos beta lies between minus one and one ok now if i just  say that this quantity is equal to k then i can just get this condition  now let us see what are we trying to do we are trying to find a u such that this quantity  is negative we are trying to find the use such that this quantity is negative now i just  stop at negative we would like to make it as negative as possible right because the more  than negative it is the more will be the decrease in my loss function right because this  quantity tell me tells me how much my loss decreases so the more the negative it is the  more the loss will decrease so let me make it as negative as possible  now what is that value when will that happen when alpha is you know the answer  you started with the answer  student refer time one554  no what is that one phrase which you have marked up move in the direction   student refer time one6zero7  ok now think of that  student refer time one6zero9  what would happen when this is the most negative it can be what would the angle be  student refer time one6one9  one8zero degrees how many of you get that because when this is the most negative that  means the cos beta is actually minus one and when is cos beta minus one when the angle is  one8zero degrees that means u should be such that it is at one8zero degrees to the gradient hence  repeat the phrase  student refer time one64two  move in a direction opposite to the gradient is that fine everyone gets it now why  you need to move in the direction opposite to the gradient    so this is what the gradient descent rule is you are at a particular value of theta you  want to move to a new value of theta such that your new loss is less than the current loss  what gradient descent tells you is move in a direction opposite to the gradient so are  you fine with this now with gradients i have come to scalars but i will just explain  what i have written here  so this quantity is nothing but theta t plus one right is equal to theta t right and what is  this right so the new theta is equal to the current theta minus why because we want to  move in the direction opposite so it is basically theta t plus one is equal to theta t plus eta  into a negative direction right the direction negative to the gradient hence you get that  minus one  now what are these quantities let me just take that carefully so this quantity is gradient  of the loss function with respect to w sorry the partial derivative of the loss function  with respect to w evaluated at w is equal to wt and b equal to bt what does that mean  so remember when you are dealing with derivatives as always a formula and then a  value add that at a particular value so what is the derivative of x square with respect to  what does not matter twox  so derivative of x square with respect to x is twox what is the value of this derivative at x  equal to one two right so you see the difference you have a formula which is twox now you  substitute in a particular value and you get the value at that particular value ok so that  is what this means because you are already at w t comma b t now you cannot subtract a  formula from here right you have to put subtract a value so you know what the formula  is you plug in the values of wt comma bt get that value and subtract it from your current  wt is that fine so everyone completely understands what is the gradient descent rule is  fine    so now we have a more principled way of moving in the w b plane what do i mean by  that remember this was our w b plane this was our error this is something what our  error surface looked like it was this flying carpet i was randomly moving on the w b  plane earlier right and trying to guess what the errors or trying to compute the error and  then settle for a particular value now i have a more principled way of moving in the w  b plane i know what is the next step based on the current step i just need to move in the  direction opposite to the gradient  so let us try to so this is what it tells me for one step but i need to keep doing this till  what is that golden word  student refer time one958  convergence right i have to keep doing this till convergence ok    so let us create an algorithm out of this rule i will start a time step zero i will do this for  some max iterations instead of saying till convergence i will do it for some iterations at  every iteration i will this is how i will update my weights i will take the current weights  subtract the gradient from that and get the new weights i mean not subtract the gradient  subtract this quantity and get the new weights so now is everything clear is the  gradient descent algorithm done can you do it for the toy network which i had is there  something still missing  student refer time twozero38  eta is fine we will take a small value zerozeroone or something actually not told you what  these are right i means to write it you know these are derivatives but what is this  actually ok so let us see that now so that is what we are going to see next    so now we want to find out we are in the car quest is for this delta sorry the partial  derivative with respect to w and partial derivative with respect to b that is the thing  which we had plugged in the formula but we do not know what that is right so we  need to find that out so now for simplicity let us assume there is only one point of it  which is x comma y so earlier we had this xone yone and xtwo ytwo now i am just assuming  there is only one point which is x comma y    so now what is a loss function earlier i had this summation over i equal to one to two but i  have just one x comma y so i will just use that this is what my loss function and what  are the quantities that i am interested in finding one is this the partial derivative of this  loss function with respect to w    so let us do this lets actually derive this so this is what it looks like now you have to  help me in deriving this what will i do first  student refer time twotwoone8  tell me the next step  student refer time twotwotwo3  two into f of x minus y and push the gradient inside of course the derivative is that fine  anyone who has a problem with this next y is a constant this is the true i remember  so that is why this is a constant is not the predicted y  now this quantity what is f of x actually  student refer time twotwo5one  sigmoid function right so i will just write it now this is the quantity that i need the  derivative for so i will just write it here what is the next step this is of the form one  over x so what will it be  student refer time twothirty4  minus one over x square and then you put the derivative and say it is that fine now the  quantity inside is of the form e raise to x so the derivative is e raise to x and you push it  inside is this fine so this on slide should come both these are coming together so is  that fine now what is this actually  student f of x  f of x what is this  student refer time two33two  this is actually one minus f of x just take my word for it for now you can go home and  work it out right so this actually if you do one minus this and do some trickery you will  get to this quantity right so what you get is a very simple formula f of x into one minus f  of x into x i am going to substitute back here so now i exactly know what the partial  derivative of w is    so there is only one point then this is what the partial derivative with respect to w is  going to be of the loss function right if there were two points what would happen if  there were two points my loss function was this is a sum of two elements and i am  taking some derivative of a sum i will get a sum of derivatives right    so how many of you will not cringe if i say this is the answer anyone who has a  problem with this you get this how many if you do not get this how many of you get  this good fine now can you do a similar thing for b can you tell me the answer  without actually deriving it  student refer time two44one  i can perfectly understand what you are saying  student refer time two448  x would not be there right because this last x that you see here came because w into x  was there but b we are not multiplying x so what we will get is this you can go home  and check    so now we have everything that we need now we actually have everything that we  need ok no more trick questions so now we will write code to do this ok we will  actually implement the code and see what happens so these are the two data points that  i had zero5 comma zerotwo and two5 comma zero9 the first thing which i need is something which  can implement the sigmoid function so this is one over one plus e raise to minus w x plus  b is that fine  now i need something which can compute the error so this is summation of half into f  of x minus y the whole square i go over all the data points summation of half into f of x  minus y the whole square is that fine now what i will do is i will take this try out a lot  of values of w comma b and plot the error surface ok but this is only for illustration in  practice i will not do this we just know that this error surface exist i just want to verify  that whatever algorithm i come up with does not efficient navigation of this error  surface that is what i want to verify that is why i am plotting this  next time you need a function which can compute grad of b we just saw this on the  previous slide this is f of x minus y into f of x into one minus f of x right simple  everyone is fine with this then i need a function which can compute the grad with  respect to w same thing except that i have this x at the end so i have all the ingredients  in place now what would i do what is the next thing that i will write the main loop  right i will write the main loop now  so this is what the main loop look like looks like i start with some random initialize for  w comma b remember that our initial theta which is composed of w comma b is going  to be some random guess so i started with the random guess which is minus two comma  two i have chosen eta to be one that means i am not going to be conservative i am going to  move in the direction of the gradient if i chosen at zerozeroone and zerozerozeroone i would have been  conservative and i am going to run this till tenzerozero epochs which is my notion of  conversions  now in each epoch what i am doing is for every data point so remember that this  gradient with respect to w was a summation of i equal to one to two and that formula right  so for each data point i am computing the grad adding it right so that is the  summation part similar thing i am doing for b once i have computed the gradient which  is the summation quantity i am just moving in the direction of the gradient is that fine  everyone understands the code it is simple python code and it does exactly what i had  shown in the pseudo code  now let us execute this code and see what happens so i will start with my random  point which was minus two comma two and now i am going to actually run this code and  keep plotting what happens on the figure so just pay attention fine so now here is  how the code is running see what is happening what is happening actually so at every  point i am changing my w so that i am moving in the direction of the gradient i keep  doing that as i keep doing that my error keeps decreasing why because that is exactly  what we got from taylor series that if we do this the error is bound to decrease right and  then we keep doing this and after a few iterations we will actually reach almost the value  which is the zero error right and this same thing would happen if you start from  anywhere else it will keep moving in a principled way and reach the low error  configuration  now some of you would say that maybe this was the shortest path right it could have  just rolled over from there but that is not a principled way of doing that right we the  principled way of doing it is to move in the direction of the gradient you might take a  longer route but reach your destination taking shortcuts is always risky in life as well  as here so so do not please this is an advice for error assignments and so on so this is  the more principled way and we will reach the solution so that is what is happening  here so we have actually derived everything that we needed and this is all you need to  write for gradient descent for this toy example that you had  now answer this question now suppose i had hundred such variables instead of w  comma b i had hundred such variables what would happen you do not have to  visualize it  student refer time two948  in terms of the code  student refer time two95two  i will just need to have these functions for all of those i will have to calculate it by hand  but still doable it is just a lot of tedious work of course later on we will see a more  refined way of doing this where we can do a lot of these computations at one go so we  can directly start operating in vectors as opposed to scalars here i am treating w and b  separately here i could have actually had a function which tells me grad of theta directly  right and later on we will see something like this ok but for now the code is still  running here    now it suffices so later on we will see gradient descent in more detail in the course  and we will also see a lot of variants of gradient descent but for now it suffices that we  have an algorithm which can learn the parameters of a sigmoid neuron so just as we  had the perceptron learning algorithm we have the gradient descent learning algorithm  which can help us learn the parameters of the sigmoid neurons starting from random  values and it gives a principled approach for doing that  
task2/super_cleaned_audios/lesson35.wav,1083.2226,in this module we will look at momentum based gradient descent  so what were the observations about gradient descent that it takes a lot of time to  navigate regions having a gentle slope so what is the practical implication of this in  practice why it what does this need to what does this mean right it takes more time so  remember we had said this max iteration equal to one thousand  now if you are initialization happens to be such that you are stuck in this large flat  region then those one thousand iterations just keep moving around that flat region right you  will not enter into one of the valleys and valleys is what you are interested in right  because values is where you will have some minima for your function right  so if you have a very gentle slope then for one thousand iterations you will keep moving around  that gentle slope right that is why this has a practical implication now this was because  the gradient in these regions were small can we do something better that is the question  right so yes we can and we will take a look at momentum based gradient descent    so here is the analogy which i give my ta’s have heard this at least ten times so i will  just repeat it the eleven time for them so i hope that is the one which i want to use here  yeah ok so now suppose you are standing at the velachery gate and you want to go to  phoenix market city something that all if you can relate to today so you want to go to  phoenix market city and you ask the security guy at the gate that where do i go right  so he will say take a left no take a right so i am slightly dyslexic actually i have a  left right dyslexia so take take a right ok so you will say he has told me to move  right but you would still be a bit  cautious right we will just keep moving slowly in that  direction that is how we find ask for directions you keep moving slowly in that  direction  now tenzero steps later or tenzero meters later you find another guy and you ask him or her  where is phoenix market city he again points to in the same direction keep moving left  right so now you will what will happen you will increase your space and then you  ask again someone when you read the signal where it is and he again points in that  direction what will happen move even fast  so what is happening here if a lot of people are pointing you in the same direction you  better start taking larger and larger steps in that direction does that make sense that is  how we find directions and move around so just like a ball gains momentum as it goes  down a slope right it is constantly moving in that direction so it starts moving faster so  now can you tell me a way of incorporating this i have been moving in a certain  direction these directions are nothing but the gradients and now at this point someone  asked me again to move in the same direction what should i do  student take a bigger step  take a bigger step so can you think or try to imagine how would you do this  mathematically  student refer time zerothreeten  so it is probably there are a few ways to do it so let us see so what i am doing here  is this is my current gradient right so i asked that guy at the signal he asked me to  move in that direction so that is this direction and this is all my history whatever i did  till step t minus one ok so now what i will do is i will so earlier i was moving like this  this is what my update rule was wt plus one is equal to wt minus in the direction of the  gradient right i will moving in the direction opposite to the gradient  now what i have is in addition to that i have this gamma update t minus one so that  means whatever i had done up till step t minus one i will also take that into account so i  will end up taking a larger step is that clear if it is not clear it will become clear on the  next slide    so let us see what this means right so it basically means that in addition to the current  step also look at the history there are three guys who earlier pointed you in the same  direction so maybe this direction makes sense right so start accumulating that and  move faster    so let us just break this down and see right so this is what the update rule is sorry this  is all my updates and this is the update rule so at time step zero my update is zero  because not started yet at time step one this is what it will look like right and this is  nothing but just move in the direction of the opposite to the gradient because this minus  sign will come later on right in the next equation   now what will happen update two so its gamma times update one plus the gradient at  the current step so remember here everything is positive i am adding the gradients  because my final negative sign is going to come in the next equation ok so do not get  confused with that eventually i am going to move in the direction opposite that opposite  will come from this negative sign  so what is happening i am moving in the current direction plus a fraction of the  direction which was pointed earlier right ok then does this make sense so can you tell  me in general what is happening here at the t’th time step what is happening what kind  of average am i taking weighted average but it is a dash weighted average this is an  exponentially weighted average ok so let us look at this right  so when i am at step four i have most faith in the current gradient right and this gamma is  always i will just set it to less something less than one right so i have a fractional trust in  the previous gradient even smaller trust in the previous guy and even smaller trusts in  the previous guys so i am taking an average of all my gradients but it is an  exponentially weighted average does that make sense my maximum faith lies in the  current guy and then decaying faith in the previous guys  and as i move further and further away from the last guy that i checked right i will give  lesser and lesser weightage to that so everyone understands what is happening here  anyone who has a problem is just raise your hands if you understand this good  so in general this is going to be the formula and you see that as as i form problem here  no as t is larger this fraction is going to become smaller and smaller right so you are  first the first step that you take will have lesser and lesser weightage as t increases  everyone gets this fine    so now this is the code for momentum based gradient descent i will just give you a  minute to stare at the code and see if it makes sense so this much part is ok you are just  computing the gradients with respect to all the points right and now we are keeping this  running sum ok which is the previous gradients and the current gradient right and then  you are just subtracting that running sum   now this looking black curve that you see here that is gradient this this guy ok this  black curve that you see here that is gradient descent when i have run it for around tenzero  iterations now i am going to run momentum base gradient descent and each click is  going to be one step ok and i want you to observe what happens ok so slowly a red  curve will start appearing on the figure  initially it will not be visible so do not worry there is nothing wrong with your eyesight  one how many if you already see the red part i see it two three four five six no now you can see it  as is nothing great about7 eight nine i want you to observe something here eleven onetwo onethree onefour came  back right so gradient descent i ran it fourzerozero iterations it was just stuck here right this  was a point and i ran this for less than like around onefive or twozero is what we counted right and  so already entered into the valley  so momentum base gradient descent is good you see that wicked smile on my face and  you know it is a trick question so we are moving fast right    even in the regions where the slope was gentle right that is the beginning of the  beginning of our trajectory right this was the gentle region even that i was very quickly  able to navigate right within five to six steps i was away from that part right so even in the  regions where the slope was gentle i was able to move fast but is moving fast always  good  so would there will be a situation where momentum would cause us to run fast ago  same thing now instead of walking you are in a car you ask the person at the security  whether i should go there he says yes go in the right direction you keep moving there  someone else you keep accelerating what will happen eventually you will go fast  phoenix market city then what will you do  student take a    take a u turn come back again while taking a u turn what will you do  student refer time zeroninefive7  overshoot and come to the signal and then go back again right so you see this you will  end up taking a lot of u turns so let us change the input data a bit and see what happens  to momentum based gradient descent    so this is what my data looks like now so this is not what my data looks like this is  what my error surface looks like so earlier we had this error surface something like a  flying carpet now i have a very peculiar error surface this is again for the two  parameter problem right w comma b that means i want to learn a sigmoid function  where i have these two plateaus at the top the dark red regions that you see and then a  very sharp valley can you tell me how i would have come up with this kind of an error  surface what are the points that i would have chosen just hold on to that part  so i have this kind of an error surface fine the error is high on either side of the valley  now could momentum be detrimental in this case yes no maybe i do not care i do not  care fine    so let us see this is the is this the two d equivalent of that three d surface everyone gets it i  can perfectly verify that you get it everyone gets it i will assume right so these are the  very high plateaus where the error is very high very sharp and narrow valley where the  error is low  so now again this sorry looking black curve is what i have done with gradient descent  after some tenzero iterations or something now i am going to run momentum based  gradient descent and you have to help me understanding what is going to happen again  you will soon start seeing that red curve appear one two three four five six what will happen now it is  already fast that is known it was that black curve was after tenzero iterations or so it is fast  now tell me what will happen  student refer time onetwozerozero  he will go out is actually almost come out of the valley right it is almost at the top of  the valley now what will do take a u turn now what will i do again take a u turn  now i will keep doing this i will take now smaller and smaller u turns and it will  converge right so what happens here is because of this speedy movement and which is  very analogous to that car movement which i described  this overshoot your goal you will have to take the u turn come back if you are again  careless you will have to keep taking these u turns but you will finally end up at the  location that you want right it takes a lot of u turns before converging despite these u  turns it still converges faster than gradient descent right because gradient descent can  just not move at those gentle slopes right it just cannot move from there because the  gradient is almost zero because the slope is flat right and it just cannot move but even with  this lot of u turn and lot of rework after tenzero iterations momentum base gradient descent  has reached an error of almost zero whereas gradient descent is still stuck at the plateau at  an error of zerothreesix ye so see you have reached the minima now  student ye  now you will be navigating there right but you know that now your loss is very slow  low so you could end that right you know that your loss is very close to zero so you  could have a condition that once you have reached something very close to zero you could  end that even if you are making these very small movements now you could just stop  there  student but in the plateau regions is also zero  but the loss is high right so if the loss is high and you are not moving you cannot stop  but if the loss is low and you are not making movements you can just stop there right  so you can just end you can define that as your convergence condition    so let us look at we will come back to three d now we look at a three d visualization and a  very different interpretation of what is happening i really want you to understand what  exactly is happening in this example which i had picked up right    so this is what the three d surface looks like view from a different angle you have these two  plateaus and the very sharp valley now this is the corresponding sigmoid function  where i started with so what i am trying to tell you is that this is a sigmoid function  corresponding to w equal to six oh no sorry w equal to two and b equal to six  this is the sigmoid function that i got once i plug that value so sigmoid is one over one plus  e raised to minus w x plus b and i have plugged in the values of w and b and plotted it  for all the values of x and this is the sigmoid that i got ok so that is my starting point is  this good how do you define good or bad  student refer time onefourfourthree  what do you expect at the end of training it should pass through all your training points  and these are my training points ok is it passing through them no its way off right ok  so now let us start this momentum based gradient descent and what just see how my  sigmoid function changes so right now i am on the gentle slope even that momentum  base gradient descent it is going to be fast but not dramatically fast because still  building up the momentum  so it is you see that these sigmoid that i am drawing here they are almost  indistinguishable from each other i have already drawn three sigmoids here so i will  just go back so there was this initial guy then i draw drew a red one then one more and  then one more but they are all very close to each other  now keep viewing both these sides in parallel what happens here on this figure and  what happens to this sigmoid ok and i will ask you questions so still i am moving a bit  slowly because i am still building the momentum right it takes time to build that  moment now i have slowly started building the momentum my sigmoids have started  moving towards where they should be everyone gets this what is happening here ok  now tell me what will happen as i enter the valley i am almost entering the valley  what will happen i have gained this momentum now so my w comma b values are  going to change much faster now so what will happen to these sigmoids they no longer  stick to each other we will start seeing a difference they are already moving away from  each other ok so that is what is happening to the function ok now you see even faster  changes ok now what will happen i have entered the valley this is how my sigmoid  looks at this point now tell me what will happen  student refer time onesixthreefour  it will go fast what will happen to your sigmoid how many of you know what will  happen to the sigmoid ok i will tell you what happens and then it will be obvious right  so now i am entering the valley all of us know that i am going to come out of the value  of the other side right so let us see what happens when i come out of the valley from  the other side the sigmoid changes that is why you have this situation that your error is  high on both sides right because on this side you have these kind of sigmoids on the  other side you have the other sigmoids and somewhere in between lies the solution  where does the solution lie at a very flat sigmoid right  so now i start this is where the oscillations will happen so notice what will happen to  the sigmoids they will toggle between these two orientations ok just see what happens  to the sigmoids you see it again moves keeps moving keeps moving it keeps oscillating  around the solution and then finally you reach the solution so you see that should i  repeat this  so when i am on one side of this valley i have one kind of sigmoids right now when i  move to the other side of the valley i have this others kind of sigma and take a u turn so  when i u turn take a u turn i again overshoot and go to the other side and this keeps  happening and i keep toggling till i reach my final solution  so these are all the oscillations that you are seeing so can you visualize this what is  happening do you understand all these relates to the actual function that you are trying  to learn so that is why we will end this module this was on momentum base gradient  descent now we will see a nesterov accelerated gradient descent   
task2/super_cleaned_audios/lesson1.wav,3464.0056,hello everyone welcome to lecture one of cs7 0one5 which is the course on deep learning in  today’s lecture is going to be a bit non technical we are not going to cover any technical concepts  or  we only going to talk about a brief or partial history of deep learning so we hear the terms artificial neural networks artificial neurons quite often these days and i just wanted you take you through the journey of where does all these originate from and this history contains several spans across several fields not just computer science we will start with biology then talk about something in physics then eventually come to computer science and so on so with that let us start so just some acknowledgments and disclaimers i have taken lot of this material from the first people which i have mentioned on the bullet and there might still be some errors because its dates as back as one87one so maybe i have got some of the facts wrong so feel free to contact me if you think some of these portions need to be corrected and it would be good if you could provide me appropriate references for these corrections deep learning so let us start with the first chapter which is on biological neurons as i said its spans several fields will start with biology and we will first talk about the brain and neurons within the brainso  way back in one87one one873 around that time joseph von gerlach actually proposed that the nervous system our nervous system is a single continuous network as opposed to a network of many discrete cells so his idea was that this is one gigantic cell sitting in our nervous system and it is not a network of discrete cells and this theory was known as the reticular theory and around the same time there was the some breakthrough or some progress in staining techniques where camillo golgi discovered that a chemical reaction that would allow you to examine the neurons or the nervous tissue so he was looking at this nervous tissue using some staining technique and by looking at what you see in this figure on the right hand side the yellow figure even he concluded that this is just once single cell and not a network of discrete cells so he was again a proponent of reticular theory so this is about camillo golgi and then interestingly santiago cajal he used the same technique which golgi proposed and he studied the same tissue and he came up with the conclusion that this is not a single cell this is actually a collection of various discrete cells which together forms a network so it is a network of things as opposed to a single cell there so that is what his theory was and this was eventually came to be known as the neuron doctrine although this was not a consolidated in the form of a doctrine by cajal that was done by this gentleman so he coined the term neuron so now today when you think about art here about artificial neural networks or artificial neurons the term neuron actually originated way back in one89one and this gentleman was responsible for coining that and he was also responsible for consolidating the neuron doctrine so already as you saw on the previous slide cajal had proposed it but then over the years many people bought this idea and this guy was responsible for consolidating that into a neuron doctrine interestingly he is not only responsible for coining the term neuron he is also responsible for coining the term chromosome so two very important terms were coined by this one person so now here is a question so around one906 when it was time to give the nobel prize in medicine what do you think which of these two proponents say there are two theories one is reticular theory which is a single cell and then there is this neuron doctrine which is a collection of cells or collection of neurons that a nervous system is a collection of neurons so what do you think which of these two guys who are proponents of these two different theories who would have got the actual nobel prize for medicine so interestingly it was given to both of them so till one906 in fact way later till one950 also this debate was not completely set settled and then the committee said both of these are interesting pieces of work we yet do not know what really actual what the situation is actually but these conflicting ideas have a place together and so the nobel prize was actually given to both of them and this led to a history of a some kind of controversies between these two scientists and so on and this debate surprisingly was settled way later in one950 and not by progress in biology as such but by progress in a different field so this was with the advent of electron microscopy so now it was able to see this at a much better scale and by looking at this under a microscope it was found that actually there is a gap between these neurons and hence it is not a one single cell it is actually a collection or a network of cells with a clear gap between them or some connections between them which are now known as synapses so this was when the debate was settled so now why am i talking about biology why am i telling you about biological neuron and so on so this is what we need to understand so there has always been interested in understanding how the human brain works from a biological perspective at least and around this time the debate was more or less settled that we have this our brain is a collection of many neurons and they interact with each other to help us do a lot of complex processing that we do on a daily basis right from getting up in the morning and deciding what do we want to do today taking decisions performing computations and various complex things that our brain is capable of doing now the interest is in seeing if we understand how the brain works can we make an artificial model for that so can we come up with something which can simulate how our brain works and what is that model and how do we make a computer do that or how do we make a machine do that so that is why i started from biological neurons to take the inspiration from biology 
task2/super_cleaned_audios/lesson34.wav,627.8322,so we look at something known as contours  so now visualizing things in threed can sometimes become a bit difficult especially for the  person who is making the slides so we can can we do a twod visualization of this  traversal have i done this in the ml course no good can we do a twod visualization of  this traversal along the error surface  so for that we need to understand something known as contours how many of you have  looked at contour diagrams before how many of you know how to read them all of you  know how to read them    so let us see now suppose this is what my error surface looks like and i have a single  scalar variable so this is just a function of w for example and this is what my error  surface looks like  now what i am going to do is i am going to take horizontal slices on this error surface  fine now can you tell me how this is going to look from the top sorry let me you  should start answering before understand the question this is this error surface is  actually so i was wrong in saying this is theta assume this is w comma b and you are  just seeing the front view of the error surface what you are seeing here is just the front  view  this error surface is actually like a dementors hat so right so imagine that it is a hat  place like this and you are just seeing the front view of this otherwise a top view does  not make sense right so now i am going to slice this hat at two vertical positions and  now you are looking at it from the top what are you going to see  student ellipsis  ellipsis everyone agrees with that    so we will see something like this do you see something peculiar about this is this a  contour map is this no ok and all of you raise your hands when i asked do you know  contour so do you see something peculiar about this what is it how many if you get  that so what you are seeing here is this portion right where the slope was very steep  the difference between the two circles or the two ellipses is small and you can visualize it  if you try to look at it from the top this distance is actually going to be small right and  in the areas where the slope was gentle relatively gentle the distance is more and you  can again visualize it right from if you look at from the top this is the distance that you  are going to see and what do you say about these guys what does that indicate they  are the same  student refer time two hundred and forty-six  value across that entire region the value is same because you have taken a verticals  you have taken a horizontal slice at a particular vertical position right so you have  taken a horizontal slice at this position that means the error is going to remain the same  throughout that rim is this clear to everyone ok it is very important that you understand  this  so there are only two things that you need to understand if you want to read contour  maps one is a small distance between the contours indicates that the steep slope exists  along that direction and a large distance between the contours indicates that a gentle  slope exists along the direction so everything today is going to be about steep and  gentle slopes and the other thing that you know need to know is that whenever you see  one circle the error is the same along that circle or ellipse whatever you boundary that  you see the error is the same because you are taking these vertical slices so we are  ready with this rule everyone understands this perfectly    so i will just give you a couple of exercises and you have to tell me whether you  understand this or not    so i have plotted a three d surface or two d i have what is this  student refer time 0threefivefour  no  student there is a contour  there is a contour everything is not going to look like clean circles always right ok so  this is a contour every line that you see here represents one cut along the vertical axis  right that means the error is the same there now what you are seeing is a contour i  want you to guess the three d surface from this you just guess it i mean just keep it to  yourself fine the color is the same right blue is good red is bad  so blue means the darker the shade of blue the lesser the value of the error the darker  the shade of red the higher the value of the error ok i want you to imagine the three d  surface if you can do that then i will be sure that you understand what how to read a  contour how many if we can imagine this you can just say yes right i can never figure  out whether you actually speak it  so let me help you with the first one and then we will do a few more so let us start  with the extremes right so let me see how to do this so this portion i also need to do it  for the video ok so let me just do it here so this portion what do you think about the  slope there very flattish why because this is the line that you see and the other line is  not even in the figure right so it is basically very flat the slope is very gentle is it a  low region or a high region high region fine ok now what is actually happening here  what is the slope here  student high  very high that is why these two regions are very close to each other so from this high  region what is happening suddenly there is a slope and you are going down and you  know you are going down because you are reaching a blue region right ok now what is  happening here  student very flat  very flat and this also flat but slightly upper than the lower guy is that fine now can  you all imagine this ok and is this what you thought it is perfectly yes right is exactly  what you thought ok just a minute so the orientation here has been changed a bit right  so this portion actually corresponds to this portion are the two this is clear this portion  corresponds to this portion right the just orientated fine  so you start off this high plateau region which is here then you start going down go  down and then you see a fold here right that is this fold so you went to a darker shade  and then you came up to a slightly lighter shade the shades are ok  guess the threed surfaces how many if you want to play this forever now start with the  extremes the bad guys the good guys the plateaus and the valleys and then see how do  you go from the plateau to the valley ok tell me the corners first this plateau or valley  student plateau   plateau this plateau higher than this or lower than this  student lower than this  lower than this this  student valley  this towards the valley it is still between red and blue right it is not like right down  there and what happens to all these guys all are very steep slows all converging down  into the valley so can you perfectly imagine this  and you will tell yes when i say when i show you the three d surface right again you need  to reorient yourself so this corner here is this corner this corner here is this corner so  we had these two plateaus at the top we had this slightly higher valley slightly lower  valley and then all of them going into a very deep valley you see that everyone gets  this how many if you have a problem with this if you have a problem with this you  will just sleep off in the rest of the lecture so i want you all to understand this very  carefully i do not mind repeating it how many of you understand this you understand  the regions with gentle slope  student yes  the regions where you have a steep slope and you end up into that valley which is the  valley here can you point it out fine ok so we will move ahead    so now we know what contour maps are and how to visualize them and so on right so  now we will try to see the gradient a descent algorithm instead of running it on the threed  error surface we will try to run on this twod contour map    so this is what i already showed you right i started from here and i showed you how it  comes here or something like this right that was the gradient descent let me just erase  this ok that is something like what the gradient descent algorithm  now again you just need to reorient yourself so let us see this corner is this corner  this corner is this corner and so on right so you get the reorientation right it just  shifted now i am going to start my gradient descent algorithm from here from this point  ok everyone see is that ok i am going to start from there and you have to help me and i  am not going to just keep clicking you have to tell me what is going to happen so what  will happen initially fast movement slow movement  student slow movement  slow movement right so i am running it one two three four five six seven eight it just keeps running very  slowly now what will happen  student fast  fast ok now you see actually you can see the arrows these arrows are the quantity the  magnitude of the movement right so earlier this movement was so small that you could  not even see the arrows i have been drawing arrows right from the beginning but you  could not see them at the beginning now you can see them right now what will  happen  student slow  slow right so you see the exact same movement that i did on the three d surface now you  can visualize it on the two d surface right and you can easily tell me where it will go fast  where it will go slow right and where it will just keep moving very drag its feet and so on  ok so this is where it starts dragging its feet and the same thing happened when we  were in this region right so just you just make the connection that we are in the  corresponding three d region there ok fine so we are moving very slow and it just keeps  running  so that is where we lend this module so we just revised gradient descent we saw that  things are proportional to the gradient that is why gradient descent and the smaller the  gradient the slower the movement the larger the gradient higher the movement gentle  the slope  student smaller  smaller the gradient steeper the slope larger the gradient  
task2/super_cleaned_audios/lesson20.wav,671.4605,them by guesswork and i will show that that is actually infeasible that is why we need a more principled approach so we will keep the supervised machine learning setup in mind and now we will focus on this model and discuss an algorithm for learning the parameters which are w and b given some data using a giving appropriate function objective function so that is what we are going to focus on now sigma here stands for the sigmoid function the logistic function in this case when this sigma is actually the logistic function and now i am going to simplify this further so that it helps us to do a better analysis i am just going to consider the case where i am just in one input and the bias ok and also following the normal terminology in the literature this w naught from now on i am going to call it b because that is the normal convention b stands for bias so i have two parameters w and b which i need to estimate ok and this is my model for the movie example and the other change which i am going to make is instead of deciding whether i like or dislike which is one zero the setup that i am going to work with is that i am giving the critics rating and i want to predict the imdb rating so i am given a real value and i also want to predict a real value for no particular reason this just makes life easier for me for explaining a few things but the same thing or the same algorithm would also hold if you add a binary output right and you will see that later on in the course so here is a setup clear we just have two parameters w and b and we are going to assume that y belongs to real numbers it is a imdb rating and x also belongs to real number it is a critics rating   now let us see what we are given as training is a set of points we are given some n training pairs and now we understand what this means that means for a lot of movie i am giving the critics rating and i am also given the true imdb rating for them of course in the two variable case this does not make much sense but just bear with me and now the training objective is such that whatever my function predicts which is a function of w x and b that should be very close to the true output that i know this is the function that i want to optimize now let me ask you this i am trying to tell you that i am going to give you an algorithm for training this network suppose i have trained this with two data points zero5 comma zerotwo and two5 comma zero9 right at the end of training i will give you some values of w and b let us call them w star and b star these are the final values of w which i have given w and b what do you expect from these values  what do you expect at the end of training if i say now the network has learned what do you expect you are still going to the test case i am just talking about the training still we expect such that what happens if i plug in at the end of training if i plug in the value zero5 here what should happen  zero9 so this is what you expect at the end of training if you plug in the value zero5 it should be very close to zerotwo the output and if you plug in the value two5 it should be very close to zero9 this is exactly what you expect and this is what training means ok fine in other words we hope to find a sigmoid function such that these two points lie on that function can you imagine a geometric picture for this what would happen actually how many if we can imagine it ok how many of you get it now this is what will happen right so you will get a sigmoid function such that these two points lie on that fair ok and that exactly means that when i plug in this value i will get this value and when i plug in this value i will get this value right so that is what it means so let us see this in more detail and now what we will do is our quest is for this w star and b star i will try to find this manually i will do some random guesswork and try to find this because i do not have any clear principle algorithm for finding it as of now so i will just use some guesswork so i will give my initial guesswork as w equal to zero5 b equal to zero for no reason i just picked up some values right and this is what the function that i got what does this mean this function an error so the sigmoid formula should be here we should have this sigmoid formula here so is this a good are you happy with the solution if i give you are you happy with this solution is this good bad ugly has to be something bad we will not call it ugly ok so why is it bad it is not passing through those points i will ask you a question how bad is it can you assign a number to it we are always good at qualitative stuff but quantitatively can you tell me a number how bad is this can you tell me a way of finding how bad this is i already told you in detail how to find that how bad it is the loss function right we have the loss function let us see that again and see if we can find out how bad this is so this is what my loss function is ok and i have two data points i will just expand it out fine now i will plug in the values i know this is zero9 and i will compute the value of f two5 i will plug in this and i will plug in this ok and this is what i get so this is how bad it is what did we actually expect it to be in the good case zero so this is not zero this is zerozero73 so now we have a quantitative handle on how bad this is ok so let us keep this in mind and let us try to continue guessing so we want the loss function to be asked close to zero as possible we are not there yet so then i make a different guess i say let me try minus zeroonezero zerozerozero what happened now is it now good bad ugly  now let us call it ugly right so it is worse and how do i know it is worse because i plugged it in to the loss function and i got a value which is greater than the value at which i was so i clearly know this is bad so now this is how my mind is working right oh i as far as w was positive things looked at least i was close to zero in the first decimal now when i made it negative that does not look good so let me just keep it positive and keep increasing it right so i saw zero9four and i also tweaked the b of it i have done complete random guesswork right now what happened good bad ugly  better ok now what will you do what would your next case would be make w even more positive perhaps that would help and be even more negative and so on i can continue in this manner and actually get close very close to the solution so i can do this guesswork and find these values but it is still an educated guess right i am not guessing in the dark this is what is helping me drive towards those guesses and i am just looking at these values and making an educated guess right and that is the educated guess which i took that probably making w even more positive would help but this is still brute force in a sense right this is not something that you would want to do when you have onezerozero onezerozerozero parameters and so on right and one million data points and so on so let us look at something which is better than our guesswork algorithm ah so we are not there yet actually on the next slide i am still going to talk about the guesswork algorithm and eventually we will get to something which is better than the guesswork that ok so since we have only two points and two parameters what i can do is i can take all possible values of w and b right that is what i was trying i was picking up some values of w and b why just pick some values of w and b i will pick all possible values of w comma b right and i will fix the range i cannot fix pick it from minus infinity to infinity but i will pick a range i will say from minus six to six let me try all values of w comma b compute the loss and plot it right let me tell something about this error function because this is going to stay with us for quite some time so what you see here is something like a flying carpet this is colour coded red is bad red are the places where the error is high blue is good blue are the places where the error is low darker the shade of blue lower the error darker the shade of red higher the error so in particular if i look at this point what has happened is i have taken the corresponding value of w comma b right which is say minus four comma minus one right something like that i have plugged that value into my loss function and i got this as the loss function this has the loss value and that is what i have plotted for all values between minus six to plus six and minus six to plus six for w and b so everyone understands how i have constructed this error surface now this of course becomes and now what i can do is once i see this error surface i know how good this is the point where i need to be this is the darkest ah shade and this is where the error is the lowest so i can just pick a w comma b value which lies there this is fine for this toy example where you just have two parameters but this becomes untractable once you have more data points and many more parameters and that is what happens in most real world applications right so this is not a feasible way of going about things right and here again note that i have only taken the range from minus six to six i do not even know what will happen if i have to look at all values of w comma b right maybe there was something outside here right which was even more lower error or something right so i do not really know that so i cannot really use this so i need something better than this plotting the error everywhere and finding it order that is pure brute force or surrogate to this was the guesswork algorithm but which is again something we cannot do for if you have large number of parameters so everyone gets this that this is a way of finding the solution but this is not feasible right that is the only point i am trying to make and we look at the geometric interpretation of what was actually happening in the case of the guesswork algorithm with respect to the error surface so i had chosen some values of w comma b the first value that i chose actually gave me an error of if you remember it was some zerozero73 or something like that right so that is the point then i decided to take a very random guess and my error actually increased so you see that i am actually climbing up on this error surface i have gone from a slightly darker shade of blue to a lighter shade of blue right and then i corrected myself and then kept moving in a direction where i was going towards the darker and darker shades of blue so what i was actually doing is i was trying to traverse the error surface and land up in the good regions which were the dark blue regions now what i want to do is i want an algorithm which will allow me to do this in a principled manner which is neither brute force nor guesswork so that is what we learn in that module 
task2/super_cleaned_audios/lesson84.wav,489.2639,now we come to this important part about how do you evaluate word representations  so there are different tasks that are set up i hope some of you have read that paper and i  can see that none of you have read that paper so semantic relatedness is one way of  evaluating word representations    so ask humans to judge the relatedness between a pair of words so i construct some  pairs of words and i show them to a human and ask them how related do you think  they are on a scale of one zero to one so it is likely for cat and dog someone would say zero8 or  at least you would expect values greater than zero6  now you have learned the representations using your model it could be any of the  models that we have seen so far continuous bag of words skip gram or glove vectors  so these are the three things right continuous bag of words skip gram which is known  as wordtwovec and the glove representations and within them you could have this  hierarchical softmax and other things and so on  so you could if i asked you what is the similarity between cat and dog according to  your word representations you could just use the cosine similarity and tell me that this  is the representation right so now i will have many search words w one w two for which i  have the human judgment and i have the model judgment right so i will have w one one w two  one then w two one sorry one two and so on i will have many such word pairs for each of these  word pairs i would have the human judgments and i would have the model judgments  right how close do the humans think they are and how close do the think they are  now i can compute the correlation between these two decisions or these two random  variables and i would want that for a good model this correlation should be high so  whenever humans said that the two words are actually similar the models word vectors  should also predict a high cosine similarity and whenever humans said that the two words  are not similar the models word vector should also result in a low cosine similarity how  many forget this    so that is one way of evaluating how good your word representations are right so as i  was saying earlier how do you tune those parameters so you could have such a set  once you have learned some word representations and you want to see whether  parameter k one was better than sorry rather hyper parameter k one was better than hyper  parameter k two you could just take those two word representations learned by these two  different hyper parameter settings evaluate them on this corpus and whichever gives a  higher correlation you can keep that hyper parameter how many of you get that    other task is synonym detection so from a resource known as word net or from other  dictionaries you could get all the synonyms of a word so then people create a corpus  where you give us in sin a word and give four candidates or some k candidates out of  which one of these is the correct synonym the others are just distraction words right and  distracting words now what would you expect your word representations to do you  have word representations for all of these what would you want how would you pick  up the synonym based on word representations  students refer time zerothreeonethree  the one which has the highest cosine similarity so again you will compute the cosine  similarity you will rank these and you will pick up the synonym right and now again i  gave you onezerozero such instances i gave you a word for candidates and i gave you onezerozero such  different word comma candidate pairs and you pick the synonym for everyone and see  for 6zero of them you got it right then your accuracy 6zero percent so that tells you how good  your word representations are  again if you are given two different hyper parameter settings one gives you 6zero percent  accurate the other gives you 7zero percent accurate you will probably go with the one  which gives you 7zero percent accurate they are find how you can use this     the third is analogy task    where you find the nearest neighbor of this operation what should it be granddaughter  this is this analogy teller brother is to sister as grandson is to something right so now  the idea here is that if i mean it is like pretty weird right so if i take brother minus  sister i get something  now if i add grandson to that then i should get granddaughter it is intuitive in a way  right i mean this is what you expect your word vectors to do right so that is how the  analogy task works so you could set up an analogy task you could have and you could  get several such an analogy tasks from online tests and so on and you would want your  word representations to exhibit this kind of a behavior right  so again you have these onezerozero analogy tasks for each of these you know the true answer  and from each of these you predict the answer from your word representations and first  see for how many of them you get it correct then you could also have a syntactic  analogy so you can tell me what this would be right in fact here again it should be the  other way round we works minus we work thus we speak would be we speaks right  so that is the syntactic thing right  so you are getting a different form of the world so your word representations should  also have this kind of properties that is what you desire so just evaluating whether your  word representations show this kind of a property or not so we have seen three tasks  one is semantic relatedness whether a pair of words how do humans rank it and how do  the model how does the model rank it then the synonymous detection and the analogy  tasks in each of these you do something with the word representations in the first two  you use the dot product and this last one you use some arithmetic operation over the  word representations  so you would want v brother minus v sister is equal to v grandson minus v  granddaughter right so v granddaughter right so that is there is a plus minus error  there     so now which algorithm gives the best result right so whenever we see a bunch of  algorithms same as we did with adam and refer time zero559 and so on we always  want to answer this question which of these gives the best result   so there was this study done by boroni et al in twozeroonefour that show that the predict based  models right which are either which are the predict based models actually skip gram  continuous bag of words and even glove for that matter right because it is also a predict  based model these continuously or consistently outperform count based models that is  what they said but a year later there was a separate study done by someone and in my  opinion this was a more thorough analysis because the earlier study right they did not  really give svd a chance to win in my opinion this is all on camera  but the later the second set of guy right they gave svd a chance to win so i will tell  you one example of how they gave is really a chance to win so remember in wordtwovec  you had this weird three by four which you are using to raise the probability right now what  they did is they said even in the cooccurrence matrix actually these counts that you  have if here you are using them by three by four in the case of wordtwovec and getting better  results why not do the same thing in the cooccurrence matrix also  at the end of the day you are raising the count to three by four right so whatever counts you  have here based on that you will compute ppmi or pmi or whatever but first why not try  to adjust these counts so why not have a parameter k such that you can raise the counts  to this parameter and then do all those computation and that is fair because the  wordtwovec has a parameter hyper parameter so why not give a similar hyper parameter  to svd  similarly they did something to take care of the k negative samples which wordtwovec  has why not give svd also similar chance right so when they did these kind of  adjustments they found that after these modifications svd does as well as or even better  than wordtwovec models for the similarity tasks but not for the analogy task but the  analogy task was the last task right brothers to sister’s grandsons to grandmother right  so in most cases we care about similarity and in very few cases we care about analogy  if you are doing nlp application so that means in most cases svd would just be fine  so that is what i just said at the beginning   
task2/super_cleaned_audios/lesson90.wav,1206.7784,so now we will go to the next module we will talk about some success stories on  imagenet right so this is the challenge which actually made convolutional neural  networks very famous back in two thousand and twelve   so they are going to look at some algorithms in fact two more hopefully today    so this is the story right so there is this challenge or competition called imagenet  large scale visual recognition competition right that is what ilsvrc stands for and  this was a data set created which had one thousand categories it actually has one thousandzero categories  but in the competition we use only thousand of those categories yes   so one thousand plus one thousand i think the roughly the data set size is one million and so that is what  was used for training a classifier and i am talking about twozeroonezero the pre deep era right i  mean so of course deep era networks existed at that time but the participants and these  challenges and that time were relying on the classical machine learning approaches so  what was that approach take the image   student refer time zerooneonesix feature     extract features which features  student refer time zeroonetwozero  predominantly  student sift and hawk  sift and hawk features were the predominant features at that time and then you train a  classifier on top of that and then you use things like on symbols or better handcrafted  features and things like that certain more tricks on top of that so that with that on this  data in twozeroonezero the error was twoeighttwo percent that means if i give you a test set of one thousand  images you will make twoeighttwo errors on that right that is what this means then in oneone there  was still some progress this was again pre deep era and there was this error came down  to twofiveeight and then in two thousand and twelve there was this alexnet which was a deep convolutional  neural network applied to the task of image classification and it gave a dramatic  reduction right from twofiveeight to onesixfour   and was i think absolute in absolute terms eight to nine eight to nine percentage better than the best  system in that competition that year ok so that was in two thousand and twelve in twozeroonethree there was further  improvement on a different architecture for doing this and that give a further error  reduction of oneoneseven then in twozeroonefour there was vgg net so these are all three that we are going  to see today which give a further error reduction of seventhree then google decided to join  the party and they make it sixty-seven and as i have said before then afterwards microsoft got  crazy and they brought it out in threefiveseven and this is when we started making claims that a  convolutional neural network has become better at this task than humans right   because if you show these one thousand images to a human even a human is bound to make a  threefive percent more than threefive percent error that means because some of these images would  be blurred so i would not be very sure whether this is a bulldog or a different type of  dog or something like that right so i even a human cannot really recognize it correctly  and that is the whole hype around how convolutional neural networks have beating  human level performance on this particular task right   and let us see so this was all the shallow pre deep era the first architecture was eight  layers and i think this was called a varied no this probably not this yeah the second  architecture was also eight layers then we had onenine then twotwo and then onefivetwo right thats how the  progress has happened so these are all the architectures that we are going to look at  today or at least some of them today and the rest maybe tomorrow   ok so we will start with alexnet and i am going to tell you the exact architecture of  alexnet what was refused what did it actually use     so the input was an rgb image so it had a depth of three and it was twotwoseven  twotwoseven that is  what the data set input was all the images in the data set were to twotwoseven  twotwoseven cross three so  the first thing that they did was they decided to use ninesix filters can you read that  anyways i will say it outright   so they resided to use ninesix filters with a spatial extent of oneone cross oneone a size of four and  padding of zero ok so the moment you see size a stride of four what do you know is going  to happen there is going to be some shrinkage roughly by how much onefourth  right so now can you compute these three things what was wtwo what was htwo and what  was the number of parameters in this layer we will do it for a few of these layers and  then i will just rush through that so whats wtwo going to be you have already done this  computation right the exercise that we did was exactly this computation so there was  fifty-five cross fifty-five and what is the depth going to be i want everyone to say that  student refer time zerofiveonefive  ok and what is the number of parameters  student refer time zerofivetwozero  ninesix into  student oneone  oneone into  student oneone  oneone into three don’t forget the depth the depth is three here     so that is the number of parameters that they had in this layer ok oneone into oneone into three into  ninesix now what is the next layer going to be a max pooling layer     ok so they had a max pooling layer where they used a three cross three max pooling that  means you are going to pick up max from a three cross three grid and the stride was two that  means we are going to get half the output and now can you tell me what wtwo htwo would  be roughly half of fifty-five fifty-five right so twoseven twoseven     and what is the number of parameter is going to be don’t be lazy everyone be say it   student zero  zero right so that is the max pooling layer   now what is the size of your input volume at this point  student twoseven  twoseven cross twoseven cross  student ninesix  ninesix as opposed to the original input which was twotwoseven cross twotwoseven cross three so as you keep  progressing your width and height is decreasing but your depth is increasing because  you are using more and more filters to capture more and more patterns in the images     now so you have twoseven  twoseven  ninesix then they decided to use twofivesix filters each of size five  five  with a stride of one and padding of zero ok is it right so how many parameters do you have  now  student refer time zerosixfivethree  twofivesix into   student five into five  five into five into  student ninesix  ninesix    so thats the number of parameters that you will have and the size since would decrease  only by one right because you have a stride of it will decrease by two because a filter size is  five and you have a stride of five ok    so these are the number of parameters we had zerosix million parameters in this layer  what is the next layer going to be pooling     so you do a max pooling again you do a three  three you do a stride of two so your width in  height is going to decrease the depth does not change remember in max pooling the  depth does not change because the max pooling operation is per feature map it is not  across the depth fine then use a three  three filter and threeeightfour of those     so how many parameters would you have  student refer time zerosevenfourthree  threeeightfour into three cross three into twofivesix the depth   so now you guys get it so i will not bore you anymore      and then you have a convolution operation again which is a threeeightfour convolutions each of  size three  three and so many parameters followed by a convolution operation again followed  by a max pooling operation then followed by a fully connected layer    so what would i do to this twofivesix  two  two i will fatten it so i will get what dimensional  output  student onezerotwofour  onezerotwofour twofivesix into two into two so this onezerotwofour dimensional vector i am going to fully connect it to  a fourzeroninesix dimensional vector how many parameters four million right four into onezero raise to six  right so roughly four million      then you have another four million another fourzeroninesix vector fully connected how many  parameters   student onesix million  onesix million then you have the one thousand classes that you are interested in right so again  fully connected so you get the full architecture anyone has any questions no one  wants to know why this particular configuration among all the possible configurations  why not onezero layers why not first eight cross eight filters why not nine cross nine filters  unfortunately no one knows laughing  student refer time zeroeightfivenine  so this i mean see this what this is what would happen right now we get into something  known as hyper parameter tuning right so what are the hyper parameters in this  network the kernel size is and the number of filters right so you would have tried a  lot of these things evaluated on the validation set seen which one gives the best  accuracy and then chosen right so that is probably what would have happened but  there is not enough insight into how this particular architecture came up   apart from some things right that three curves three neighborhood sounds reasonable initially  when you have the full image you use larger filter sizes because you want to capture a  lot of things there but once the image has shrunken you use smaller filter sizes so those  are some rational decisions which look reasonable but why these three convolutional filter  layers back to back instead of convolution max pooling convolution max pooling and so  on right  so the some of those things are not clear so just in case you are wondering do not  wonder this is just the architecture this is known as modestly named as alexnet so  that is laughing yeah and so i said that this has eight layers but you clearly see more than  eight layers here so why did i say that has eight layers which are the layers we are not  counting  student refer time onezeroonezero  why   student refer time onezeroonetwo  because they have no parameter right so when you count the number of layers you only  count those layers which have parameters so you have five convolutions and three fully  connected layers     then so the total number of parameters in this network is twosevenfifty-five million parameters and  ok at this point i will and obviously you notice that most of these parameters were there  in the fully connected layer so you had four million here then onesix million here and then  again four million here right   so roughly twofour million of the twoseven million parameters were there in the fully connected  layer you see that skew in the number of parameters ok    and i will just look at the fully connected layer again so the last max pooling layer  actually gave you a twofivesix cross two cross two output you just flatten it to get a onezerotwofour  dimensional vector and then you connected fully to the fourzeroninesix vector right so that is what  i mean by a fully connected layer why do you move max fully   so the reason for that is basically to shrink the size of the image right because after that  if if you keep working with this size right then the number of parameters is going to  really blow up a by using a larger stripe yeah both of them are feasible right so now  see from here remember that we had the original image sizes twotwoseven cross twotwoseven and by the  end we were just left with two cross two  and then adding a fully connected layer on that makes sense right if i had not done this  shrinkage throughout either by increasing the stride of the convolution layer or by doing  max pooling right then you would have left with something of the order of twozerozero cross  twozerozero here and then you have to do a fully connected on top of that is just infeasible right  it just throws away all the hard work that you have done by doing weight sharing and  sparse connectivity right so that is not feasible   there are also papers with say which i think it is titled fully convolutional neural  network which does not have any max pooling layers and they show that that also works  fine in fact when we see vgg net we will see that it has back to back convolution  layers and very few max fully layer right so these are all things which people have  trained   not so many years two years the challenge came out in twozeroonezero and in two thousand and twelve this was used  right so it is like not really a large gap right and if you read the original paper they  had to do a lot of tricks to actually make this work it was not as simple as i am showing  it of course now with all the stability which comes from these platforms tensor flow  pytorch you can probably just go and implement this as it is and you should be able to  reproduce the results but six years back that was not the case there was a lot of hard  work involved in getting this too work and they this was also the paper which  introduced the relu nonlinearity in the context of convolutional neural networks right  so they had to change from sigmoid or tan edge to relu   a lot of these small small things which they had done and at that time it is also not  possible with the existing hardware to train this on the given gpus that you had at that  time so they had to do some splitting across gpus and so on so it was not as simple  as it is today with all the hardware as well as api developments or platform  developments around this right so probably that is why it took two years to yeah sure   so each of these things so after you do the convolution operation you pass it through  the relu nonlinearity ok so what does that mean is that the convolution operation  gave you a feature map every entry here was just a weighted average of the neighbors  right you take this entry or rather you take this feature map and create a new feature map  where every entry here is the sigmoid of every entry here do you get that or not sorry  sigmoid some nonlinearity and they use the relu has the nonlinearity so you do get  everyone gets this so all the convolution layers are followed by a relu nonlinearity  layer  so you get this volume pass it through the relu and get a new volume but i have just  shown that as a single operation it is before pulling so this was the fully connected layer  so now we look at the next architecture which is zfnet    now i am going to compare zfnet with alexnet so on the top you will see alexnet  on the bottom you will see zfnet ok so again the input was the same twotwoseven  twotwoseven  three  now instead of oneone cross oneone filters zfnet decided to use seven  seven filters and their rationale  was that you do not need such large neighborhoods you do not need as small as three  three  but probably you need at least as much as seven  seven you do not need oneone  oneone so that is the  first change that they did and that would also result in some parameter pruning right  because the number of parameters now would be seven cross seven into three so the difference in  the number of parameters at this layer for zfnet which is at the bottom and alexnet  which is at the top would be this how many of you get this ok so thats in the  difference in the number of parameters so now the output volume still remains the  same its fifty-five  fifty-five  ninesix     then again they had the same max pooling operation this layer there was no difference  between zfnet and alexnet and then after that you had again layer three which was  exactly the same as alexnet     then layer four again the same as zfnet afterwards layer five instead of threeeightfour filters they  decided to use fiveonetwo filters the rest of the thing remains the same that means the size or  the spatial extent of the filter remains the same that again results in some difference in  the parameters so thats the number of parameters that got added in zfnet as opposed to  alex net   and of course the  oh sorry sorry oh sorry the bottom one is a zfnet yeah that is  correct sorry so in zfnet you had fiveonetwo filters as opposed to threeeightfour filters in alex net ok  is it fine     and then the next layer again instead of threeeightfour filters they had onezerotwofour filters     then again instead of twofivesix they had fiveonetwo filters and then a max pooling layer then the  same dense fully connected layers ok    so everyone gets this this is the difference between the two architectures and this led  to that difference in the error of around three to four percent is that we have seen earlier    so difference the total number of parameters was onefourfive million and of course zfnet had  more parameters because is that it has these more filters in the deeper layers ok so we  go to the last point which is may be more in that vgg net     so again in the case of vgg net the input was ok so i just want to i will not see it  refer time onesixfourone in so the input was again the same it was rgb cross twotwoseven cross  twotwoseven   so this is what the vgg architecture looks like they have so in vgg network  throughout ok wait so how many layers this zfnet have eight so you only count the  pink boxes because the those has ones which have two parameters now vgg net has  slightly more number of layers but in all the convolution layers they use three cross three filters  right from the beginning they use three cross three filters ok so you have the first  convolution layer then another convolution layer another convolution another max  pooling layer followed by two convolution layers then a max pooling layer followed by three  convolution layers max pooling just keep adding box is writing just because you can  and then you have the fully connected layers   so again there is not much intuition for why onesix in fact later on someone came if this is  the vgg onesix architecture because it says onesix layers later on some of someone came up  with the vgg onenine architecture which has onenine layers right so a lot of this is data center  even right so you try your best to get the best possible accuracy on the imagenet data  and that is the architecture you came up with right   but as long as how many of few feel comfortable with what is happening right and i  mean when i say comfortable i mean that you really understand the gory details of  what is happening at each layer in terms of input volumes output volumes number of  parameters how are you going to train this network end to end can you see how are you  going to train this so you will get some loss here that is going to propagate all the way  back to the first layer right and this propagation is going to happen over some sparse  connections that fine now this is one very important point that i have skipped and  which none of you is questioning is everything that is happening here differentiable  student refer time oneeightthreeeight  what happens to max pooling is max pooling or differentiable operation so i am  going to ask you this how are you just note this down how are you going to back  propagate to the max pooling layer because you need to see whether the max pooling  layer is actually a differentiable layer or not  so here i just some statistics about vgg  net everyone is writing that down laughing this perhaps means i will not ask it   the kernel size is three cross three throughout the total number of parameters in non fully  connected layers is onesix million the total number of parameters in fully connected layers  is onetwotwo million so you see that this fully connected layer is really a problem it like really  hogs all the lime that it has the maximum number of parameters there right and so and  the most number of parameters are there in the first fully connected layer because you  have this fiveonetwo  seven  seven you remember then alex net and zfnet the last layer was twofivesix   two  two which has definitely more manageable than this layer which has grown almost eight  times in size but not even eight actually four into four into two right onesix threetwo times in size right  so that is really blown up the number of parameters in the first fully connected layer  so you just imagine the i mean you have such a deep layer and then you realize that all  the main number of parameters are there in this one particular layer everything else is  much fewer parameters or orders of parameters less number of parameter is less then  this one fully connected layer   
task2/super_cleaned_audios/lesson47.wav,200.3429,now you go to the third interpretation where we will try to say something about the  variance  so we started off with the following the wish list that we wanted low covariance and we  wanted high variance so far we have paid attention to the covariance because  everything was revolving around this covariance matrix in both the solutions but what  about variance have we achieved the goal with respect to high variance    so let us see so what is the i’th dimension of the transformed data it is this you take  your data and project it onto the i’th dimension right so x hat is equal to x into pi now  what is the variance along this dimension how do you compute the variance  so this is  my projected data and let me just call it x hat i  so this is the i’th column after projection is that fine everyone is ok with this now for  this i’th column i want to compute the variance how will i do that remember that the  data is zero mean what is the formula actually it is going to be x hat i minus mu i into  x hat i minus mu i right but mu i is zero so it just turns out to be the dot product dot  product of x i hat with itself ok and of course divided by m is this fine    i can write this as x p i and then when i take the transpose i will get this ok now what is  this quantity this is exactly the moment where i feel like saying fl what is this  quantity  student refer time zero2zerozero  no look at the circle what is x transpose x times p i  student refer time zero2zero8  what is p i with respect to x transpose x  student eigenvector eigenvector  eigenvector so what is this product going to be  student lambda refer time zero2one4  lambda i p i is that fine what is p i transpose p i  student one  one ok so what is actually the variance along the i’th dimension  student lambda refer time zero226  what is lambda i  student eigenvalue  so what will happen if i retain the highest eigenvalues  student refer time zero2threethree   i will get the highest variance dimensions right fine so all roads lead to  student refer time zero2three9  eigenvectors eigenvalues right  so andrew ng in one of his lecture says that there are  onezero different interpretations of pca i only know three of these i do not know the remaining  seven maybe he was bluffing so that people like us can keep busy oh this is getting recorded  so yeah so you get this so we have satisfied everything in our wish list variance  covariance and also did this detour where we saw that it actually amounts to minimizing  the error in reconstruction where we are throwing away the dimensions along which  reconstruction did not add much to our knowledge about the data so these are the three  different interpretations that i have right so hence we did the right thing by throwing  away those dimensions which correspond to the lowest eigenvalues because lowest  eigenvalues is nothing the lowest variance also    so this is the quick summary the covariance between the new dimensions you can  leave actually those you can just read it later on  
task2/super_cleaned_audios/lesson53.wav,690.1542,then we will go to the next module where we will talk about regularization in auto  encoders and we will talk about a motivation for doing that  so poor generalization so why do we need a regularization people have done the  machine learning course or any equivalent course why do we need regularization to  avoid  student or enable  or enable generalization right now in the case of an over complete auto encoder what  is likely overfitting is likely why is it so what does what do you mean when you see  generalization actually can you talk in terms of training time test time and so on  so generalization is essentially that your are training so remember that at training time  you are trying to solve an optimization problem where you are looking only at the  training data so it is quite likely that you will drive the error to zero for the training data  that means you have learnt perfectly everything for the training data right but now it is  also possible that when i give you a new test instance which you had not seen during  training that means you had not seen instance while doing the optimization that means  this instance did not contribute to your loss function  then it is very lightly that when i gave this instance then you would get a non zero loss or a  loss much higher then what you get for your training data does that make sense that is  what over fitting is and it leads to less generalization your model should have generalize  to unseen data but it cannot do this one typical situation where over or where  generalization happens is if you have a dash number of parameters now what did i ask  actually  student generalization  no ok if a case where a over fitting would happen is when you have a dash number of  parameters  student large number of  large number of parameters right now do you see why i am saying this what is there  on the slide an over complete auto encoder what would it have  student a large number  a large number of parameters so what could it do  student overfitting  over fitting what do we do to avoid over fitting  student regularization  regularization so that is why we need regularization i have still no told you why do we  need an over complete auto encoder ok still that is an random variable i still need to  decide but can this happen in an under complete auto encoder also it can right because  under complete auto encoder just says that your k is less than n it does not say how  much less it is right so it is it is still have and depending on a data that you are trying to  model it could still have a large number of parameters    so for example let us take an example for the under complete case suppose you are  doing image classification where you have a digit three at the center of the image ok and a  lot of these are white spaces so what is the dimension and suppose this is a onezerozero cross  onezerozero image what is the dimension of this image input how many if you cannot multiply  onezerozero into onezerozero  student onezero  onezero k right of this a lot of data is not important so my n is onezero k and at least by this thing  that i have drawn it looks like probably only twozero percent of that is what actually captures  the digit but now if i choose k to be equal to onezerozerozero it might still be large for this  application so i am using an under compete auto encoder but it could still be a  situation that my under complete is still having a large number of parameters all of get  this intuition  it is a very weird example but still really do you get the intuition you could have a very  high dimensional input and you might think you are shrinking it a lot but there is so  much redundancy in your input that even that shrinking still leads to a large number of  parameters and you could still over fit therefore even for an under complete auto  encoder you could still need over a regularization  so fine so that was the motivation since the over complete case of course the model  can simply learn to copy we have seen that and that is why we need to introduce  generalization fine now what is the simplest sorry we need to introduce a  regularization what is the simplest regularization technique that you know that is not  the simplest l two regularization  and you see why i say that is the simplest we can take the derivative for those of you do  not get it do not worry we will get to it or if you do not get to it do not worry    sook the simplest solution is to add the l two regularization term to the objective function  so this was my objective function i wanted to minimize the squared error loss i have  added a term to this what does this term do what does it doing first of all tell me  what is this quantity theta is a  student all  all the parameters that you have right and i am assuming that they have just put it into  large vector i am taking the ltwo norm of that vector so even you though you have those  matrices and just flattening them all out and putting them into a large vector called theta  right so what is happening here i am not allowing my weights to shrink or grow  grow because if my weights are very large what would happen  student grow grow  this quantity would grow so then i cannot really minimize this minimize this as  effectively as i want right why this makes sense how many of you why this makes  sense so i am now why am i not preventing the weights to go to zero ok so we will see  this in more detail in the next lecture this is again a basic lecture on bias variance and  regularization and so on so we will try to arrive at a more reasonable answer for this  for now just see that i am putting some constraints and the weights  so effectively and i am doing gradient descent i am not allowing the weights to take  very large values i am trying to restrict them to a certain area so i am not allowing to it  to explore the entire w comma b plane but trying to restrict it to smaller values of w  comma v how many of you get this intuitive explanation  so in other words what i am trying to do is that i am not giving it in a freedom so that it  can completely drive the error on the training data to zero and my hope is that if i do not  do this if i do not allow it to completely memorize a training data then it should  generalize well on the test data is that intuitive fine ok now i have changed the loss  function again i have the square i have told you how to do it for squared error loss for  the cross entropy loss and so on but now i have changed the a loss function again so  again i need to teach you back propagation no what will change now again i need to  derive with respect to the last layer  what is the minimalistic change that is going to happen now just tell me this theta is  actually w one w two and so on right just assume all the parameters just flattened out into a  vector fine and now tell me what is dou l theta by dou w one going to be or let us  simplify things let us call this l theta and let us call this omega theta let us call this l  dash theta and then your l theta is the combination of these two terms so this derivative is  going to be a sum of two derivatives out of that one you already know what is the second  student two times lambda  two times lambda w one so it is a very simply change to your gradient descent update  rule how many of you see that whatever update you will had just add minus two lambda  w one to that ok should have been two lambda w but of course you do a half here so it is  fine is it      another trick which is typically used at least in the context of auto encoders is to tie the  weights of the encoder and the decoder how does that help what does tying the  weights mean now i appreciate what you are trying to say so one we have doing this  is just say w star is equal to w transpose you will enforce that you actually have only  one matrix w and here you are using w transpose mathematically does that make  sense all your operations go through because this is going to be n cross k and this is  going to be k cross n  so whatever effectively done i have reduce the number of parameters in my network  right i am enforcing i am forcing this upon the network that i am not going to give you  two sets of weights you just learn the w’s in a way such that when you use w transpose  you should be able to reconstruct this how many of you get this not many ok please  ask me doubts if you do not there is nothing very  student why is it w transpose  why is it w transpose because otherwise  student refer time zero85two claim that  how can you claim that that would work because you have no linearity’s in between  right no w inverse would not work what is the simplest thing to do why would you  want to compute an inverse that is an interesting question how would you implement  this how would you if there are multiple paths from a weight to the output how do you  compute the gradient sum it across all those paths what is happening here how many  paths to there exist from the weight to the output one is this direct path and then the  other is another this path also so you just sum it across these two paths do you get that  how many of you do not get that how many of you do not get that ok  so if this was w star you did not have a problem you could just have computed dou l  by dou w star and dou l by dou w now think of it as this right that you have this this is  one path w w to the output ok and now the gradient is just going to be sum across  these two parts one path is the single path and the other path is the double path so it is  just going to be a sum across these two paths oh no no so you just have one matrix w  which are going to update you do not have two matrices you just have one matrix w at one  place you are using w the other place you are using w transpose but just look at it  element wise right do not try to look at it in the terms of matrices  so you have n cross k elements here w one one to w n k right you have to computing the  partial derivative with respect to each of these and every time they are considering all  possible paths to the output and that value is getting updated right and at one place you  are using a particular arrangement of these w’s at the other place you are using a  different arrangement of those w’s that so it will just remain the same right is that ok  student no  no this is for regularization right so we are reducing the number of parameters by  half  student refer time onezero5one  yes  student refer time onezero5two  no that i mean that also has but that is not be the objective we are trying to do  regularization how many of you have lost at this point please ask me if you have  questions really i do not mind answering but if you just give me blank spaces i cannot  read them so this is used at quite a few places where you tie some weights right so that  so effectively you are saying that learn it in such a way that it works at both the places  and you are reducing the number of parameters so weight tying is something which is  very commonly used for regularization in the context of neural networks  so that is where we will end the motivation part and it is too very simple ways of doing  regularization one is the standard known trick which is to use ltwo regularization and the  other one was something special that we saw which was tying the weights you all have a  lot of doubts about tying the weights  
task2/super_cleaned_audios/lesson52.wav,1004.9245,so we will move to the next module where i would like to show you a link between  pca and auto encoders  so this is what i am trying to show you that under certain conditions pca is or rather  an auto encoder is equivalent to a pca and the conditions are if you use a linear  encoder if you use a linear decoder if you use a squared error loss function and if you  normalize the inputs to this so for the time being just ignore the last bullet let us look  at the other three bullets using squared error loss functions   so remember i gave you different choices right you could have used the cross entropy  or the squared error loss but i am going to prove this equivalence only under the  condition when we have the squared error loss what do i mean the u encoder is a linear  encoder g is a linear function we are not using a sigmoid or any logistic or anything  like that and linear decoder again the same thing we are not using the sigmoid or soft  max or anything at the output it is a linear function under these conditions i will show  that or i will try to show you that pcas equal auto encoders equal to pca   what does this mean actually now what do i mean by it is equivalent what do i have  to show you actually how many of you understand what i am trying to prove how  many of you can mathematically define it ok so we will try to make this clear over the  next fifteen minutes    first let us look at the last condition right which i ignored ok i always anticipate all this  right so i have full faith in you guys ok what is this mean now what i am doing  centering the data and i am also doing one by square root of m why mean as the  standard deviation  so the operation in the bracket ensures that your data now has become zero centered right  it is a zero mean and now let x dash be this matrix this one right such that all it is  elements are zero mean is this still a flicker again alright  so let i am calling x dash as this matrix ok so this matrix where i also have one by  square root of m i can write it as everyone gets this is simple now do you see where  this is headed what would x transpose x be covariance matrix so i needed that one by  m right at the out  so now this is the covariance matrix so if i do this normalization to the original data  and then if i take let x dash be that quantity and then if i take x transpose x then i will get  the co variance matrix everyone gets this that i did this to get the co variance matrix so  that i mean i did this so that when i take x transpose x i get the co variance matrix after  this normalization only it will be the covariance matrix    so first we will show that if we use the linear encoder decoder and a squared error loss  function then the optimal solution to the following objective function what does this  objective function  student squared error  squared error loss is obtained when we use a linear encoder do you understand the  implication of this what does being stated here ok so i have fixed the decoder i  have said that the decoder is going to be a decoder i have fixed the encoder or i have  fixed the loss function this is going to be a squared error loss function this is given to  me now under these conditions i am trying to minimize this loss function ok  then i am telling you that the only solution to this is that the function dash should be a  linear function which function the function g should be a linear function you cannot  choose sigmoid or logistic or anything else right the optimal solution will occur when g  is a linear function everyone gets what is being stated here    so this summation that i have written right or in fact this the entire objective that i  have written is actually equivalent to this objective is this fine with everyone even  though i have not defined what h is just fine with everyone so we had   x which was x  one to x m ok i had picked one of these xi’s what is the dimension of this  student one cross  one cross m and then i had multiplied it by a weight matrix w not w star remember that  what do the dimension of w  student n n  n cross k and what will i get as the output  student refer time zero45eight  i got an h which was one cross k what did i do this  student multiply it by  multiply it by  student w star  w star which was k cross m and what did i get as the output  student x hat  x hat which was one cross n right so what i am telling you is that i could do this together  for all these x is i could do this operation at one go and i can call this as x matrix and  what will i get here h one to h two to h m and i can call it as the h matrix and i multiply it  by w star and what do i get x cap ok is that fine ok but without defining these things  also it was fine so it does not matter    so now how many of you get that this quantity is the same as this quantity now how  do i explain this was the frobenius norm of a matrix some of the squares of the  elements  now what is the matrix x it is the x one one up to x one n and x m one up to x m n and all  elements in between right what is the matrix h w star we just did that the same thing  expect that it is x hat  student refer time zero6zero4  i take the difference between these two what do i get  every element of that matrix is  equal to this quantity that i have underlined right so i get a new matrix such that every  element of that matrix is equal to this quantity is that fine now i am taking the square  of every element of that matrix and adding them up what is that equal to  student refer time zero6fifteen  a frobenius norm how many of you get that now almost everyone ok so this is  equivalent to the frobenius norm ok now where have you seen the frobenius norm  before what did we show in the svd theorem  let us try to connect things right if you do not learn how to connect things it is going to  be very difficult what is this x hat it is a dash of x  student reconstruction  reconstruction it is a dash of x approximation what is the solution to this optimization  problem what is the solution to this optimization problem i shall started off with the  answer that we saw this in the svd theorem and then i asked you a question what threezero  hours threetwo hours not even threetwo hours are passed since we did this come on what is the  solution to this no no that is fine  but what is the solution x hat is equal to what the best approximation to x is given by   what is it fine yeah so some k yeah but it is going to come from the svd theorem  right is that fine it depends on what rank approximation you want but it the best  approximation to this is going to be given by the svd of x is it ok everyone gets that  yes forgot about it but now do you remember it all those extra lectures eight’o clock in the  morning    so that means h w star should be equivalent to this that we know from the svd  theorem that the optimal solution is going to be given by svd so if i just compare  terms ok then i could write that one solution is this that h h is equal to u into sigma  and w star is equal to v transpose i could have chosen the other solution also where h  is equal to v or sorry u and w star is equal to sigma v ok but i will work with this  particular solution you see this i am just matching variables right it is said that a b is  equal to c d e so i am saying that a is equal to c d and b is equal to e  now we will work with this so and we will try to show something so let us see what  we are trying to show    now first thing that we will show is that h is actually a linear encoding so what does  this mean you first always understand what has been tried to prove right i am saying  that i am going to show that h is a linear encoding of x then what is it that i am trying  to show   i am trying to show that h is equal to a linear encoding of x when h is of the form w x  and not something of the form w sigmoid of w x or something like that or any other  nonlinearity for that matter is the statement clear that is what i am trying to show  when i say h is a linear encoding i mean that h is obtained by a linear transformation  of x    now h as we defined on the previous slide is equal to this now if i already had an x  here then i was done but i do not have any x there yet so i want to a get to a form  where i can show that h is equal to w in to x so i will just do some simple trickery and  arrive try to do arrive at that form    so the first thing i am going to do is pre multiplying pre multiply by this quantity and  this is fair because this is just equal to i what next i will write these three x’s as u sigma v  transpose and i will leave one x as it is that  now just can you just try to see what the next step would be this v transpose v will  disappear because it is equal to i now what happened here i actually just expanded this  inverse so i will think of this as a b c so a b c inverse is equal to c inverse b  inverse a inverse   so i have just applied that it just that my inverse is a very straight forward matrices here  they are just the transform of the original matrices everyone gets this step well you can  stare at for a for a few more seconds if you want how many of you do not get this how  many of you get this ok now what is next this u transpose u disappears  student refer time onezero59  this also disappears  student refer time oneonezerothree  no  student refer time oneonezero6  it is this u is only the first k columns of u right this is not the entire u this is just the  first k columns of u fine now what next a into b inverse is  student b inverse  b inverse a inverse what will happen now that quantity will disappear so what do  you have left now ok so this is something ok so now let us look at this is let us say  this is n cross n and this is n cross k what is the output going to be  student n cross k  n cross k and what is the output going to look like is the first k columns of  student identity  the identity matrix everyone gets that if you do not you can just work it out with the  small matrix after going home and you will get it right if so if i done the full  multiplication i would have got the identity matrix but i am just talking the first k  columns so i will get the first k columns of the identity matrix do not fed too much if  you are not getting this you can just work it out on paper and you will get it  so i get the first k columns of the identity matrix and this inverse disappears this sigma  transpose into sigma transpose refer time onetwoone9 now what next what is this product  going to be the first k elements of  student sigma inverse  sigma inverse and that is going to get multiplied by sigma k cross k so that will give  me the first k elements of  student identity  identity matrix there is some very simple matrix operations where you are just taking  some columns right so if you do not understand this right now do not worry you can  work it out everyone is confident they can do this please raise your hands if you are  confident and now what do i finally get this multiplication will give me  student the first k columns  the first k columns of v ok so have we come to the desired form what i have shown  now h is a dash of x or linear transformation of x that means my optimal encoder was  a linear encoder and what was the optimal weight matrix w the first k columns of v yeah  i someone pointed it last time also i could not i ignored it i will just pretend i  understood  but i get it i know that there is a simpler solution i do not know why do it this way but  there is a simpler solution i just like making life miserable for you guys but but the  point is you can figure it out that it is a it is a linear transformation of x now    we have that the encoder is equal to the first k columns of v ok what is v eigenvectors  of x transpose x ok  student a  what is the other thing that you know about the eigenvectors of x transpose x they are  the solution for the  student eigen  if you have given an matrix x then the pca is the eigenvectors of the co variance matrix  was the co variance matrix x transpose x what is are it is eigenvectors capital v right  so what have we arrived at are we done with the proof yes how many of you think that  done with the proof how many of you think that we are done now  so it is done right so we have proved what we wanted to prove right so what did we  want to prove that you are doing auto encoders you are trying to train an auto encoders  and you are loss function is the squared error loss function we saw a neat way of  writing that squared error loss function as a matrix operation where x minus capital h  into w  and then we saw that these squared error loss function is nothing but the frobenius  norm of this and we knew that the minima of this objective function the frobenius norm  of x minus h w would occur when s w is equal to svd of x right we started from  there and showed that h is actually a linear transformation of x and what was that linear  transformation which matrix was used for the linear transformation v capital v what is  capital v it is the eigenvectors of  student x transpose  x transpose x so what is happened in effect is that if i was trying to train my auto  encoder with this objective function the weights in my initial layer w would actually  converge to v which are the eigenvectors of x transpose x that means the  transformation that i have learnt this transformation which i have learnt is the same as a  transformation that i have had learned using pca because pca would also have given  me v into x where v was the eigenvectors of the co variance matrix and we just arrived  at the same solution everyone gets it now we are done with the proof    so what we have proved is under these specific conditions that the encoder of a linear  auto encoder is linear auto encoder is equal to pca if we use a linear decoder if we use  a squared error loss function and if we normalize the inputs to this and you understand  why each of these steps was important why was the last step important  student refer  time one6threetwo  only then we would have got the co variance matrix why was a step before that  important because only if it was the squared error loss we would have got that frobenius  norm objective function right and why was the linear decoder important again the same  thing because x minus h w we wanted it to be linear right is it fine   so you see why all these assumptions were important and under these conditions we  have proved that auto encoders e equivalent to pca   
task2/super_cleaned_audios/lesson46.wav,985.5828,so that is what we look at in the second interpretation of pca right  so again we have the same setup that given n are linearly independent for n orthogonal  vectors we can represent x i exactly as a linear combination of these vectors what do i  mean by exactly perfect ok if you actually describe the whole things in words ok so  that is exactly what i mean right so you are going to write x i as alpha one i into p one plus  alpha two i into p two and so on and when you do the summation on the lhs on the rhs  you just get back the lhs when you do the summation on the right hand side you get  back the left hand side  so that means it can exactly be represented when you use all the n eigenvectors now if  i start chopping of stuff what will happen  student refer time zeroonezero4  it will just be an approximation ok now we this is what i meant and this is this the  equation holds that means this is exact and we know how to find the alpha is because p  js are conveniently orthonormal so we know how to find that easily ok now what if we  consider only the top k dimensions what is going to happen there is going to be some  error in the reconstruction i am not capturing all the information in my original data but  there is some error which i am not being able to capture and i made a conscious decision  that that error is not important i am willing to let it go   hence i want to represent the data using fewer dimensions ok so this is exactly what  you do in pca when you take the top k dimensions is this fine ok so now we want to  select pi’s such that we minimize the reconstructed error ok and this is again erratic  actually we should try to write it as x i minus x since these are vectors and the square of  vectors would just meet this right    so but you get the point right were just trying to do the element wise squared error loss  were trying to minimize that we want to do this so now let us try to see that if you are  aiming to do this what is the condition that we arrive at ok so no i thought i would ask  for some changes on this   for a minute all of you can you just bear with the fact that these are actually vectors and  not scalars so this square actually does not mean anything it actually means x i minus x  i hat transpose x i minus x i hat so when i use square with vectors this is what i mean  is that everyone can work with that notation fine    so now what is x i actually the real point right the correct point which can be obtained by  the full reconstruction if you consider all the n dimensions what is x i hat just an  approximation where you are considering only the k dimensions remember that each of  these quantities is a vector fine ok now what is happening here let me just try to say  this so let me just do this way so this is your original x and you are actually writing it  as a linear combination of your p’s somewhere you will have alpha k pk and then all the  way up to p n   so this is p k alpha n ok now what is this full thing this is x and what is this x hat ok  you see the picture what is the equation trying to tell you ok now what is the difference  between these two then these guys right if i want to take difference between x and x hat  everyone gets that it is the remaining term say that means alpha k plus one into p k plus one  up to alpha n into p n is that clear    so can i write it as yeah can i write it as this ok so you get this right so i am only  taking these guys because the rest will get subtracted so one is the full n dimensions the  other is only k dimensions so if i take the difference between them what remains is k  plus one to n dimensions and that is exactly what i have written here ok  and now i am coming back to the proper notation where this is a vector right so i am  writing the square as the dot product between the same vector is this ok these are the m  data point right this sum this is overall the m data points you need to minimize that is  that clear ok so this is fine now want just some rearrangement so i have just  expanded out that summation this is what it would look like right i have just expanded  out these two summations   now let us try to do this in your head and see what are the kind of terms that you get  there are two different types of terms that you will get so first of all let us understand that  when you expand this you will end up with a lot of dot products you will get a dot  product between this and this and this and so on right so can you split those terms into  two different types  student refer time five hundred and thirty-seven  square terms so one where i is equal to j and one where i is not equal to j is that clear  fine so let me just write it as that so i will have k plus one to n right that means n  minus k terms where i would be equal to j right so that means pk plus one was getting  multiplied by k pk plus one pk plus two was getting multiplied by pk plus two and so on and  then i will have these remaining terms where i is not equal to z right so these are the  dot product between the other vectors is it fine you see why i have split it this way  what will happen now the second term will go to zero ok and what about the first term  alpha ij square ok now what is alpha ij actually how did you find alpha ij  student refer time zero6two8   it is a dot product between we did this right finding any of these components is just  taking the dot product between x i and that dimension so x i transpose pj is that fine ok  is this fine and again this is slight abuse so this is actually what no this is ok right a  this is ok sorry i am just going to write it as this is this fine i just written it twice and i  can change the order since it is a dot product   now what i am going to do is so this is actually summation over an index i and a  summation over an index j and i can change the two summations i can interchange them  ok so that is what i am going to do now is this fine i will push the summation all the  way inside what is this actually this entire thing actually m times covariance of  student refer time zero7two5  so is this i is this what you are telling me that this is m x transpose x is this fine how  many if you do not get this i see a lot of blank faces how many if you do not get this  quite a few so this is so i is equal to one to m right so you are going over the data points  ok so this what is the dimension of this actually  student n cross one   n cross one and this is one cross n what does this product give you  student refer time zero749  n cross n what are the entries in this matrix so this was say x one one up to x one n and this is  again x one one up to x one n ok so that is going to be x one one square or rather let me just write it  in the generic form right so it is going to be x one i into x one j right is that fine and how  many such matrices are you adding  student refer time one7zero7  m of these so what would you get then what would the first let us so ok so let us do  this so the first entry of this matrix is going to be x one one square what about the first  entry of the next matrix in this series  student refer time zero84zero   x two one x two one square right so this is slightly tricky to demonstrate let me just a give me a  minute i will just collect my thoughts and do it properly ok let us take a small example  ok so x one one x one two x one three suppose we have a three dimensional matrix three dimensional data so  i am taking a sum of m such matrices ok i equal to one to m that means this is going to  vary this indexes the first index is going to vary from one to m now let us see the first  matrix and let us look at the first element of that matrix the first element of this matrix is  going to be x one one square ok  now let us look at the next matrix what is the next matrix going to be it would be x two one  x two two x two three right and multiplied by x two one x two two x two three what is the first element of this  matrix going to be  student refer time zero945  x two one square what about the third one x three one square this is fine so far now you are adding  all these matrices so what is the first element of the resultant matrix going to be x one one  square plus x two one square plus x three one square what is this actually this is the dot product  of x one with itself right and what does that give you the variance if the data is zero mean  right ok now can you make a similar argument of the ij’th entry is going to give you the  covariance between the i’th and the j’th entry is that clear right you could do a similar  analysis you can actually work it out after going back how many of you have found  comfortable with this there is still many who are not ok  so let us look at an ij’th entry right so can someone help me with say that one comma two  entry ok or the first matrix what is it going to be x one one into x one two right for the second  matrix  student refer time onezero5two  x no this is some yeah correct and for the third matrix three two ok now what is this sorry  what is the summation of these when you take the full sum you will get these three as as  what is this in this summation tell you  student refer time oneoneonezero  covariance between  student first and second  the first column and the second column is that clear now is it with everyone nowok  fine so what you have here is actually the covariance matrix you seems to be lost is it  with you sure  ok fine     so what we have here is something of this form ok    so now what we want to do is we want to minimize this quantity subject to the following  condition is that ok what is the solution for this if i did not have the summation ok  suppose i just wanted one dimension so i want to minimize say p sig p transpose sigma  p such that p transpose p is equal to one what is the solution for this  student refer time onetwozero8  smallest eigenvalue of sigma right and you can show by induction that if you want k  such things that here i am looking for n minus k such things right then these would be  the n minus k smallest eigenvalues of sigma but now i am talking about the smallest  eigenvalues but in the first solution i said we need to pick the largest eigenvalues so  what is the difference  student refer time onetwothree5  these are the ones we are throwing away these are the ones along which the error is  going to be minimum if we throw these away the error is going to be minimum so we  will throw away the last n minus k dimensions which means well keep the first k  dimensions is that clear so you arrived at the same solution is that right so that means  in pca you are actually trying to pick the dimensions in a way such that your  reconstruction error is minimized and this was exactly what our reconstruction error  was so do not worry about this math bit just see that we started with this quantity this  is what we wanted to minimize ok   and we did some trickery and we came to this formula that minimizing that error is  equivalent to minimizing this quantity and for this we know the solution that the  solution is the smallest eigenvalue and we want n minus k such things that means there  would be the n minus k smallest eigenvectors is that clear that means we are going to  keep only the k largest eigenvectors ok that means you are going to project your data on  to k largest eigenvectors    now so the key idea here is this right minimize the error in reconstructing x i after  projecting the data onto the new basis    so let us take an example and we will work with our toy example again    so this was the data that we had and suppose i give you a new basis which is one comma one  and minus one comma one ok this is a new basis this is an orthonormal basis orthogonal  basis you can see that u one transpose u two is equal to zero ok  now i need convert it to an orthonormal basis so i have just divided by the magnitude  is it ok fine now consider the point threethree comma three this was our original point according  to which coordinate axis x comma x that means this was threethree and this was three ok now i  can find the alpha is right because this is an orthonormal basis i can directly find the  alpha is now the perfect reconstruction would be this so actually if i do this i get back  the original point  now what would happen if i throw away the second dimension because the second  dimension had corresponds to a smaller eigenvalue ok i will get this so you see that the  point is still close to the original point i have not actually lost much right what has  happened is i have actually projected the boy lie point on this line right the line x equal  to y that is why i get x equal to y and in doing that i am not losing much information  from the original data is this clear right so you understand what happens when you  reconstruct the data    there is no end to this ok so just to recap the eigenvectors of a matrix with distinct  eigenvalues are linearly independent and we use this fact conveniently at least in the  case of square matrix where the also happen to be orthogonal so we know that they can  form a very convenient basis and pca exploits this to find the top k eigenvectors which  to be retained   and while doing this they have seen that two things are at least ensured one the  covariance between the dimensions is zero because that is exactly how we formulated it and  found the solution we saw that it turns out that we need to diagonalize a certain matrix  and the solution is the eigenvectors  we also saw a different interpretation where we saw that it is the same as throwing away  the dimensions along which the error would be minimum right and both these  interpretations led to the same solution which was project the data onto the eigenvectors  of the covariance matrix of the original data ok and this n minus k dimensions current  contribute very little to the reconstruction now what is the one thing which i have not  proved yet what was our wish list  student variance and covariance  variance and covariance right high variance low covariance i proved low covariance i  have also proved something with respect to reconstruction error because that is  something i require for auto encoders so just remember this bit about reconstruction  error  
task2/super_cleaned_audios/lesson91.wav,135.1838,this is where we left off in the last class so we look at three networks for image  classification starting with alex net then zf net and then vgg net vgg net in  particular had sixteen layers including convolutions and fully connected layers and one  thing that we saw that a large number of parameters are there in the first fully connected  layer  because you are connecting a five hundred and twelve  seven  seven volume to a four thousand and ninety-six dimensional vector right so  that is one thing the other thing that i would like to kind of mention right now so that it  becomes useful for the later part of the lecture is that if i look at any of these pink boxes  here right or even these things which are known as the fully connected layers and if i  just flatten them out and view them as a vector what does that vector actually capture  it captures a it captures an abstract representation of the image right  so now imagine what would happen is suppose you have trained one of these networks  alex net vgg net or any of your favorite networks and by what i mean by training is  that you have been tracking the cross entropy laws and you have run it for several epochs  with some patience and so on and i was satisfied with whatever training error you are  getting and you have stopped training now right   now after this if i pass images through these net through this network and i take the  representation from any of these boxes or from the fully connected layer what is it that i  have essentially got now i have got an abstract representation of the image that i have  been feeding it right so just remember this and this is something that we will use  so this is very common to do so you have a trained image net many people have  released different models for image net the ones which we have covered being included  them and now for him any image task if you want to do some processing then it’s  common to take the strain network pass your image through that  so you can train any you can use any image trained image net and pass that image  through it or sorry any trained convolutional network trained on image net and pass the  image through that and you can get a representation for that image and these are known  as the fc representations and these are as the convolution representations ok any of the  convolution layers fine   
task2/super_cleaned_audios/lesson85.wav,213.8642,now and later on actually the same guys they also came up with this formal relation  between svd and wordtwovec which is again under some assumptions        but i am not going to do the proof here i am just going to give you the intuition so  recall that svd does a matrix factorization of the co occurrence matrix levy et al showed  that wordtwovec also does such a implicit matrix factorization so what does this mean  so recall that wordtwovec gives us w context and w word it gives us these two parameters  so they say that there exist a matrix m such that ok this is wrong just be the product of  two matrices right this is the product of two matrices it should be w context transpose w  word or just see which way the transpose should be   so it is actually a product of these two matrices that we have learnt ok and what is m m is  actually nothing but the pmi matrix minus this log k where does the k come from  what was k the negative samples that you have taken so they actually showed that  whatever representations wordtwovec runs it is actually doing a factorization of this  matrix where this matrix has a strong connection to the pmi matrix and svd also  works with the pmi matrix  if you take svd matrix and do these modifications to it that means you take every  value which is the pmi and then subtract this log k from that and then just do an svd  of that you will essentially get back the same word representations as wordtwovec  there was some certain assumptions made in the paper but that is i mean i do not want  to go into those but the key idea here is that you can actually show that svd and  wordtwovec are actually connected and if you think about it at an intuitive level though      these methods are relying on the same underlying principle that words appear together  based on that the word representations get updated or in svd based on there the counts  get updated and you then eventually end up with certain representation  next the underlying principle is the same so there has to be a connection right it is not  that they are doing fundamentally something different both of them are relying on the  idea of co occurrence or the idea of distribution right so they have to at some level be  similar in some ways right so that is what they finally showed and so now but still in  most applications wordtwovec is preferred   so one reason for that is that this is an iterative training procedure right as compared to  svd and i come back to your question right how do you do that how do you  compute the eigenvectors of x transpose x and the answer is there is no simple way of  doing that and you have to do that expensive matrix multiplication  and then rely on various very smart libraries for computing the eigenvectors which are  still order n raise to two point something or something like that they not order n cube but  they are still order n raise to two point something means they are still expensive and then  of course you have this memory issue that if you have a very large vocabulary your  pmi matrix is going to be very high dimensional and then you need to do the  factorization of that high dimensional vectors so that runs into these computational  efficiency issues  on the other hand wordtwovec by design is an iterative algorithm because you are going  to grade gradient descent which is that every times that you are going to update some  parameters of the model you are not learning all the parameters together you are only  dealing with some parameters at every time set right so that is more computationally  efficient especially if you do the contrastive divergents or the negative sampling or the  hierarchal sample so that is why perhaps it is still more popular than svd  
task2/super_cleaned_audios/lesson93.wav,353.8336,dream deep art fooling convolutional neural networks  so in this lecture we will look at various ways of visualizing convolutional neural  networks and although it is not very obvious at this point as we go along we will see  what i mean by that so let us start this lecture    so i forgot to add the acknowledgments slide so a lot of the material that i am going to  cover today is based on some content by andrey karpaty in his online course stanford  course we will add the appropriate acknowledgments and a link to the course ok  so with that i will start module one which is visualizing patches which maximally activate  a neuron ok so what are we trying to do here is we are trying to the quest today largely  is going to be able to understand what a cnn has actually learned right and what i mean  by that is we said that there are these filters which try to detect edges which try to detect  blurs and so on and then there are these neurons which fire for certain things and so on  so we want to see different ways of finding out what a convolution neural network has  actually learned or what have the filters actually learned or what are the different neurons  in the convolutional neural network actually capturing what do they fire for what are  the kind of images that make them trigger and so on right so that is the first thing that  we are going to look at how do you visualize patches which are causing a neuron to fire    so this is again our vgg network just put it vertically say have passed an image to  that and then at every layer you are applying convolutions and then match pooling and  so on right up to the last layer right now we consider some neurons in one of these  layers so i am considering this neuron and i want to find out what exactly is this neuron  trying to do right and which is the same as asking what kind of images does this neuron  fire for  so i have thousand different classes i have cats dogs cars trucks and so on i am  interested in figuring out what are the different kinds of classes that this neuron fires  and this is more from say i am already getting some output accuracy and i am either  happy with it or not happy with it in either case i just want to see what is it that my  network is learning is there any scope for improving is that that there are no neurons in  the network which actually fire for the dog class did not should i do something  differently was it that most of the neurons fire for all classes that means they do not  have any discriminative power so what exactly is going on right   so that is why we are that is why this study is interesting and you will do something of  this sort in your cnn assignment ok so and by now we are clear that if i am focusing  on any neuron and any layer i can always go back and trace the patch to which it  corresponds in the input image everyone is fine with that right so we saw that if i am  somewhere here then every neuron here corresponds to some one6 cross one6 patch in the  original image and the same is true for every layer right i can always this is a  deterministic process i can just find out which are the original image pixels which  contributed to the computation of this particular neuron in any layer ok   so i can do that so now what i am going to do is i will send as many images as  possible whatever images are there in my training data test data whatever images i  have i will pass these images through the convolutional neural network ok and for the  neuron of interest i will note down which when does it fire and where ever it fires and  by fire i mean it is a output is close to one or it is a output is high because these are  relu neurons i look for high output they do not saturate at one right so this i look  which images for which this neuron had an high output and for those cases i will go back  and trace the image and see which patch of the image actually caused this to fire  so i want to see whether my neurons are actually learning things like noise detector or  eye detector or something right    so let us look at the results of one such experiment done by a group of researchers so  they considered some neurons in the pool five layer and they did this experiment that they  pass a lot of images and whenever this neuron fired they went back and saw what was  the patch in the image which was causing this neuron to fire   so that they found that one set of neurons is actually fires for people places so if you  go back and trace which is the image which caused is to fire or which is the patch then  it is largely centered around a persons face or which is something which is very clearly a  person ok another set of neurons fires for dogs another set of neurons fires for flowers  all sorts of flowers and different orientations different maybe colors are same here but  they are all different thing right somewhere inside a bouquet somewhere inside a flower  pot some somewhere on a table and so on but expected of that these neurons are firing  for any flowers that appear in your input image and the fire only for that patch nothing  around it  so it is very is actually able to localize and fire there are some images which fire for  this images the digits and alphabets written in the image so these are some addresses or  dates or billboard signs or something like that and whenever there are these characters or  numerals there and this neurons fire  and some neurons fire for houses and then some neurons fire for shiny surfaces so  there is this different sets of neurons which fire for different sets of things right so also  that means your convolutional neural network is trying to learn specific characters of the  input characteristics of the input and this is one way of visualizing so this is not like  anything tricky here it is just that its good you can think of this as debugging tools for  your convolutional neural network right because in your you i guys are used to  programming where you give different inputs and see what is the output and then try to  debug it    so this is one way of trying to figure out whether your network has learned does it  really need more training is there a certain class of images for which it is not firing at all  or is it confusing between two classes and so right so that is one way of visualizing   
task2/super_cleaned_audios/lesson87.wav,704.6226,ok so now we will go to the next module where we will try to learn the relationship  between the input size the output size and the filter size ok   so so far we have not said anything about the dimensions of the inputs i have just been  very vague that its m  n and also for the filter i have just said three  three five  five and so on  and in fact i am not even told you what the dimensions of the outputs are except that i  be got some intuition that it seems that the output dimension is smaller than the input  dimension right  so let us look at these in more detail and see what do these different outputs look like  why do i care about these things what do the inputs and the output sizes look like  because these are your matrices that you will be dealing with this tell you these tell you  how many parameters you are going to have  these tell you what is the size of the memory that you need to load this entire network  into your memory and so on it so that is why this computation is very very important  and i will have some more things to say about it towards the end of this lecture or some  lecture ok     so if you first define the following quantities so we have the input which has a width  wone height hone and depth done so if you are looking at the original image then the depth  is going to be three in most cases it is going to be rgb ok there is something known as  the stride s i will come back to it later on  but i am just defining it here or others just introducing it here and then you have  number of filters so i said that every filter that you apply you are going to get one  feature map which is two dimensional if you have k such filters you will get k feature  maps each of them is twod right   so we will have something known as number of filters k and then you will have  something known as the spatial extent of these filters so that is the number three  three five  five  which i have been saying so that is known as the spatial extent i am going i am going  to refer to it as f ok and we are going to always assume square filters  so it is always going to be f f is that fine ok and the depth of the filter is one more  thing which i need to worry about but i have already said that i am going to assume that  the depth of the filter is the same as the depth of the input ok   now the output is again a volume which is of size wtwo htwo and dtwo and my quest is to  find out how do i compute these wtwos htwos and dtwos that is what i want to figure out it is  not very difficult but i just want to do it properly so that is what the setup is right so  we have defined the sizes of everything on the input and the filters now you want to look  at how do we get the output sizes ok    so let us compute this for one example so this is my original image so i am looking at  a two dimensional input which i believe is seven  seven and i am applying a three  three filter to it  so every time i slide the filter i will get one pixel in the output and i got the entire  feature map  now it is obvious that the size of the output is less than the size of the input why is it so  because there are certain pixels at which i cannot place the filter why you will go out  of the boundary right so i cannot if this is my pixel of interest i cannot place my filter  there because then the filter will go outside the image and i do not know what the  average to come to how to compute the average right those values are undefined do you  get that ok    so in general for let me see for the three cross in fact this is true for all these pixels which  have been shaded or any of these pixels i cannot place the filter because you will go  outside the boundary so now for a three  three filter what is the reduction in the size of the  output compared to the input the width decreases by two and the height decreases by  student two  two right so can i be bold enough and say that the width and height decreases by not yet  ok   so let us see if we had a five  five kernels ok then which are the places at which i will not  be able to place the filter these two shaded boxes and both these boxes i cannot place  the filter because the filter will go outside the image     so now can you tell me how many so in this case the size reduce is by how much  the width reduces by  student refer time zerofourtwozero  no the width reduces by four sorry and the height reduces by  student four  four so now can i say that the width and height actually reduce by f minus one where f is  the size of the filter is that ok how many of you get this so you did not get this no  you did not get this ok so in the three  three case you see that that is one row and one  column on each side left and right which i cannot apply ok  so let us focus on the columns so columns give me the width right so when i have a  three  three filter there are two columns which get subtracted because these are the boundary  columns when i have a five  five filter how many columns get subtracted two on the left  hand side two on the right side is a total of four  if i have a seven cross seven filter three on the left hand side three on the right hand side so total of six  so you see the relation it is always f  one right so your new width and height is always  going to be wone  f  one which is w one  f  one is that ok everyone gets this and same for  the height the width and height are going to be symmetric because the filter we have  chosen to be symmetric it is f  f ok     now but what if we want the output to be the same as the input what do we do  padding ok you can use you know something known as padding so that means now i  will have a boundary of zeros so i am saying that this is my original image and outside it  there is a black border or a white border i do not know whether zero stands for black or  white it is embarrassing but  student black  black ok so it is a black border outside the image ok and now i am going to take an  average that way  now this was the pixel earlier on which i could not place the filter but now i can  artificially place on it assuming that it there is a black boundary around it so now what  would be the output size  student refer time zero6zero8  same as the input so now can you tell me so i have wone i have f and now i have  something known as p also ok so i know that w output rather wtwo is the output is one  now when i add the padding what would the formula be two p is that fine everyone gets  that ok     so now if i have a five cross five filter and if i want the output size to be the same as the  input size what is the padding that i should use  student p  p is equal to two right that is clear from the example that we saw that there were two  shaded columns and rows which were problematic so i need to do a padding of two and  then if i substitute in this formula you can just see right so five  four  one right so you will  get back wone is that fine is that ok how many if you get this how the formula works  with the padding how many of you do not get this please raise your hands you do not  get this ok   so remember in the three  three case there was one column on the left hand right which was a  problem so when i say a padding of one i add one column to the left one column to the  right one column to the bottom and one column to the top  and that is exactly the problematic region in the three  three case right so that means this was  my original formula ok now the new width is going to be plus two times the padding  which i am going to use because i have used one padding here and one padding here  right now in the three  three case that is ok  now in the five  five case how many columns are problematic two so that means i have use  a padding of two when i say a padding of two i add two on all the sides so now again if you  substitute in this formula so you would have wone  five  one  four so that will give you back  wone right so that is how it is right     now the question is you have if you have taken care of filter size and padding now the  other thing that we need to look at is stride ok so the stride defines the interval at which  the filter is applied now what do i mean by that  so remember that stride is basically a step right the same definition as it is applies to  walking rate so right now what we were doing is we placed the filter at as this pixel at  the center then this pixel has the center and then this pixel has the center instead of that  i could take a that means my stride is one i am taking one step at a time so if i do a  stride of two what would happen  student refer time zero8four8  i will apply two alternate pixels right so this is how i will move so then what would  happen to my output size if my stride is two  student refer time zero8fiveseven  what would happen  student refer time zero8five9  it will become half ok so what would the formula be then so so far my formula was  now if i have a stride of two what would the formula be  student refer time zero9onetwo  they divide the whole thing by two  student refer time zero9one6  by s right if it was s was three then this would have become onethird roughly right if s  was four this would have become onefourth so what should i divide by  student s  s so i should divide the whole thing by s  student refer time zero9threetwo  s ok     so it turns out that is not exactly that but you get the intuition and you can work out the  formula so you do not divide this by s and you will figure it out it is easy to see  because of some ceiling and flooring and things like that so you can go back and check  that out and basically you could just think of this that this was your original weight in  the absence of stride or rather than the stride was one  so now if you are going to take longer strides you have two account for that if you take  a stride of two stride of two your width is going to become half you should take a stride of four  your width is going to become onefourth is that fine do not worry about this additional  plus point you can go home and figure it out    finally coming to the depth of the output what would the depth of the output be so let  me just see right now all our formulas were wtwo htwo in the presence of filter padding and  size a stride sorry but we never had a formula for  dtwo so that is what i am asking  now what is the depth of the output same as the every filter is going to give you one  two dimensional output if you have k filters  student k  you will get k two dimensional outputs that means the depth of your output is k right     so the depth is very simple it is just equal to k the number of filters that you have so i  want you guys to note down these three formulas    now let us do some exercises so this is my original input which is twotwoseven  twotwoseven  three i  have decided to apply oneone cross oneone filters and i am not going to tell you the depth of the  filter because it is going to be the same as the input ok  and i have ninety-six such filters i have decided to use a stride of four there is no padding can you  tell me what is the output volume going to look like what are the dimensions of the  output volume ok so dtwo is simple what is dtwo ninety-six what is wtwo  student fivefive  fivefive htwo ok you guys have the last class fine so similarly you can do it for so actually  the computation which i had that this was just not some random computation this is  actually the first convolution layer from alexnet right so one of the popular  architectures that we are going to cover later on right  so this is what aalexnet does at its input it takes the rgb input and gives you a volume  of this slice this side and then there is something else with this volume right so we will  see that later on there is one more exercise you can do it later on i do not want to do it  now ok   
task2/super_cleaned_audios/lesson50.wav,1517.8239,so with this we will move on to the last topic in the yeah so that is something that you  will have to so the way i would do it right is that you keep aside some one hundred images from  your data as validation data now once you have learned these eigenvectors try to  compute the reconstruction error for these one hundred images and just vary it do one hundred one hundredzero  one hundredzerozero written as many dimensions as you can and see at what point is a reconstruction  error ok for you right and this is assuming that you have some notion of what is a  reasonable reconstruction error so we all know that the minimum is zero  but if you have zero5 then maybe for face database it might be but if it is a database  where you are trying to look at mechanical parts so suppose you are looking at motors  and rotors from a machine assembly now there you want to be able to distinguish minor  detects defects on this and a detect could a defect could actually just be one single or two  pixels getting different from the original image right so there the reconstruction loss  would be much needs to be much more robust you get the point so it depends on your  application so you will have to take some validation data either have a domain expert to  tell you what is reasonable or go by the number that you get right and this is the  validation error that i get  so everyone understands the question and perhaps the answers ok so we now go to the  last module  student refer time zeroonefourzero  yeah if you can  student refer time zeroonefourone  yes you can now project any face into this database a so that is the eigen basis that  you have got you have got the basis vectors now any data you can project onto this  basis  student refer time zeroone5four  now so if you are trying to learn these eigenvectors by say using one hundred images all of  which belonging to a particular demographic say all caucasian images right and now  at the runtime you have an asian image then you will have obviously have some error  right but you have large even of data say if you have if you are constructing this from  million images then it should generalize that is i mean just as for any machine learning  algorithm  the training it from small data and you bring out some outlier at test time it is not going  to work right but if you have reasonable data it should generalize any other questions  to calculate the eigenvectors x is m cross onezero m cross onezero k yes now we move on to the  last topic for the basic portion and the next class we will do auto encoders will be back  to deep neural networks so singular value decomposition right    so this is actually the stuff that i need an important theorem from here at multiple two  places in the course so now before doing the right let us get some more perspective  on what eigenvectors do and why are they actually important    so let v one to v n be the eigenvectors of a and let lambda one be the corresponding eigen  values so we know this a v one equal to lambda v one and so on ok now suppose all the  vectors in r should be r raised to n ok so if a vector x belonging to rn can be  represented using this basis ok now what if i am interested in the operation a into x  what is the advantage of representing it using this basis so this is what you are saying  the other day  student refer time zerothree5four  what is ax it is a matrix vector multiplication right and it is going to be a heavy  computation now if all my vectors in rn are represented using the eigenvector as the  basis what happens to this matrix operation  student refer time zerofourone6  it reduces to  student refer time zerofourtwoone  let us see so i was interested in ax but i know x is this so you get this step and what  happens finally do you have the matrix anywhere here so what happens to the matrix  operation  student refer time zerofourfourone  it reduces to a sum of scalar operations right if your vectors were representing using  the eigenvector as a basis  so this is one reason why this is important right so you can now get away of the get  rid of the matrix operations and just do scalar operations right so now there is a catch  here which i am going to ignore just to try it if i bring in the catch you guys will get  confused so i will ignore if anyone has a doubt maybe talk to me after the class but for  now let us go with the fact that the matrix operation reduces to a scalar operation    now so far what we have done is discussed square matrices i have said that they are the  villains of linear algebra but who are the super villains of linear algebra  rectangular  matrices everyone says that but why imagine what they do to a vector yeah so can  rectangular matrices have an eigenvector  student refer time zero5four5  yes obviously yes that i mean any matrix can have an eigenvector  student no  no why can you write something of this form you can’t right because when the  matrix operates on an n dimensional vector what does it give you  student refer time zero6zeroone  an m dimensional vector right hence they are super villain right because they take  the vector from one space and transform it to a completely different space that  completely lots lost it is identity right so that is why rectangular matrices are even  harder  so now we just saw that for square matrices this eigenvectors form a very convenient  basis where these operations reduce to a scalar operation but now rectangular matrices  do not even have eigenvectors so then cannot we have the same advantage there can  we have the same advantage there you can’t right because you do not have an  eigenvector but i would teach you about singular value decomposition so i better have  something so get the connection ok there is a problem with square matrices with the  rectangular matrices so now let us see so we will try the aim is to see if we have  something equivalent to this scalar transformation that we had for square matrices  how many of you have seen this in linear algebra before so you know whatever i am  going to talk about fine so the result of ax is a vector belonging to r m and the  original x belongs to r m so we do miss it miss out on this advantage that you could  have reduced the matrix operation to a scalar operation and now we will try to see if we  can still get back that advantage    so notice this is matrix you can think of it as a function which provides a  transformation from rn to rm so what is the set of inputs to the matrix it is vectors  belonging to rn  that is the set of input  now suppose we had a pair of vectors v one u one v two u two vk u k each belonging to these two  different universes one is rn the other is rm and there was a specular relation between  them that a into vi is equal to sigma into ui suppose i am just being ambitious let us see  whether we can actually have this pair but suppose we had this pair then can you  connect this back to the discussion on scalar operations so let us just see that in detail  and we will of course assume that these are orthogonal and form a basis so the vi’s  form a basis in rn and the ui’s form a basis in rm is that clear that is all  straightforward we have these vectors  now every a vector belonging to rn which was the input space can be represented using  a linear combination of v straightforward and any vector belonging to the output space  can be represented of  student refer time zero855  of u right so that means any x in the input space i can write it as this linear  combination and now if i do the matrix operation what happens  student refer time zero9zero9   you get this a into vi what is a into vi sigma ui i have still not shown you how to find  these sigma is ui by the way right ok once again the matrix multiplication reduces to a  scalar multiplication    so now let us try to look at a geometric interpretation of this    so what you have is this original space which is rn you are using a as a matrix  operation right as a function and you are transforming vectors from n to rm right so  this is the space transfer that i was saying it vectors are being picked up from rn and  being put into r m ok and rn is a space of all vectors which can act as inputs to this  function and rm is a space of all vectors which are the outputs of this function ax  now we are interested in finding a basis u v such that v is the basis for the inputs when  i say basis all of you should immediately start thinking of dash vectors  student orthonormal vectors    orthonormal vectors orthogonal or orthonormal right once we have orthogonal we do  not care about the rest u is the basis for the outputs such that if the inputs are and  outputs are represented using this basis then all our matrix operations reduce to scalar  operations so we are just trying to find the rectangular analogy for the square a  phenomenon that we observed ok that is what we are trying to do now can you tell me  i have told you that if such a v and u exists then you could do this can you give me  such a u and v  so what do we mean by so here i said actually i said this right that the dimension of  the row space is actually k and the dimension of the column space is also k what do you  mean by the dimension is i mean right here i am telling this is rn and this is rm and  now i am telling you the dimension is k what do i mean by that  student refer time oneonezero5  the only k linearly independent vectors    and this is again something from linear algebra which i expect you to know is that all  possible vectors in rn only a subspace belonging to rk can actually act as input to a x to  produce a nonzero output so i am talking about a null space column space and things  like that right so this should be clear if it is not it is it is not very important at for us  right now  and hence we have only k dimensions    so let us look at a different way of writing this so you have this a v one is equal to sigma  one u one av two is equal to sigma two u two so i can again do the same trick that i put all the v’s  into one matrix where vi’s are the columns of this matrix and i will put all the us into  another matrix where ui’s are the column of this matrix is that fine everyone ok so far  and then i can write it as this matrix operation same thing that we did when we are  doing eigenvalue decomposition right so we had written it as a into u is equal to u  into sigma right because there we had the condition that ax is equal to lambda x now  we have a u is equal to sigma v or rather the other way around so av is equal to did i  missed up did i no right  student refer time onetwotwo9  sorry  student refer time onetwothreetwo  fine yeah so av is equal to sigma into u  so is this fine no but when you do the diagonal operation you will get it as u into sigma  y the same way as a x equal to lambda x but when you write it is a into u is equal to  lambda comes later on    and if we have k orthogonal vectors so remember i said that this basis consists only of  k dimensions right because that is r the set of vectors which can act as input to a so  what i but i want a basis for the full rn so what do i do for the remaining n minus k  have you heard this gram schmidt othogonalization right so if i give you if there if  you are trying to construct a basis for n ok for rn rather and if i give you k orthogonal  vectors they can do k you can construct the remaining n minus k using gram schmidt  orthogonalization right so you can get the full basis ok fine so let me just see and this  is orthogonal ok so you can write so you see these two forms can you relate it to  something that we have seen before in the course   this is singular value decomposition what else did we see before  student refer time onethree5three  eigenvalue so this exactly the same forms right and i have used the same set of tricks  to arrive at it right so i first put the vectors into a column as columns into a matrix then  wrote this in the matrix format and then pre multiplied post multiplied by certain things  and i got these two formats and remember that v and u both are dash matrices  student refer time onefouronethree  orthogonal matrices right so that inverse is just their transpose ok ok so far  everything is fine now i still do not know what u and v are all this analysis is assuming  that i know what u and v are so now can you tell me how to get these u’s and v’s    suppose v u and sigma exist then we can write this right so a is u sigma v so a  transpose would be the transpose of that now can you work with me what is the next  step  student refer time onefourfour7  ok next  student refer time onefour5one  this is u sigma v transpose so then this would be i think the next step is no the next  step is also wrong that fine ok fine i just had some error with the transpose ok what  will happen now what will disappear from here  student refer time one5two6  u transpose u that is i right u transpose the inverse of u    so you get this what does this look like this looks like the eigenvalue decomposition  of  student a transpose a  a transpose a that means v consists of the  student eigenvectors  eigenvectors of the  student a transpose a  a transpose a so now can you tell me what u would  student refer time one55four   ok fine  so this looks like the eigenvalue of eigenvalue decomposition of a transpose a  similarly we can show that a a transpose is equal to u transpose sigma square u ok so  then u is the set of eigen vectors of a a transpose right and now here what was with  will the eigenvalue decomposition always exist for a matrix  student no  no under what conditions would it exist first of all it has to be a square matrix  student refer time one6threetwo   ok right but now for a rectangular matrix would be singular value decomposition  always exist  student yes   yes right because it depends on the eigenvalue decomposition of square symmetric  matrices ok is that fine ok so for any matrix shall always have the eigen value oh  sorry the singular value decomposition    so this is perhaps yeah ok now just one last bit and let us see if all of you can  understand this so now i can write a in this form this is nothing but what i already said  right this is u this is sigma this is v transpose ok now from here from this step do you  see how i got to this step this is something that we were struggling with yesterday also  when we were trying to find out summation x i x i transpose something similar here you  know the four ways of multiplying matrices right so this is which way one of the ways  yeah so does everyone get this right so a simple thing would be first to just take these  sigmas inside right because this is a diagonal matrix right this is all zero’s so these are  actually you can write it as sigma one u one sigma two u two and sigma k u k right  now this ends up being the product of two matrices right and you can write it as a sum  of columns into rows right so what i am writing it as a sum of sigma one u one multiplied  by v one so sigma one u one into v one transpose is a scalar matrix vector matrix right so each  of these terms here is a  student matrix  matrix and you are adding k such matrices ok now try to relate it to reconstruction  error you are taking a matrix trying to write it as sum of many matrices if i trim some  terms from this some terms from this sum what would happen if i have all the terms  then what would happen i will get a back exactly if i drop some terms what would  happen  student refer time one857   i get an approximation of a how good would that approximation be  student refer time one9zerothree  first is depending on the number of dimensions but is there a natural ordering in these  dimensions if i want to throw away some dimensions which one would i throw away   student refer time one9oneone  smallest  student singular values  singular values sigmas are the singular values so you see that this is getting multiplied  here every matrix is getting multiplied by the singular value corresponding singular  value so if i drop out the terms which have the smallest singular values then those  matrices the elements would have been very small so i will not lose much in the  approximation so again the same idea that i am trying to approximate the original  matrix by a smaller rank   by of so now the original matrix had m cross n entries ok how much if i use only k  eigenvectors or the sorry k singular vectors or k dimensions to approximate it how much  storage would i need how many values do i need so the original matrix was m cross  n how many entries are there here  student refer time twozerozero6  each of this is how much  student refer time twozeroonezero  m for ui plus n for vi ok and plus one for the sigma and how many of these are there  student k  k so if k is very less than your m and n right then again you will have some  compression you get this ok so all of these ideas are related and i want you to be able to  connect them right that all of this is towards doing some approximations  reconstructing some reconstructing a matrix from it is components and doing this  reconstruction in a manner that you end up making minimum error in the reconstruction  is this idea clear even if some part of the math is not clear is this idea clear how  many forget this ok so some of you do not you do not  student refer time twozero59  yeah so what is the original dimension of a m cross n right now i am trying to  reconstruct it using a sum of sum k terms ok so hence this k comes now each of these  terms how many elements do i have i have ui which is of dimension m i have v i which  is of dimension n and then i have the sigma i which is of dimension one right and i  have k of these so this is the total amount of storage that i need i am saying that as k is  much less than m and n which would typically be the case   then you are getting a much lower space reconstruction of the original data right and  you are doing this reconstruction smartly because you are not taking any k dimensions  you are taking the k most important dimensions and this most important is defined by  the singular values this is designed by the sigma is that fine    ok and actually there is a formal theorem which says that sigma one u one v one transpose is  the best ranked one approximation of the matrix is this a rank one matrix is sigma one u n  i hope you guys have done the assignment right sigma one u one v one transpose is the rank  one matrix and if i take this idea further this summation is the best ranked two  approximation and if i keep going this summation is the best rank k approximation so  what it says is that if you are trying to reconstruct the original matrix right from these  components and if you go by the eigen or the singular values and you pick the ones  corresponding the top k singular values then the best that is the best possible  reconstruction that you could have done  now how do you formally define reconstruction how would you make it as an  optimization problem what are you trying to minimize  student refer time twotwo59  the actual matrix has some values which is the matrix a ok b is the reconstructed  matrix using only k dimensions how many of you understand what is this product  saying what is this  student refer time twothreetwozero  first k columns of u these are the k singular values and these are the first k rows of so  ok i was just talking about this is the first k columns of u these are the k singular values  are put across the diagonal and this is the first k rows of v transpose and this is exactly  the product which i showed you here is that fine    ok so there is a theorem this is called the svd theorem it says that if you want to  reconstruct a then this is the best rank k approximation that you can get now if i want to  pose it as an optimization problem what will i say what would i have minimized  actually this is the reconstruction right so let us call it a hat actually and what does  this mean this is the dash norm  student refer time twofourone5  frobenium norm what does the frobenium norm give you squared difference between  the elements right roughly speaking ok so it will tell you what is the square  difference between the ij’th element of a and the ij’th element of b so whenever we  have this situation if you are trying to if this is our objective function that we have trying  to reconstruct a or try to transform something and get a predicted a or a reconstructed  a then the best possible reconstruction would be given by this solution  so this optimization problem has a solution that you just use the eigenvectors of xx  transpose and sorry aa transpose and a transpose a right is that clear ok so this is  the theorem that we will be using when we are talking about autoencoders and we will  try to connect auto encoders to pca ok so just revise this is the prerequisite for next  class whatever we have done in the last three sort of extra lectures you have to revise it  before you come for class tomorrow right ok and yeah this is sigma is just some  terminology sigma is actually the square root of lambda a that was obvious and u is  called the left singular matrix of a and v is called the right singular matrix of a  
task2/super_cleaned_audios/lesson44.wav,543.3895,actually which i was hoping all of you will give because all of you have done two  prerequisite which is linear algebra and machine learning both of them teach you  principal component analysis so i was hoping that you will give that answer  now can you give that answer he already of course gave that answer is that make  sense so we relate it to that so but before going to principle component analysis we  look at eigen value decomposition  this is very straightforward so let uone to un be the eigenvectors of a matrix a and let  lambda one to lambda n be the corresponding eigenvalues  now i am going to construct a matrix u such that the columns of u are these vectors uone  to un is that fine what u looks like and now i am going to do this product i am taking a  the product of the matrix a with the product of with the matrix u where u is this right  it is the all the eigen vectors tagged one after the other is this fine the next step i am  just pushing the matrix inside if you know the four different ways of multiplying a matrix  you will know that this is correct or else for now just thing that you can just push the  matrix inside  now what is this i can replace them by the lambda one u one lambda two because a u one is  equal to lambda one u one by definition ok now can you write this again as a product of two  matrices one is of course the matrix u and the other is  student diagonal  diagonal so the diagonal matrix will come first or the matrix u will come first how  many if you say u will come first how many if you say the diagonal matrix will come  first the sum is never one ok  so it is going to be like this ok and you can write this as u lambda so u is again the  vector the matrix containing the eigenvectors of a and lambda is a diagonal matrix  where every diagonally element is a corresponding eigen value    now this is what we have so far a into u is equal to u into lambda now suppose u  inverse exists i will assume that u inverse exists and later on i will tell you under what  conditions it exists then i could write it as this any of these two forms in one case i am  post multiplying by u inverse in the other case i am pre multiplying with u inverse ok  so this is known as the eigenvalue decomposition of a matrix and the other way of  writing it is known as diagonalization of the matrix right you take a matrix apply some  operations to it so that the result is a diagonal matrix is this clear to all of you is very  straight forward ok and again eigen vectors play an important role in this now the  important question is under what conditions would u inverse exist u inverse would  exist if the columns of the matrix u are  student linearly independent  linearly independent ok do we know the columns of the matrix are linearly  independent  student yes  yes because it is a  student refer time zero3onezero  set of eigenvectors and we already saw the proof that the eigen vectors are linearly  independent ok this just follows whatever i say ok now do we need proof for this i  slide one9 we did this i did not realize it fine     now if a is symmetric the situation is always more convenient why is it  student refer time zero3fourzero  what would u be  student orthogonal matrix  what is an orthogonal matrix actually  student refer time zero3four7  so the eigenvectors are orthogonal so we have this situation right suppose i want to  do u transpose u ok this is how that operation would look like ok now what is the  ij’th entry of the resultant matrix   student dot product  it is the dot product between the  student ui and uj   ui and uj everyone gets this right the ijth entry of this product is going to be the dot  product between ui and uj this dot product would be dash if i is not equal to zero or j  student j  j and there is no point in this so each cell of the matrix q ij is given by the dot product  and it is going to be zero if i not equal to j and it is going to be one if i is equal to j ok so u  transpose u is equal to the identity matrix that means u transpose is the dash of u  student refer time zerofourfour5  transpose of u and of course inverse also ok so u transpose is the inverse of u and it  is very convenient to calculate what is the complexity of inverse so now you appreciate  that that is a that has high complexity and in this case if the vector if the matrix is  orthogonal that means it is a collection of orthogonal vectors and the inverse just comes  for free right    so now given this situation and do not read the hint as if this is going to help but yeah  what can you now say about the sequence the same sequence that you saw earlier so i  have given you that the evd of a is equal to u sigma u transpose where u is the  collection of the eigenvectors and sigma is the eigen values the diagonal matrix  containing the eigenvalues  now what given this and ignoring the knowledge of the first section of this lecture can  you tell me something about this series what would be the nth element of the series  student u sigma power n  u sigma  student power n  power n  student u transpose  u transpose and you arrive at the same conclusion right where i was talking about this  operation right so if we can say something about this matrix then we can say something  about this series what can you say about this matrix if the largest eigenvalue is greater  than one as you keep raising it is power that value is going to explode and hence the  entire product is going to explode less than one that product is going to vanish and  everything else would be less than that right remember is the dominant eigen value  so everything would be less than that so that product will vanish ok so the same  conclusions you can arrive at right so that is why i want to do these sections again so  you would have done these in linear algebra but you would have not arrived at these  conclusions from a very different interpretation but i want to focus on the interpretations  that i care about i do not how many of you have seen this series in the course on linear  algebra you have ok but i do not see why anyone else would teach this is not required  is only required for some things that i want to do in the course right that is why i  wanted to do this section  so everyone is comfortable with eigenvalue decomposition it is a very simple stuff right  i mean there is no proof or anything involved there we just use some properties of  eigenvectors and eigenvalues and do it    now there is one more important property of eigenvectors which well use today so let  us see what this means right you have a matrix a which is an n cross n matrix ok and  your import interested in computing this value x transpose a x where x belongs to rn x  belongs to rn  so what am i trying to do here of all these vectors possible in rn i want that vector  which maximizes this quantity what is this quantity scalar vector matrix tensor  student scalar  scalar ok such that x is equal toone this is the problem that i have been given to solve  why it is not clear as of now but suppose this is a problem i am trying to solve or the  inverse of this which is minimize the same thing of all the vectors in rn find the vector  which minimizes this quantity subject to these constraints then the solution for this is  given by the smallest or largest the solution is the smallest eigen value of a  and x is the eigenvector corresponding to that so if you are trying to minimize and the  solution is a smallest eigenvalue we need to clarify that if you are trying to maximize  and the solution is the largest eigenvalue is that clear and the value of x would be the  corresponding eigen value so largest eigen vector is the same as something that we  have defined today dominant eigen vector right  so let me just repeat so that there is no confusion let us focus on this problem the  solution to this problem that is the x which will give me the maximum which will  maximize this is the dominant eigen vector of the matrix a right is that clear fine ok  and if you want to minimize it is going to be the smallest eigen vector that means the  inverse of the dominant    so there is a proof for that i will not go over the proof you can take a look at it at your  own leisure    so what has been the story so far the story has been that the eigenvectors  corresponding to different eigen values are linearly independent  if you are dealing with the square symmetric matrix which is something that we will  deal with soon then things are even more convenient because the eigen vectors are  actually orthogonal ok and they form a very convenient basis and now we are going to  put this to use when we talk about principal component  
task2/super_cleaned_audios/lesson78.wav,2153.4651,so from here on so none of this that we covered had anything to do with neural  networks say but it was important to understand the context and i will tell you why it  was important to go over the traditional way of learning word representations and then  we will see how it ties to the modern way or the neural network way of learning  representations  so we will start with the first neural network based model for learning word  representation which is known as the continuous bag of words model  so just to set the context the methods that we have seen so far are known as count based  models because they rely on these co occurrence counts for learning representations of  words and the methods that we are going to see now are called prediction based models  and it will become clear shortly why the term prediction and how they learn the word  representations  so in a way in the original thing there was no learning involved of course you can say  that you were trying to learn these eigenvectors and eigenvalues and so on but it was not  in the same way as it be as we have been learning parameters of a neural network and so  on right it was not in the same spirit but now once i do these second type of models this  distinction would become very clear one why is there a learning involved and why they  are prediction based models    so the story is we are going to look at continuous bag of words model then something  known as skip gram so this is the famous wordtwovec model which you guys have  already started looking at then we look at glove word embeddings which is some kind  of a hybrid between the count based models and the prediction based models and then  we see how to evaluate word embeddings and then end with this depressing note that  good old svd is just fine right  so all the progress that has happened in the past fifty-six  years you could just use svd and still go by but if you do that you will probably not get  a job right you have to learn these things    so now let us start with the continuous bag of words model so consider this task and  just bear me for a few slides that when why this is connected to our problem and all that  so i am going to consider ka task we are here to predict the n’th word given the  previous n minus one words right  as this is something that you do regularly sometimes  even in the class when you are whatsapping or smsing so this is what you do right you  start typing he sat on a and you get this prompt that the next word should be chair or  something like that right   now you can think of this as a classification problem tell me why you can think of this  as a classification problem can you tell me what is so remember that we have always  thought of this that there is always a y there is always an x and then we are trying to  learn this relation from x to y so given this example can you tell me what is x here and  what is y here   student refer time zerotwofiveone   everyone is clear about that right  so this is x and this is y now i made a statement  that i could think of this as a classification problem right  so the minute i say  classification what is the y that comes to your mind or dash hot by  student refer time three hundred and seven   y not i anything else would have been inappropriate but so one hot y is what you  would expect there right and now what is the size of this one hot vector  student size of refer time zerothreeone9  size of the vocabulary so we are trying to predict one of the words in the vocabulary  so you see why this is a multi class classification problem you see that there are many  classes and you want to select one of these class now the moment i say classification i  give you an x and y i will start asking me who will give me the training data for this so  can you think of training data for this any corpus similar to the one that you are creating  right    in specific what you will do is suppose you have framed it as the following problem that  you are given for words and you want to predict the fifth word so in general i have call  it as that you are given n minus one words and you want to predict the n’th word the n that  i am considering here is four  now what is going to be the training data for this if you take any corpus that you have  built anything right consider all five word windows from there do not get too engrossed  in the story so there are four the first four words you can treat as x and the fifth word  would be your y  so you can construct many such x comma y pairs from the raw corpus  that you are creating any five word window and you can keep sliding this window right  what i mean by that this is your first training instance x comma y this could be your  second training instance so this could be these overlapping training instances you keep  sliding this window and you will get many training instances ok you see that  so this task the advantage is that given the size of the web and so on at least for popular  languages the training data almost comes for free right  compare this to mnist or any  other task where you have to actually acquire these labels that this is an apple this is a  banana and so on here you get the training data for free just need to scrape it from the  web know no window size is something that you will set right whether you want to  learn four word windows or what do you mean we do not know the window size no  so this again there is a lot of existing literature in the traditional nlp where various and  lot of work has been done to figure out what is the right n so in most nlp task right if  you want to predict the next word a three word window is enough actually if you know  the last three words and you can try this as a mental exercise right if you know three  words you do not really need to know the words before that so this is the mark of  assumption with where this is a trigram dependency in the words right   so this n is not really so difficult and in the default tool that you guys would try  probably they take the value of n is seven that is an overkill but that is again it comes from a  lot of existing literature in nlp right this is not a this task is not deep learning broad  right this task is a simple language modeling task which has existed for many years right  from probably one9fivezeros or 6zeros or something  so this is all n word windows in your corpus as i said training data comes for free and  for ease of illustration we will now focus on the case when n is equal to two that means i  am given one word and i want to predict the next word    and we will see how to model this using a neural network so these are the two questions  which i need to tell you how to model this task and what is the connection between this  task and our original task of learning word representations these are the two things that i  am going to answer    so we will model this problem using a feed forward neural network what is the input  one wordok so say the word is sat i am going to represent it using a one hot vector ok  and what is the output i want to predict a distribution over all the words in the  vocabulary and i want to predict i want to pick the word which has the maximum  probability that is how you did so for example in the case when you had this  classification problem of banana apple orange mango you predicted a distribution over  these four classes and then picked the one which had the highest probability exact same idea  here it just that instead of four classes now you have v classes and your v is very large ok   it is trying to learn a distribution over there  and you know that in this case or the example that you are considering on is the actual  next word so you type sat and the next word is on and probably leading to sat on the  chair or something like that so this is what you would want to maximize ok i have  given you the input i have given you the output give me a neural network to model this  there are lot of hints in the diagram itself right you see some space between the input and  the output  so what will you put in the middle layer we will put a middle layer there right  is this  an ok way of modeling this task i have an input i want to predict an output so i just use  a regular feed forward neural network and let us analyze these parameters a bit more  carefully right  so i am something known as w context i have something known as w  word i am already using some notations from the svd lecture there at the end we  ended with w word and w context right it is not clear why i am using the same  notations but it will become clear in some time but let us look at their dimensions   so we have this one hot vector i have a parameter w context which i am going to learn  right and its size is k cross v so what does that mean this matrix is going to multiply  by the vector and give me a k dimensional output right is that clear so i have this is of  size v because always keep surprising me i do not know why you cannot do this r this is  a v dimensional vector you multiply it by a k cross v vector so you do w into x so  you will get a k dimensional vector so this is k dimensional you have a k dimensional  hidden representation   and from there now having captured this hidden representation you are trying to predict  which is the next possible class this is the same as any other thing right if you had done  the image classification or the mnist digit classification you had this seven8four dimensional  input vector you pass it through a hidden layer and then you predicted one of the onezero  classes there is nothing magic here it is the same thing that you have done seen before    how many if you get this and what are the parameters w context and w word ok and  we are going to focus on these parameters and understand what they actually mean    so what is the product w context into x given that x is a one hot vector so i will tell  you this suppose the i’th entry is hot here how many if you say it is the i’th column of w  context so it is simply the i’th column of w context why because you have this w  context matrix you take a one hot vector which has the second entry as hot if you do this  multiplication you basically get the second column of w and you can just see it everyone  gets this now how many if you get this now  so if you have a one hot vector if its i’th entry is on u multiply it by a matrix you will  get the i’th column of the matrix ok so if the i’th word is present in the input then the  i’th element of the one hot vector is on and the i’th column of w context can be would be  selected so then can what can you tell me about the i’th column of w context you see  there is this one to one correspondence between words in your vocabulary and columns  of the w context matrix how many columns has w context have v columns how many  words are there in your vocabulary v any one word is on only one column will get  selected and that is a unique column it is not going to change right  so there is a one to  one mapping between the columns of w context and the words in your vocabulary that  means the columns of w context are the are the vector representations  do you know these vector representations no these are parameters of your network so  they will they will be learned how we will see ok so you see the intuition for w  context setting it up this way so now i have set up the problem in a way that by  parameter matrix directly gives me the word representations ok but any kind of  learning has to be driven by some objective so what is that objective it is already clear  to a lot of you but we will just do that in a bit more detail so this is exactly what i have  just said    now how do you obtain p on given sat no no so for a given training instance so when  you so you could so i will so for a given training instance you said that your corpus  has been divided into those training windows right  so it is possible that engineer  sometimes the word does not and is not the next word but for this training instance what  is it so that is what you have to predict right is that fine    and at test time so you are saying that what you are saying is more practical that when i  have typed sat in the whatsapp message i do not want on as the always the answer so  we get these five options right three to five options so what could be that you have this  probability distribution pick the top five from there and show it as options so is that fine   so we are done with this now how do you compute p on given sat what is the actual  operation happening there what is the appropriate output function this is a multi class  classification problem softmax  this is what softmax looks like so the property if suppose on is the i th word in your  vocabulary then i am saying that the probability of on given sat is going to be this  quantity how many of you agree with that i mean those who agree is fine i am asking  why the others do not agree what is not clear about this i do not know how to explain  this i mean it is just so plain obvious what is the softmax function first of all you will  do this aggregation so you will do this w word into h that is fine right  so for you will  compute this vector consisting of w word into h fine what is the dimension of that what  is the dimension of that mod this is k dimensional this is v cross k or k cross v  depending on how you multiply it so what is the output going to be v so you have v  entries  these are dash entries the options are normalized unnormalized unnormalized now what  does softmax do  student normalization  normalization that is exactly what this formula is doing right  you want for the i th  word you see what was the end this product right this gave you a v dimensional vector  you look at the i’th entry there right that is what you are doing here raise it to an  exponent and divided by the summation of all these entries come on guys this is highly  disappointing i cannot teach softmax i had in the tenth lecture eleventh lecture of the  course right what is wrong how many if you get this now just have to ask of it tangle  you  so you see this right this is what is happening here so you get this v dimensional  vector and you just con converting into a probability distribution using the softmax  function so now this value how did he compute this value actually you computed this  product which is w word into h and then you took the i’th entry of that and then this  some transformation on that the softmax transformation you see that    so now i can say that p on given sat is actually proportional to the dot product between  the j’th column of w context and i’th column of w word why am i saying that  so remember that this was the i’th word in your vocabulary and on was the j’th word in  your vocabulary so can you explain the meaning of this sentence to me first let us  look at the first part what is h it is a j’th column of w context oh sorry this should be i  this should be j so this you already saw that h is the jet column of w context because i  am multiplying a one hot vector with the matrix is that fine and what is the i’th column  of w word so why what is this product actually equal to if i say w word into h w  word into h that is a vector and then i am taking the i’th entry of that so i am saying  that is the same as taking the i’th column of w word and multiplying it by h how many  if you get this is basically in your algebra right   now these four different ways of multiplying matrices i am just using one of those right   so if i multiply a matrix with a vector and then take the i’th entry of that that is the  same as multiplying the i’th column of the matrix with the vector ok just go back and  verify this just take my word for it for now so now what is happening is that it is  proportional to the product between the j’th column of w context and the i’th column of  w word is that clear now everyone gets this    so p word equal to i given sat does depends on the i’th column of w word ok  so now what can you say so earlier we saw that the i’th column of w context  corresponds to a particular word now what can you say about the i’th column of w  word it also corresponds to a particular word so now why these two correspondences i  already had a correspondence between w context and every word in my vocabulary now  i am saying that there is also correspondence between w word and every word in my  vocabulary how many of you first of all are comfortable with the sentence that every  column of w word has a correspondence with some word in the vocabulary  the second sentences every column of w context has a correspondence with some word  in the vocabulary do you all of you agree with both these statements okay that is what  we have try to prove so far so now for every word that means i have two columns  waiting for it how do i deal with this situation have you ever dealt with it before the  same thing happened in svd also right  svd also gave you this u sigma which was w  word and then v which was w context so you can always learn two different  representations for the words one is when the word appears as a context word and the  other is when the word appears as the target would you get that you see why we have two  different representations fine  and as i said hope you see the analogy with svd right you already saw there that there  were these two representation now given all this set up and please do not disappoint me  can you learn these parameters with some tweaks to the code that you have written for  mnist can you use the same code to learn these parameters how many if you say yes  so what is the tweaks what are the tweaks the input changes instead of the image  input you have this v dimensional input what else changes   student output  the output changes instead of a onezero dimensional output you have a v dimensional output  all of you are absolutely clear about this and what is the training algorithm   student refer time onesevenfive9   back propagation what is the loss function cross entropy good fine    so for some i will do some more stuff on this because there is some in interesting  interpretations of the gradient descent update rule here so i will refer to the word sat by  the index c and the word on by the index w ok and you already saw that the  appropriate loss output function is softmax the appropriate loss function is cross  entropy so let me just look at this right  so w was the index of the output word so  my cross entropy formula would just boil down to this everyone is fine with this ok i  will just try to maximize the w th entry in my y hat how many of you are fine with this  okay and that is nothing but the probability of the word given the context  now remember that h is equal to w context into x c i am going to call that as u c so u  c is the dash of the word sat title of the lecture it is a vectorial representation of the  word sat everyone is fine with that because that is exactly what this product is going to  do and now my y hat w is equal to this because i already said it is the product of the c’th  column of w context and the w’th column of w word    so now i have a formula for y hat w what is the training algorithm that you will use  gradient descent with back propagation now let us consider one such input output pair  and see the update rule for v w    so my loss function is this this is actually this quantity ok now i can just rewrite it as  this i have just expanded the log so the log of a by b is log a minus b ok now i want this  quantity because this is the parameter of the network right v w is one of the columns of  w context or is it w word w word v w is one of the columns of w word and i want to  learn i want to learn it what are the what are these column entries so that means i am  interested in this particular gradient  so i will start taking this so what is it going to be so only u c will remain here of all  these summation terms only one of them would remain and then you can derive de  derivative right  so this is what it is going to look like what is this quantity the  softmax function so this is what i get    so now my gradient update rule is going to look like everyone is fine with this i have  derived this formula and i have just substituted that here and this negative and this  negative ok so now let us look at this update rule    so this update rule has a very nice interpretation which allows us to understand what  does the continuous bag of words model actually learn now suppose y hat w tends to one  what would that mean your prediction is very correct right you are almost predicting it  has probability as oneok what would happen to the update in that case there will be no  updated if it is one there will be no update if it is close to one there is going to be very  minimalistic update that means you have already learned the v w well enough ok  on the other hand if i am very bad if y hat w is close to zero what would happen just tell  me the case when y hat w is actually zero what is the update rule have you seen something  similar ever before have you seen something similar before where did you see this  update rule perceptron what happened when you did this w and x came closer to each  other the angle between them actually decreased so the same thing is happening here   so what you are trying to do is you are trying to make your word representation closer  to the context representation is that clear how many if you get this it straight away  follows from the update rule right because you are adding a fraction of your context  vector to your word vector and we know that when we add two vectors they come close to  each other the cosine between them decreases that is what we proved in the word to it  lecture in the perceptron lecture right     so you can go back and refer to this slide on lecture two now so the training objective is  essentially ensuring that the cosine similarity between v w and context word is  maximized between the word and the context word is maximized    now what is the result of this now i want you to think go back with a starting example  where we said that we want to learn representations such that cat and dog are close to  each other but cat and truck are not close to each other  i want you to think whatever you see on this slide the conclusions that you drew from  this slide how do they help you to relate back to that initial goal ok so now let us let  me give you the intuition right  so what happens to the representations of two words w  and w prime which tend to appear in the same context c so say dog eats cat eats right   so dog and cat are two words which appear with the same context eats so what will  happen to the representation of dog it will come close to eats what will happen to the  representation of cat come close to eats not only that dog will also go close to pet  animal sleeps right and so on and cat will also go close to these so transitively what  will happen dog is going close to a certain point or certain sets of points cat is also  coming close to the same set of points so transitively dog and cat will come close to  each other you get this intuition  anyone sees a problem with this no so known objective and i said that dog comes  close to eats is that what he wanted i mean why should dog be close to eats that means  if i find the nearest neighbors of dog i will get words like eats sleeps barks and so on is  that what i wanted so that is exactly what is happening and based on that i convinced  you that dog and cat will come close to each other but there is a subtle gap here i want  you to close that gap how many matrices do we have two that is enough hint we are  going to either take columns of this matrix as the representations or the columns of this  matrix as the representation not mixed  so now can you tell me so dog here will come close to eats sleeps barks here word  will come close to context word right cat here will come close to eat sleeps and so on  right  so transitively dog and cat will come close to here and this is the representation  that you care about not representations across these two matrices ah so what i said is  that the training rule ensures that the words representation comes close to the context  words representation ok that is what we saw with the training update rule  so that means dog will come close to any kind of context word that it appears with so  dog i would expect it to appear with context words like eats pet animals dog barks  drinks and so on right  so dog is coming closer to these words and i expect cat also to  come up here with these words and of course i do not expect truck to appear with these  words right  so then cat will also come close to these set of words dog will also come  close to these set of words so transitively dog and cat will come close to each other  right all of them are coming close to each other ok which is fine which was my original  goal  but my original goal was not that dog and eats should come close to each other because  eats and dog are neither synonyms when they do not have any semantics i mean they  have a semantic relation but that is not what i wanted i wanted similar words to come  close to each other but now i have the side effect that dog is coming close to eats but  that is bad was how can i live with that  so the i mean the key thing that you should notice is that you have one matrix of words  the other matrix is of context words so the representation of dog in the word matrix is  coming close to the representation of eats sleeps etc in the context representation on the  context matrix the representation of cat is also coming close to these words in the  context representation and transitively because of this dog and cat in the word matrix are  coming close to each other and this is the matrix that we care about  in this matrix dog and eats dog and sleeps are not close to each other right is that fine  everyone gets this now ok so this is only an intuition and this becomes very tricky  when i will blow this up what do i mean by blow this up right now what am i trying to  do what is the size of n two right i am taking one word and outputting the other word  hence you get all these neat interpretations that you are moving close to that vector and  so on the moment i add more words to n these interpretations become more and more  hard right but this again i mean this is good to understand that this is what happens at  least in the best case so this is only an intuition which is reasonable in my opinion i  have not come across a formal proof which says that this is what actually happens and  that is one criticism for wordtwovec it works very well but there is no formal proof which  tells you why exactly it works  as opposed to svd right there we know there is a principle behind it here that is not  very clear right but it works very well based on this intuition ok so everyone gets the  whole set up how we started with a classification problem of predicting the n th word  given the n minus one words which had nothing to do with word representations that is a  simple language modeling problem which has existed forever we smartly modeled it or  someone smartly modeled it using a neural network such that the parameters of the  neural network end up giving you the word representations and this network is end to  end trainable using an objective function the training data comes for free for popular  languages you have like tons of training data the entire wikipedia entire web whatever  you can scrape that is why with more and more training data you can learn even better  and better representations so for popular languages the representations are really good  and then we saw an intuitive explanation for why this works because of this movement  of things closer to each other and the key thing to notice there are two different  representation matrices one for the words one for the context and this is not surprising  the same thing happened for svd also u sigma was w word and v was w context  right  so it is all in the same spirit right     now in practice instead of window size of one it is common to use a window size of d  either d could be four or seven i have i have even say and seen oneone actually but not beyond that  ok now let us see what happens if you have two and here itself it should become clear that  now those interpretations are not very neat so what i will use suppose i want to take a  context of two words then i have he sat and now i want to predict the next word  so what is my input now he and sat right  so i will take the one hot representations  of he and sat i will just concatenate them sorry i just concatenate them is that fine and  my input now belongs to r raise to two v in general it will belong to r raise to d v ok and  now what is the next step do you see something funny here i have just created two  copies of this ok i am telling you an inefficient way of doing this later on it will be a  very simple thing to do a very efficient way of doing this right but first just to get the  math around i will just do inefficient way of doing it  why have i staged it twice two words right  so now my h is actually going to be the sum  of all the columns of w which correspond to my input words is that fine i have to earlier  i had just one word as the input so my h was just equal to that column of w now my h  is going to be equal to the sum of all the columns of w corresponding to the words that i  have and i will tell you why so i have taken w contexts comma w context which is just  the w context matrix staged twice back to back so this was my w matrix this is my two  hot vector because i have two inputs now right  so my vocabulary size is three so the first  one hot vector followed by the next one hot vector and now i am going to repeat w w  now what is the product of this is the sum of the two columns that you see highlighted  right and exactly that is what i have written here  so if i do it this way then i can just do this very expensive matrix multiplication and to  do a something very trivial which is just taking the sum of two columns right  but at least  you get the operation and i will just on this next slide or something i will tell you an  obvious simple way of doing that so i just get the sum of the two columns so that is the  input to my network if i had k words as input if i had my window size four what would it  be i would have these four copies of w context i will have these four one hot vectors and it  will just give me the sum of those four columns  ok that is going to be the input ok and the  rest of the story remains the same right  once you have this h the rest of it from there  remains the same and this is the formula for h in general  in the special case it was just the i’th column in the general case is the sum of all the  columns that are there in your input    now in practice of course this is a very mate expensive matrix multiplication it is  stupid to do it that way what you will do is you will just slice of those columns from w  context right and then just add them up so you do not really need to do that stupid mate  matrix multiplication because you know that the matrix multiplication is essentially just  selecting these columns and adding them so just select those columns and add them up  so you do not do that bad matrix multiplication operation is that fine    now what happens during back propagation in this case in the generic case the  ordering does not matter is what you have seen yes it does not matter yeah there is some  assumption of the model so it is that is why the name of bag of words you are not  relying on the sequence so this comes from nlp that if you rely the sequence you call  it sequence if you just going to take the words in the sequence you just call it a bag of  words because once you put them in a bag there is no ordering there right that is why  the word name bag of words  so and again p on given sat is given by this softmax formula ok now tell me during  back propagation and if you give me a right answer to this i really feel happy that you  have understood everything right from the beginning of the course so no pressure so  which are the parameters which are going to get updated during back propagation which  are the two large matrices w word and w context so obviously the answer is not w word  and w context otherwise i would not have asked you the answer is some dash of these  two some subset of these two which subset let us start with w context which is the input  do we are we going to update the entire w context did it participate the entire w context  participate in the computation only those columns corresponding to the words so only  those parameters will get updated   so how many columns will get updated d columns right  w word till all the columns  of w word participate in the computation how many of you say yes how many if you  say no the others do not care can you just focus on this circle did all the columns of w  word participate in the computation you see the summation at the bottom it is over all  the columns of w word all of them participated so the parameters which will get  updated are w word and all the columns of the input words and same back propagation  will work again is that fine  so remember that and this is i cannot emphasize it enough whatever i have explained is  only for an intuitive explanation you will never ever do this matrix multiplication right  and that is why what you are going to do is you are just going to select those columns  add them up and feed them and the network will take care or rather you will take care  that you update those parameters only and you do not update the entire w context matrix  because anyways there is no gradients flowing to the other components so remember  that in the practical implementation of w of word two vec do not search for this matrix  multiplication at the input or if you are writing the code on your own which is highly  unlikely do not do it that way  so if you whatever code that you look at did not have this complex matrix multiplication  typically they will just pick up the columns and add them and feed them right and i  think the tensor flow way of doing is you have this word embeddings matrix and you can  slice columns from there and so so everyone understands this so far now what are  these problems with this why is this not as simple in some sense as the mnist data set  again focus on the circle this softmax computation is a very expensive operation right  you have a v cross k sized matrix somewhere there and unlike at the input here you will  have to do this matrix multiplication right   so we have a v cross k matrix multiplied by a k cross one vector and there is no  simplification of this you have to do this multiplication what are the sizes of v that we  saw in practice fivezero k onezerozero k and if you had googled onethree million or something right  so  this is not feasible we cannot do this expensive matrix multiplication    so although all of this works very fine we need to think of ways to simplify this  softmax computation where the denominator requires the summation over all the words  in the vocabulary so you have to do that many matrix multiplications  
task2/super_cleaned_audios/lesson79.wav,617.3475,so with that we will move on to the next model i am still not telling you how to solve this problem we will come to that later i am just going to the next model which is the skip gram model ok and this is the famous wordtwovec which you are trying to implement the model that we just saw was known as a continuous bag of words it predicts the output given the context skip gram model does the reverse of that you are given a word you want to predict all the context words so now i am given the word on i am trying to predict the words which appear on the left and right side of it is that fine so how many prediction problems am i solving how many predictions am i giving you four in this case right so you see that this is a case where your y actually belongs to r four right of course it is not r four it is four into v and because you are predicting the entire distribution but what i meant is that you want these four different outputs you just do not want one single output apart from that does everything else remain same you have an input word you compute a hidden representation from that hidden representation you try to predict the outputs you get a probability distribution what is your loss function it is a dash of cross entropies sum of cross entropies how many cross entropies do you have four in this case and also notice that i have i hope i have yeah i have changed w word and w context they are flipped now is that fine the role of context and the word has changed in the simple case when you are trying to take one word as the input and only predict one word around it it just becomes the same as the first case that we saw in the continuous bag of words there is no difference there because there also you take one word and predicts the other word so the entire math’s remains the same how many of you get that and even when we have multiple context words our loss function is just going to be the sum of the cross entropies for all those d predictions that i need to make or d minus one predictions that i need to make and then once i see a loss function which is a sum of some things i am not worried because i know how to deal with each of these components and gradients are additive so if you have the gradient of some it is just the sum of the gradients so as i long as i know how to deal with one of these i can deal with the sum so that is why i do not really worry all of you are at that level where you do not worry with the sum as a loss function what are the problems with this already written there same as the bag of words right now we are doing these four expensive computations at the end so the softmax computation is expensive there are three different solutions and there are three different ways that we can deal with it one is something known as use negative sampling the other is to use contrast of estimation and the third is to use a hierarchical softmax so we are going to see all of these and i will shamelessly continue for a few more minutes so first we will see use negative sampling because that is very easy so let d be the sat set of all correct w comma c pairs in the corpus what do we mean by that all words which actually appeared in the word comma context pair so you can look at the vector which i have constructed so sat on sat or sat chair you can imagine that all of these appeared in the context of each other so this is my correct corpus as from what i got from my data now let d prime with a the set of all incorrect w comma r pairs in the corpus and r here stands for random some how am i going to construct this corpus so i take a word i know all the words which appeared with it and i know all these other words which have not appeared with it so i will randomly sample a word from there and put it as r is that fine so i can compute d prime again d prime comes for free d was always for free now d prime is also obviously for free so i have w comma c and w comma r and i have these corpora d and d prime and as before let v w be the representation of the word and u c be the representation of the context word ok so v w will try to these two and you see will try to this and u r something else that we will use for this hopefully is that fine okay now for a given w comma c which belongs to d which is the true corpus what are we interested in maximizing so let us think of z is a random variable weather which tells us whether this is a true pair or not so given w comma c i want to maximize that p of z is equal to one ok now this is what i want to maximize now it depends on me how do i model this probability so can you guess how am i going to model this the answer is there in the figure can you tell me what is the model that i have chosen can you tell me what is the formula for z equal to one given w comma c that i have chosen this stands for dot product this stands for the sigmoid function i know this is some uc representation this is some vw representation note that these representations are not learned yet i need to learn them using the training objective that i set but at the beginning they are initialized to some random values and the way i am going to model probability of z equal to one given wc is that i am going to say that it is just the sigmoid function of the dot product between them how many of you get this are you comfortable with this this is the modeling choice which i have made or rather the authors of skip gram right now how am i going to now what do i want to do for all w comma c belonging to d i want to maximize this probability is that fine for all the w comma c pairs which belong to my true corpus which is the d corpus i want this probability to be high how many such pairs do i have many many right so let us call them as i have n such w comma c pairs so can you tell me what my loss function is going to look like maximize this for the first pair and for the second pair and for the third pair all the way up to the end pairs so what is it going to look like for every w comma c which belongs to my correct corpus i want to maximize that probability of z equal to one given that w comma c pair right and since it is an and i will have this product how many of you are comfortable with this so this as such and this is something that you do regularly you should have done this in machine learning or pattern recognition or somewhere right that you want to basically maximize the log likelihood of the data which is saying that you want to maximize the probability of every training instance which is saying that you want to maximize the and of all these probabilities right be you take the and of all of them is that fine now for the other case wr belonging to d prime what is it that i want to maximize this probability right because i know this is an incorrect pair so i want my random variable to output zero ok now what is this going to be the probability one minus the probability that it was correct and that actually if you just simplify a bit it turns out to be this now for all the elements which belong to d prime what is the objective function that i have i want to maximize this for the first w comma r random pair for the second w comma are random pair and so on for all the random pairs in my corpus so it is just going to be a product of all these probabilities is that fine so now what is my total objective function for every pair in d maximize that for every pair in d and for every pair in d prime maximize the zero probability so what is the total going to be is this fine how many of you agree with this so for everything belonging to d i had this and rule for everything belonging d prime i had another and rule and i am interested in both the acts right maximize for d and maximize for d prime of course different quantities for d and d prime ok fine so you get this once you basically take the log and so on so this is a simple set of math operations that i do you will end up with this neat formula ok that for all the w comma c pairs belonging to d you want to maximize this quantity which means you want to maximize what you want to maximize the when will this quantity be maximized when the dot product between the two is high that means again what are you doing we are trying to bring the context vectors close to the word vectors again transitively what will happen the words which appear in the same context will go close to each other what is the additional thing that you are ensuring here the words which do not appear in the same context you are trying to push them apart why because of second loss function you see the difference between the two now in the first case you are only opt i mean you are obsessed with bringing things close together here you are also focusing on the case that where you do not want certain things to be close together because they never appear in the same context is that fine so you see that this is a more powerful loss function in the earlier one so that is what the skip gram model does and in the original paper mikolov et al sample k random pairs for every positive pair right so that means if your size of d was n what was the size of d prime b k into n so that they had that many positive examples and k times that the number of negative examples and this was a hyper parameter which was tuned and they used a value of k such that it gave them the best results also remember that we have this problem of constructing w comma r now i said that consider all the words which do not appear with your word and sample from there and put something there so they used a slightly that means how do i sample that one is i assign all the words a uniform distribution that every word is equally likely what is a better way of doing that okay i think i just finished this next time 
task2/super_cleaned_audios/lesson45.wav,1499.5879,so in this module we will talk about principle component analysis and it is different  interpretations in this model we will look at one interpretation and then in the rest of the  module some other interpretations  so the story i add is going to be this we will talk about pca and it is interpretations ok    so now let us try to motivate pca first consider the following data ok in what  dimension is this data  student two dimension  two dimensions it is r two ok and each point here is represented as it is x coordinate and  using it is x coordinate and it is y coordinate ok now it means that were using x and y as  the basis right that is clear that is the standard way that you would do any data point  you will just represent using that basis  now what if we choose a different basis let me give you one basis and then let me ask  you some questions on this    suppose we chose this basis so in the previous modules we made a case for the x and y  coordinate axis there is nothing sacrosanct about it you could use any basis the only  condition on the basis that the vector should be linearly independent and in fact if they  are orthogonal it is even better right  so now i have given you a different basis now what do you make any observation here  so they have all the points here have a very small component along the u two axis right  so now this so far this point right if i consider at this point then this is the component  along the u one axis so that is it is u one coordinate as akin to the x coordinate and this is it  is u two coordinate akin to the y coordinate is a are the arrows clear here  so that means there u two coordinate is very small and it is also very small for all the data  points right so it is almost as if there is some noise there it is all within some epsilon  now so it seems that the data which were actually represented in r two can actually be  represented in r one by getting rid of this noisy dimension right so if you had chosen a  different basis you realize that with just one dimension you could have captured  everything that was there in the data and the other dimension was just adding noise it  was redundant there is hardly any information there    so now can you state this more formally because this is this intuition but can you stated  more formally in terms of things that you have learned and say probability for it for  example what is wrong with the direction u two the spread of the data points along the u  two direction is very small what is the spread mean the variance right so we do not care  about u two because the variance in the data along this direction is very very small ok and  in particular right if i were to build a classifier then would u two have any predictive  power because along this dimension the points are indistinguishable ok so think of it  that you are trying to find out whether you have so you have say onezerozero candidates and  you want to decide whether they would be good basketball players or not  and quite naturally all the people that have shown up are say six foot two and six foot three inch  and so on and there in a very small height difference between them and all of them are  sixtwo is the average and very close the spread is not much so this feature is not going to  help you decide whether this person is going to be a good basketball player or not you  will have to rely on other features where the variance is more for example how many  teams has he participated in the past how many matches as he won as a team as a  member of some team and so on it  so those who expect some spread to be there all these onezerozero candidates might have  different things right but if the height is the same for all of them it is not going to be a  good predictor and that is exactly what is happening along the u two direction the points  are almost indistinguishable there that is why it does not matter    so in general given any data now this was a simple case where the data was r two i am  talking about the general case where the data is rn right and you will find this situation  in higher dimensions also so you would not want to use that entire n dimensional data  where you know that there are some columns along with the variance is very small so  you want to represent the data with fewer dimensions such that the data has high  variance along those dimensions  now let me just clear a confusion here right so i am not saying that take your n  dimensional data ok find the variance across each of these dimensions and then throw  away the columns which have the lowest dimension in this particular example if you had  done this what would happen could you have done that think of the original  dimensions x and y along these two dimensions there is enough various in the data  right the x coordinates vary from here to here and the y coordinates also vary from  this point right up to that point right so there is enough spread in the x and y  coordinates  so in your original data i am not saying that pick look at each column and see if there is  no variance along that column then throw it away that would not work because you  might end up with the situation that there is enough variance across each of these  dimensions it is just that when you look at the data from a different angle that means  you projected onto a different basis this becomes clear right  so you see the difference i that is not the same these two things are different operations  so what i am looking at is projecting the data to a different basis that is exactly what i  did with u one and u two and then some things became clear about the data now this  projection along a different basis i would be interested in doing that only if i can get rid  of the number of dimensions right if now i had already had one basis where i had n  dimensions now if the new basis is also going to be that all these new n dimensions that  i have come up with are important then you are not gaining much i do still have this  high dimensional data but you would like to project it in a way that you get rid of the  lower variance dimensions  so you might project it onto n dimensions but you want to rank these dimensions  according to variance and then throw away some of these dimensions is that clear is the  objective clear ok fine is that all that we care about n dimensions’ project to a new  basis and throw away the key dimensions which have less variance is that all what else  would you want people have done the mlpr course no i would not so i am not going  to classification or anything i just want a better representation of the data at this point i  am not really thinking about what i want to do with the data maybe you are talking in  terms of classification and we have already seen even if the data is not linearly  separable we have solutions for dealing with right so that is not a critical point ok so  there is something else that very interested in and let us look at that    now consider this data i have three dimensional data ok do you find something odd about  this data  student refer time zerosix5six  y and z are  student refer time seven hundred  are highly dash  student correlated  correlated right do you want these dimensions can you think of any practice such  dimensions occurring height in centimeter and height in inches someone would have  just given you data right or if you if you take the credit card a credit card fraud  detection case right someone would give you the salary and it would also give you the  income tax now these two are highly correlated right  so then you do not really care if you have one you could probably almost with certainty  predict the other right modulus some rules right because you get some tax exemptions  and all that but still so you can have this in practice but even in our oil mining case  your salinity pressure density those things could be related right so z is not adding any  new information beyond what y is happening so the two columns are highly correlated  so actually yeah this is the formula for correlation all of you know this anyone who  does not know this formula good so not nothing is a stupid question right so you can  always ask  so y hat is the mean of this column ok sorry y bar z bar is the mean of this column and  this is how you compute correlation this is just the formula ok so from every entry you  subtract the mean ok so this is known as centering the data so if you do this what  would the mean of the new data be  student zero  zero right so that is why it is called centering the data ok so i will have zero mean zero mean  and you so what does this what is the intuition behind this formula does anyone  know can anyone tell me so this is a summation ok so this quantity is going to be  high if the summation is high it is a summation of some n terms now these terms could  be positive or negative if all the terms are positive what would we happen to the sum  student refer time zero85zero  it would be high if there are some terms which are negative it would be low now when  would all the terms be positive whenever y is above the mean z is also above the mean  right therefore this quantity is positive this quantity is positive whenever both are  below the mean again the product would be positive when one is above the mean the  other is below the mean then there is something wrong happening right and in that case  you will have a negative term right  so for more details of course you can refer your other textbooks and so on but this is  just the intuition an important step here is to zero mean the data right we are computing  the subtracting the mean of the data ok another way of saying this is that the column z  is actually linearly dependent on y ok it is almost linearly dependent i of course have  some noise twoone zerosevensix and so on but it is largely linearly dependent i can get i can write z  as some c times x fine ok    so now can you tell me the refined goals that we have we are interesting the  representing data using fewer dimensions such that remember that when i say fewer  dimensions i mean a new set of dimensions right it is not throwing away dimensions  from the current data we are looking for a new set of dimensions what are the  conditions that we want from these new set of dimensions  student refer time onezeroonetwo  one there should be high variance along these dimensions the new dimensions and  student refer time onezeroone5  the dependence are linearly independent or uncorrelated ok fine  and even better of course if they are orthogonal why  student refer time onezerotwosix  because we are looking for a new dash  student basis  basis and the most convenient basis is  student orthogonal basis  orthogonal basis ok fine    so now let us assume someone has given us this new basis ok and let us call this p one p  two pm so instead of this x y z and so on someone has given us this new basis eventually  we will of course figure out how to find the basis but let us assume that someone has  given this new basis right and they are both linearly independent and actually it is  redundant actually so yeah this example of a redundant feature such an orthogonal  vectors is sufficient they are linearly independent  let p be an n cross n matrix such that p one p two p n are the columns of p right same thing  as we had put the eigen vectors in a column and probably i have unknowingly given out  the solution but ok and let x one to xm be the m data points given to us ok so we are  given this data as usual we have this x matrix each one of them belongs to rn and we  have m such data points right that is the standard thing that we are operating and you  always write this as a matrix ok and we have already done the data is zero mean and unit  variance  actually unit variance is not required but the data is zero mean fine that we will sorry i am  going to deal with covariance as a unit variance is not required so the data is zero mean is  what i am going to assume but what if the data is nonzero mean i can always make it zero  mean right  so just to remember this is an important trick that you will always have to use whenever  you are doing any large scale machine learning so whenever you are participating in  kaggle competitions almost the first thing that you do is standardize the data that means  make it zero mean and unit variance so why is that important  student scaling  right scaling issues would not be there right so if i have something in centimeters and  some other unit in kilometers right now remember that always you are doing  somewhere this linear operation w transpose x you might add a nonlinearity on top of  that but now if your xi dimensions some of them are in the range of zero to onezerozerozerozero some of  them are in the range zero to onezero right  then there is some abnormality here right some dimensions are winnings in terms of  their magnitude and some dimensions are losing out right that is why you always make  it unit variance and you also make it zero mean you center the data ok so we will assume  this and if we all understand if the data is already not zero mean and unit variance we can  always make it zero mean and unit variance just scale it and make it centered    now we want to represent each xi right so xi is one of these data points that we had  that means one of the rows of our matrix ok and you want to write it as a linear  combination of this new basis  so if you have any basis any vector you can write it as a linear combination of that basis   is it fine so far it is ok ok now for an orthogonal basis we know that we can compute  these alphas just by taking a dot product of the vector with the dimension ok and just  repeating some of the things right    so now let us see what this means for one of the dimensions this is my data point xi  which i want to transform ok for one of the dimensions i just had to take the dot product  with that dimension and this will give me how many values one value that means the  coordinate along p one i want to do it for all the n of them i can write it as this vector  matrix multiplication right what is the dimension of this  n cross one how many if you get that ok so this oh not many why  student refer time onethree55  one cross n fine that is fine yeah how many of you get this ok fine yeah so this will give  me all the n alphas is that clear for this data point  so it will give me alpha i one to alpha i is it    now i want to do this for the entire data right so i have done it for x one i also want it to  be done for x two and all the way up to x m for each of these i would have such an  operation where i have a vector multiplied by this matrix if i just stack all these vectors  i get back my matrix x and the whole operation i can write as x into p ok is that clear to  everyone ok what is the dimension of x into p  student m cross n  m cross  student n  n right so for all the m data points i have alpha oneto alpha n is that clear anyone who does  not understand this  so x hat is the matrix of the transformed points is that clear i have now the new  coordinates instead of the original coordinates according to the coordinate axis i have  the new coordinates in this matrix    now i will just go through some very simple theorems or rather results and i will not  prove them you can prove them on your own or other proof is there in the slides we can  look at it later on right so if x is a matrix such that it columns have zero mean and if x  hat is equal to xp then the columns of x hat will also have zero mean is this obvious to  most of you not really is it how many of you think it is obvious ok then let me just go  over the proof  so for any matrix a one transpose a right so that means you have this vector this is a  vector or a matrix yeah this is a vector right so i have a vector of n one so one this is  nothing but a vector of n ones so what is this product actually going to give me  it will give me a vector containing n elements what is each element  student sum of that column  sum of that column right is this fine ok this is very obvious to see from if i have this  suppose i have two three one and three six seven ok and then of course the corresponding    so if i do this multiplication i will get a two dimensional output which would be just seven and  onesix right so that is just the sum of that column  student refer time onesixthreethree    so now we have this x hat that is the transform matrix now let us see if i do this  operation i x hat what happen i can write it as this i can club it as this what is this it  will be all zeros because the original matrix was mean zero that means the of the elements of  all the columns each column independently was zero that what this is going to be a zero  vector so zero vector multiplied by any matrix is going to be zero now is it obvious i  hope this is obvious x transpose x is a symmetric matrix i still have the proof for that    now if x is a matrix whose columns are zero mean then a matrix sigma which i am going  to call as a covariance matrix which is given by this is actually the covariance matrix  how many of you agree with this how many of you have seen the covariance matrix  before so all of you agree that this is the covariance matrix if you do not please raise  your hands if you do not you will not understand the rest of the stuff now you have to be  given the right incentives  so let us see be the covariance matrix of x now what is the covariance matrix actually  first of all tell me that if i say that i have an n cross n matrix x    let me not make it any cross n let me make it m cross n ok what does the covariance  matrix actually capture what is the dimension of the covariance matrix first of all  student n cross n  n cross n ok and what does each entry of the covariance matrix capture the covariance  between the i’th column and the j’th column    student refer time one8zero9  so the entry ij of the covariance matrix captures the covariance between the i’th column  and the j’th column is that fine now what is the formula for covariance suppose i give  you two columns right let us see i have give you x one one x one two x one three and x two one x two two x two three can  you give me a formula and of course i will go up to k or rather m  so what is the formula summation  student refer time one85two  i equal to one to  student m  m  student refer time one85two  mu one  student refer time one85three  mu two anything missing  student by m  by m anything else in the denominator no no is it fine ok so an what is mu one mu one  is just an average of this ok so this is the covariance formula now if the mu’s are zero  then what does this boil down to  student refer time one9one4   x one i into x two i  what is this quantity actually  student refer time one9twotwo  this is the dot product between the i’th column and the j’th column fine ok now that is  pretty much the explanation right so now the c ij’th entry is supposed to be given by  this formula if the means are zero you are just left with this formula and this is nothing  but the dot product between the i’th row and the j’th i mean the i’th column and the j’th  column is that fine  and now if you write it as a matrix then you can just say that it is the ij’th entry of the x  transpose x matrix everyone gets this no one has any confusion the people who raised  their hands fine good    so we now this is where the we are so far that we have assumed that someone has given  us these dimensions’ p one to pn which we have put in the matrix p right and we have  also made a case that x into p which is what i have written here actually is just a  projection of the original data onto this new basis right everyone gets that ok and i am  calling that new projection or the new result that i get as x hat so that is what my  transform data is  what is missing here  student refer time twozero4two  we do not know what p is that i am assuming someone has given me that p now i need  to figure out what is the p here now using the previous definition we get that this is the  covariance matrix of the transform data so let us just write that this is fine this is fine  what is this  student refer time twoonezerothree  covariance matrix of the original data ok so i will just write it as sigma fine ok now  each cell ij of the covariance matrix towards the covariance between columns i and j of  x hat where x hat is the transformed data what is the property that you want to hold i  give you two conditions or i will give you only one condition for now when i is not equal to  j  student refer time twoonetwo8  zero ok so what should the covariance matrix look like  student refer time twoonethreeseven  remember that this is what is this this is the covariance matrix of the transformed data  right that is what i started with right this is the covariance matrix of this transformed  data what do i want this covariance matrix to look like  student diagonal matrix  a diagonal matrix ok because i want every non diagonal element to be zero right and this  point i am not telling you what i want the diagonal elements to be i am just telling you i  do not want them to be zero  well if it is zero what would that mean  student refer time twotwozero5  that is the variance right if you take the along a diagonal what you get is the variance it  is if it is not clear right now well return back to that right now we just know that the off  diagonal elements are the covariance between the i’th and j’th column and we want that  to be zero so we want this condition to hold this is something very new that you have  never seen in this course before they have actually not seen in this course before have  you seen this or not  student refer time twotwothree  thank god fine  so what is this  student diagonalization  the diagonalization of which matrix this matrix right and what was this matrix it was  x transpose x this is clear so what is the solution all rows always lead to  student eigenvectors   eigenvectors right    so we want p transpose sigma p to be a diagonal matrix and we know which are the set  of vectors which i put in p such that they will diagonalize sigma  student eigenvectors  eigenvectors of  student refer time twothreezerosix  x transpose x right ok wait why did i put this it is the matrix of the eigenvectors right  so it is a matrix of the eigenvectors of x transpose x  so now have we finished it do we know principal component analysis now so we  started with the intuition that we wanted to transform the data ok i cannot stress enough  that we want to transform the data not chopped off dimensions from the existing data ok  that means we need to project the data to a new basis and we had a couple of conditions  the variance should be high and the covariance should be zero we have satisfied one  condition which is the covariance is zero and we arrive at a solution which says that the  eigenvectors forms the basis that you should project on so that the covariance would be  zero  so we have a solution we know exactly which basis to use to represent the data ok so  that the covariance condition is satisfied what about the variance did we do anything  about the variance  student refer time two4onezero  so we will come back to that ok fine    why is this a good basis what does the what is a good basis the best basis  student orthogonal  orthogonal right because the eigenvalues of x transpose x are linearly independent that  ok and they are also orthogonal because x transpose x is a dash matrix  student refer time two4two4  ok  good real symmetric  so this method is called the principle component analysis for transforming a data to a  new basis and that where the dimensions are nonredundant because they have low   covariance and not noisy because they have high variance the second part i have not  proved right and i will get to that at some point fine no that is what we saw right no  what is i did not get that now in practice how many eigenvectors would you have  student n eigenvector  n eigenvectors do you want to keep all of them which ones will you throw away  student refer time two458  the low variance ok  and now in the next interpretation actually we will try to see what is the what happens  when you throw away the least important dimensions right what do you mean by the  least important dimensions  
task2/super_cleaned_audios/lesson51.wav,3110.6605,autoencoders sparse autoencoders contractive autoencoders  welcome to lecture seven of the course on deep learning cs sevenzeroone5 in this lecture we are  going to talk about auto encoders and we will focus on their relation with pca then talk  about regularization in auto encoders wherein we will look at denoising auto encoders  sparse auto encoders and contractive auto encoders so let us begin with the introduction  to auto encoders what they are  so this is what a typical auto encoder looks like and as you can see this is very much  like a feed forward neural network you have an input which is x i so you are given  some training data you are given some i samples x i to x n so this is your training  matrix x which we have seen in the previous lectures so this is one of those training  inputs x i and then you have a hidden layer and then an output layer so let us look at  what is the configuration of the hidden layer and what does the output layer actually try  to do    so it is a very special type of a feed forward neural network what it does is it encodes  with input x i to a hidden representation h ok and it uses an encoded function to do this  so this is what the encoded function does it first does a linear transformation  so w is a matrix and x i is a vector and you again have the bias b as a vector right so  let us look at these dimensions right so let us try to fix some dimensions so suppose x i  belongs to r n that is what we have been considering throughout the course so far and  let us say h belongs to r d  so it is a d dimensional representation so in that case what would w be yeah so w  would be r n cross or the d cross n right so it will multiply with the n cross one vector  which is x i and give you a d cross one output right and similarly the b would also be d  cross one and then on top of that you have this non linearity g which will be operating at  element wise just as we had seen earlier so it could be any of the sigmoid functions the  logistic or tanh and so on  so the end result is you have taken an input x i and encoded into a hidden represent h by  using a linear transformation first and then a nonlinear transformation right so i refer  to w x plus b as a linear transformation because it is a matrix multiplication now once  you have constructed this hidden representation       the job of the decoder or the latter half of the feed forward neural network which is this  half is to take this encoded representation and then try to reconstruct x again from it  so again let us first look at the equation so this is the equation for the decoder where  again you first take the hidden representation do a linear transformation and then you  again have some function on nonlinearity on top of it right so we will see what this  function can be so we will refer to it as f for now we will not say whether this is sigmoid  or linear or what kind of a function it is we will come back to it later on  so now let us again look at these dimensions so what is x i x i is again r n and your h  was r d so you have to go from a d dimensional input to an n dimensional output so  again your w star is going it to be r d cross sorry r n cross d so it will multiply with a  d cross one vector and give you an n cross one output right and that will pass through some  function and it will give you x i hat which is a reconstruction of x i  so why are we trying to do this right we took an input x i we computed it is hidden  representation by doing some nonlinear and linear transformation and then again we are  trying to reconstruct x i hat so why are we trying to do this so reason we are doing  this is that we want to learn what are the most important aspects or most important  characteristics of the input data x i right so if you compute a hidden representation h  which is presumably smaller than your original input data  and from that hidden representation if you are able to reconstruct x i right then that  would mean that this hidden representation captures everything that is required or  everything that is yeah everything that is required to reconstruct x i from x i from the  original input right    so the model will be trained to minimize the difference between x i and x i hat so you  want to make sure that after passing through this bottleneck which is the hidden  representation you are able to reconstruct x i and the reconstructed output is very close  to the original input right  so can you see an analogy with pca where you are trying to find this hidden  representation or this most important elements of the original input x i so there we had  used this linear transformation where we are taking the original input x and transformed  it to a new basis and we had used that basis for representing the original input right so  something similar is happening here we are using this hidden representation h to  represent our original input    now let us consider a few cases the first cases when the dimension of h is less than the  dimension of x i in this case as i was trying to say earlier if we are still able to  reconstruct x i hat perfectly from h then what does it say about h it tells us that h is a  loss free encoding of x i it captures all the important characteristics of x i write just  repeating what i had said on the previous slide and now you can see an analogy with  pca because h has all the important characteristics required from the original input data  so it has probably got rid of all the noise or all the low variance dimensions or the  correlated dimensions and so on and this is just the compact representation which is as  good as the original representation and from there you can reconstruct the original  representation and such an auto encoder where the dimension of the hidden  representation is less than the dimension of your original input is known as an under  complete auto encoder      now let us look at the other case where the dimension of the hidden representation is  greater than the dimensional of the original input ok such an auto encoder is i will tell  you what it is called so we will we are looking at the case where the dimension of the  hidden representation is greater than the dimension of the original input  so now in such a case the auto encoder could learn a very trivial encoding by simply  copying x i into h and then copying h into x i right so think of this from a compression  point of view right so now suppose you have onezero bits initially right and then you want  to somehow compress it and store it only in four bits  and now this four bits should be such that it captures everything that was there in the  original onezero bits because you would want to reconstruct the original input again right so  this is what we do typically when we compress any of our files right we have a larger file  we compress into a smaller information while making sure that everything important is  there so that whenever i want to recover it i can just recover it from there  so this is definitely a hard task but now what i am doing in this auto encoder is that i  had onezero bits i am actually giving it more bits now because the dimension of h is greater  than the dimension of the input and then from these onesix bits i want to reconstruct the onezero  bits now this is a very trivial task right because all i could do is copy these onezero bits into  the first onezero bits here leave the remaining six blank and then from those onezero bits just  reconstruct the input that is very very trivial if you give me more storage and what i  originally needed then definitely i can easily reconstructed    so this looks very trivial and this is what it could do right just copy the input to the first  the n bits    so this was n and this was d and we are looking at the case where d is greater than n so  it will just copy the input to the first n bits and then just take it back to the output just as i  said in the case of you have onezero bits onesix bits and then again onezero bits it is very trivial to do  this  so such an identity encoding is useless because you are just not running any important  characteristics of the data your h is almost the same as x i it also has all the useless  information that x i had in fact it has slightly more because it has these blank units also  but this is not really useful right why would you want to actually learn such a hidden  representation right so it is not clear why would you want to do that so we will take a  look at it we will come back to why this is important  so such an auto encoder is known as an over complete auto encoder because it has the  hidden representation has more number of neurons as compared to the original input  now let us look at a case where this would actually be important right so this is a very  rough intuition for why you would want an over complete auto encoder  so let us consider the case where you have as input one of the features that you are looking  as bmi so suppose you are trying to find out whether the person is likely to get a  certain disease or not right so whether he would have a heart attack or whether he  would have a diabetes would have diabetes and so on and you are looking at various  parameters or various medical parameters of that person and one of them could be height  one of them could be weight and one of them could be bmi  now for whatever reason you have not computed the height and weight and you have  only looked at the bmi so now what has happened in your input and all of you know  that bmi is actually body mass index which is a function of the height and the weight  so now what has happened is that in your original input there was already this compact  the your feature space is already compact because you would actually look at you should  have actually looked at both the features height and weight but for some reason you  have only computed bmi and you could think of various some other correlated features  which are functions of many other features but you do not look at all those features and  just this final function of those features   so now in that case if suppose your prediction is that this person has or has a high  likelihood of being of high likelihood of having diabetes at some point in his life then  you would want to know whether it was the height or whether it was the weight which  was responsible for this  so in your original input your features are actually entangled and you would like to  disentangle them right so you would want to go from this smaller feature space to a  larger feature space where some of these entangled features get is disentangled so in  those cases we reach an over complete auto encoder however the problem still remains  that there is no reason why the machine should actually learn to disentangle these  features it could still just simply copy the bmi here and then copy it back here   so that is why when you are dealing with over complete auto encoders you will have to  do something special to prevent this kind of identity encoding so as you just take the  input and copy it to the hidden layer and then copy it back to the output so we will  look at what kind of special treatment you need to do to prevent these kind of identity  representations    here is the road ahead so first we will talk about the choice of f x i g x i right so we  did not say anything about what these functions f and g have to be so we will talk about  those and then we will talk about the loss function so i have just told you so far that we  will train this model in a way that x i is very close to x i hat right and i have argued that  if we are able to actually achieve that that x i hat is the same as x i in which case  presumably presumably the loss would be zero that means our hidden representation has  captured all the important characteristics of the original data  same as in the analogy of onezero bits to four bits to again onezero bits right if i am able to  reconstruct this without any error that means loss is zero then these four bits or the hidden  representation of my original x was actually able to capture everything that was  important in x so that it can reconstruct x again as x hat without losing any information  right so that is the loss function that we would want now what is the actual  mathematical formulation for this loss function that is what we will see next    so first let us start with the choice of f f and g so we will consider two case two cases one  case when your inputs are binary and the second case when your inputs are actually real  numbers right so the first we will look at the binary case so now just some notation  clarification so remember our original data was this matrix x which was m cross n  that means you had x one x two up to x n and each of these was r n   so now when i am referring to the entire row or entire data instance i will use bold x i  as i have circled here and i want to refer to one of the elements of this guy then i will  use this notation x i j same as what i have written here so what i am saying is that each  of these x i j‘s actually is a binary variable    now which of the following functions would be most appropriate for the decoder so  remember was the input was binary that means your output also has to be binary you  do not want to produce numbers arbitrarily large belonging to any or want do not want to  produce any real number you want to produce numbers which lie between zero to one so in  such a case what would be an appropriate loss function or sorry what would be an  appropriate function for the decoder so remember i am asking you what would f  be  so i am giving you three choices it should be tanh or just a linear decoder or a logistic  function which of these would be most appropriate logistic why would that be because  it will make sure that your outputs are between zero to one tanh would give you outputs  between minus one to one but you do not want that because your inputs were between zero to one  so when you are reconstructing obviously you want outputs between zero to one you do not  one minus one to one and linear of course can give you any real number which is not what  you want right   so if you produce any arbitrary real number like hundred and so on your loss is going to  be very high because your inputs were just zero to one and you are producing these arbitrary  real numbers which are very different from what your input was ok so in this case the  logistic function makes the most appropriate choice and g is typically that means the  encoded function is typically again chosen as a sigmoid function so it could either be  the logistic function or the tan h function right so the there is you could choose any of  these as the encoder function fine    now let us consider the other case where your inputs are real valued that means when  you reconstruct something you should again produce real values that means your  function f should take whatever is the input given to it and map it to some real numbers  right so that is what we want from this function f earlier in the binary case we wanted  it to map it to binary numbers right so that is the difference that we have now  so in this case which of the following would be appropriate the second one right  because tan h does not make sense because it will just produce minus one to one but you  want to produce any possible real number because some of these are actually higher than  one greater than one linear would be fine because it will produce any real number logistic is  again not fine because it will produce numbers between zero to one    so the logistic and tan h as i said would clamp the output to certain ranges so that is  not appropriate hence you should choose the linear function and again in this case also g  is typically chosen as the sigmoid function fine ok so the next thing that we look at is  the choice of the loss function    and again we will consider both the cases where a case the first case is the inputs are  real valued and the second case is when the inputs are binary  so let us look at the real case first now here the objective of the auto encoder is to  reconstruct x i hat to be as close to x i as possible now we have actually seen this  before so something similar before when we were talking about regression so now  you want to produce real valued outputs and they should match your real valued inputs  so what is an appropriate loss function that you can choose the squared error loss  function right   so what does this actually capture it says that for all my input data x one to x m for each  of these dimensions x one to up to x one n right i want to make sure that my original input i  will have a similar x hat reconstructed where i will have x one one hat x one two hat and x one n  hat  so i want to make sure that each of these pairs of variables are actually similar and i  can capture that by ensuring that the squared error loss between the ij’th entry in my  output is the same as this or sorry rather i could capture the squared error loss between  the ij’th entry in the output and the input ok that is what this function is trying to  capture straightforward similar things we have seen while we were doing regression  except that there we had y hat and y but here we are just trying to reconstruct the input  so there is no y here we just have the x ok and the parameters of the objective function  are of course all the variables or all the parameters that we have in a network which is  w w star c and b    and the matrix or the vector way of writing this is the following so we have x i so  what i am looking at here is i have gotten rid of this summation and i am just written it  in vector form so let me just explain what this means so this is what x i would look  like right so this would be x i one x i two up to x i n ok this is the vector and then you have  the x i hat vector which is going to be x i one hat x i two hat up to x i n hat  so taking the difference between these two vectors that is what this term is so what  you will get is essentially x i one hat minus x i one up to x i n hat minus x i n right and then  you are taking the dot product of this vector with itself which will essentially give you  this summation right so the dot product of this vector with itself is actually going to be  this summation it is going to be the sum of the squares of the elements of this vector and  that is exactly what we wanted    so this is a more compact vectorial way of writing the same thing and now we can just  train the auto encoder by treating it as a regular feed forward neural network this is just a  like any other feed forward neural network you have find the loss function  and you can just use back propagation to treatment right but and in this case all we will  need is a formula for the gradient of the loss function with respect to with your  parameters which are w and w star i have again ignore the biases and the bias is here b  and c so we will also need dou l theta by dou b and dou l theta by dou c right so these  two gradients also you will need but these are generally the easier ones to handle if you  know how to compute this b and c are very easy    so let us look at this now what we need for back propagation as i said we will need this  gradient right all these four gradients but let us focus on one of these now we have  already done back propagation and we have looked at arbitrary neural feed forward  neural networks here right we did not have we just said that there are l hidden layers  and in this case l is equal to one right or other we had said there was l minus one hidden  layers and the l’th layer was the output   so in this case l minus one is equal to one that means there is just one hidden layer so it does  not matter we had actually derived it for the general case when l is equal when the  number of hidden layers is l minus one and here we just have one eight n layer so it is much  more simpler than what we had learnt and even for the number of neurons in the each  of these layers we are just assumed general that it could be r n and in this case we  would have some r d which is less than n or it could even be greater than n right   so but it does not matter because whatever algorithm we had or whatever equations we  had derived for back propagation they did not care about what this n or d was we had  just derive it in general terms right and the same for the output layer  we did not assume any number of inputs any number of neurons in the output layer we  again said that it has some k neurons but there the catch is in the earlier case when we  had derived back propagation we were dealing with classification and we had these k  classes that we want to predict at the output  and in which case our loss function was actually the cross entropy or the negative log  likelihood function right where we were trying to maximize the probability of the  correct class out of the k given classes but here our loss function is slightly different it is  actually this squared error loss between the input and the output  so now given this difference in the loss function does it mean that everything that we  learn in the previous lecture on back propagation we just have to throw it all away  because now there is a new loss function that means my gradients are going to be very  different from what i had derived for the back propagation loss where i was looking at  the cross entropy loss as compared to the squared error loss so does it mean that i will  have to throw away all the hard work that we had done in that course in that lecture or  can we reuse something from them we can reuse  so let us look at what we can reuse and i will just give you an intuitive explanation for  that so you can think of this as a composite function right and you are taking your  input passing it through a lot of functions and then arriving at the output and then your  loss function is actually a function of the output itself  so what we have is something like this right we have a situation like this that you had  an input x you computed some function of it say x square right so i will call this as y one  then you computed some other function of it say y one say log of y one right so they this  was log of y one so in effect it is actually log of x square because y one is equal to x square  and then some other function and then finally you had the output so you had this other  function which was sign of i am calling this y two so say this was sign of y two and finally  you had this function which was e raise to y three  so you have a very complex composite function of your original input right and this is  your final output function that you are considering which is e raise to y three now the way  you would do this is if you want to take the gradient of d l with respect to your input d x  right in that case what would you do is you just apply the chain rule you will write it as  dou l by dou y three dou y three by dou y two dou y two by dou y one and then dou y one by dou x right  and this is something very similar that we are done in the back propagation lecture we  had constructed this chain and then we had attacked every element of this chain and  derived how to deal with that right derived an neat expression for that  now the question which i am asking you is that in that lecture we had assumed a certain  l and that l was actually cross entropy but in this lecture i have actually changed the l  what i am saying is the l is actually equal to the squared error loss now does that mean  that i have to throw away all this work that i had done no right  so even in this example if you look at it suppose i change this function from e raise to y  three to say square root of y three so i have just changed my l but notice all of these other  guys are going to remain in the same because y three is still sign of y two so that the  derivative of y three with respect to y two is not going to change even though i have changed  the output function the loss function everything else is going to be remain in the same  right   so that means all these portions i could just reuse from the time when i had computed  for this chain i just need to rework on this final expression and plug it in right so that  is why all the work that we had done in the case of back propagation will not go to waste  in particular everything that we had done    so let me just go to the next slide so in particular everything that we had done for this  portion of the network right which is actually dou a two all the way up to dou w right so  if ok so let me write it like this i want dou l by dou w so i can write it compactly as  dou l by dou a two and then dou a two by dou w right   so this portion is not going to change because i am not change any of the functions here  i have just assumed sigmoid or logistic or the same kind of network the only thing i  have changed is something at the output layer so i will just need to recomputed this and  the rest of it can be reused right so that is the intuition which i wanted to give you    and that is exactly what is written on this slide so i am written it as dou l theta by dou  w star that is the first gradient i have interested in and i could write it as dou l theta by  dou h two dou h two and dou a two by w star right   now this portion as i was trying to say is something that we have already seen in the  back propagation lecture and nothing has changed in the network in that part so you  can just reuse it as it is and this portion is something that we need to recomputed right  that is the only thing that we need to recomputed and plug it into our back propagation  code or the algorithm which we had in the previous lecture and similarly if you want to  do dou l theta by dou w it is the same idea here that you could write it as the following  chain and this part of the chain you already know how to compute from the back  propagation lecture  all you need to do is change the loss function and just try to find the derivative of the  loss function with respect to your output layer which is h two that is the final thing that  you have changed just as in my toy example i had changed e raise to y three to square root of  y three right that is the similar change that i am trying to do here fine  so all we need do is dou l theta by dou h two but dou h two is the same as x i hat right  because that is my output and my output i am calling it as x i hat so i need to take  actually the derivative of this so i am just using the vector form here i could have also  written it as this summation over i equal to one to n x i j minus x hat i j the two whole square  right i could have also written it as am i just writing it as the vector here in the vector  form here right but this quantity ultimately is going to be a scalar because it is a dot  product between two vectors which is the scalar  so what i am doing here is taking the derivative of a scalar with respect to this vector  so what is that derivative going to be it is going to be a vector    and i am just so we have similar stuff in the past so you can actually easily work this  out so this will actually turn out to be the following vector which is to times x i hat  minus x i right so this is very simple i have just computed this and all i need to do is  go back and change my back propagation code and change this derivative of the loss  function with respect to the output clear and the rest of the code i can just reuse it as it is  so now similarly so we have both of these ready    now let us look at the other case when we have binary inputs ok this is the most more  this is something different that we will have to do here so we will now look at the  second case where the inputs are binary so first we look at case when the inputs were  real numbers and hence your outputs also needed to be real numbers  now we look at the case where inputs are binary and hence your outputs also need to be  binary ok now here so each of these guys is actually a sigmoid functions so it is in or  rather if you look at the output you could divide into two parts so this is the pre  activation and this is the activation so your this is actually the pre activation and this is  the activation right   so this activation is actually chosen as the sigmoid function or the actually the logistic  function not the sigmoid function of course logistic is the sigmoid function but the  logistic function which was one over one plus e raise to minus z right so logistic of z is  equal to one over one plus e raise to minus z and remember that this sigmoid function was  element wise  that means this is a is a vector it has elements a one a two up to a n and then you know apply  the sigmoid to it you get h which is going to be sigmoid of a one sigmoid of a two and  sigmoid of a n right so it is just the sigmoid applied to every element of the activation  layer that means every element of this vector which have circled  so now in this case your outputs are going to be between zero to one right because your  inputs were also between zero to one and your sigmoid or the logistic function is going to give  you clamped outputs between zero to one so since this is between zero to one we could actually  interpret it as probabilities right so we could say that whatever you are reconstructing  is actually telling you that with zero8  suppose the reconstruction value is zero8 then you could think of it that with probability  zero8 it is telling you that the output should have been one right and if it tells you that the  output is zerotwo if the sigmoid gives an output as zerotwo then you could think of it that with  probably zerotwo the output was actually zero or rather the input was zero because an input is the  same as the output  so that is one way of interpreting it and this way of interpreting it why does it make  sense so we will just look at that right so before at if i do not give you this  interpretation and remember that the sigmoid is going to produce values between zero to one  but not necessarily zero and one right it will try to be as close to zero when the input is zero but it  could also produce zerozero5 and so on and when the input is point nine it could also  produce something like zero95  so at the output also you are going to get these vectors which are of which would look  something like this right and suppose you are input was zero one zero one now can think of a  suitable loss function for this yeah so again these are two vectors these are x hat and x  so once again you could have just gone with the with a squared error loss right you  could have taken the squared error difference between these two and you could have  been fine  so that is definitely one way of going about it but whenever we are looking at these  binary inputs and whenever this probabilistic interpretation is possible we tend to do  something better which is look at the cross entropy loss instead of looking at the squared  error loss so i am not saying that the square error loss is wrong in this case but you  could also use this cross entropy loss and in practice for our binary inputs the cross  entropy loss often works much better than squared error loss    so let us see what i mean by the cross entropy loss so remember that you have n  outputs right that is why this summation let us not worry too much about what is  written inside for the time being i will explain that but that is the i just want to explain  the summation first so what you are saying is that for each of these green guys at the  output you are going to make some loss and you just want to some over that loss that is  what we are trying to see  now ideally you could have just written it as just done what you had done before and  written this entire replace this entire box by this squared error loss and that would have  been just fine right of course there should have also have been this summation i equal to  one to m here because you are going over all the m training instances and for each of the m  training instances you are trying to minimize this loss so this two summations followed  by this squared error loss would just have been fine     but instead of that i have this something special here ok so let us look at what this  special quantity is ok and now for that remember that i am trying to interpret each of  these inputs as a binary random variable i am saying that they can take values zero or one  so i can think of it that when i am given that this value is zero i can write it as this  deterministic probability distribution where i have p and the probability mass is entirely  concentrated out on this zero value and my the probability mass on the value one is zero this is  something similar to what we had done earlier when we are given these labels suppose it  was apple orange mango and banana and the class label was given to us that this is an  apple then we could still write it as the probability distribution where all the mass was  concentrated on apple and everything else was here  so i am saying something similar here right so you could think of it that two possible  values can occur here one and zero and if i tell you this is zero right then i am telling you that  with probability one into it is zero and with probability zero it is zero so i still write it as a  probability distribution now the same thing i can have at the output so for this unit  when i am trying to reconstruct it and if i produce the output as zerotwo then i can or rather  let us say zero8 then i can say that with zero8 probability i am predicting zero and with zerotwo i am  predicting a one right   so now i can think of this again as two probability distributions and once i some have  two probability distributions i know that cross entropy is the right or a better loss  function to look at right and what is cross entropy actually in this case it would be  given by summation i equal to one to two right or rather i equal to zero to one because if the those  are the values it can take p of i right into log of q i plus yeah so p of i into log of q i that  is how i can write it  so let me just since there are only two terms i can just expand this summation right so  i can write it as p i or rather p zero log of q zero plus of course it is a minus sign here this is a  minus sign at the out p one log of q one i can just open up because there are only two terms  so i can write it as this is that fine ok  now also i know that there is this relation between p zero and p one right that p zero is actually  one minus p one yeah similarly you have this relation between q zero and q one that q one is equal  to one minus q zero because the sum is going to be oneok now let us look at this sum right so  in the binary case this sum becomes interesting because now suppose your input x i j  right which is the entity that i am looking at suppose that was equal to zero in which case  all the probability mass would be concentrated on p zero and p one would actually be equal to  zero which means the second term would display  on the other hand if x i j is equal to one then the reverse situation what happen that  everything would be concentrated on p one that means p one is equal to one and this guy  would become zero because p zero is going to be zero right ok so there is this another way of  writing it that you could day that instead of x instead of writing p zero and p one you could  just write it as x i j right into log q zero plus one minus x i j into log of q one  so now let us look at it again so when x i j is zero first which is the same which happened  here just an refer time threesixtwozero in same thing right because whenever s x i j is zero p zero is  equal to sorry it should have been q one and log q zero sorry i made a mistake here so it have  been x i j into log q so or rather let me just rewrite it  so this is going to be actually i can write it as i look at this term first so i can write it  as x i j into log q one and then the second term i am going to write it as one minus x i j into  log of q zero right and then i am going to simplify this further but let see what is the  consequence of this  so now whenever x i j is equal to one this term will remain and the second term will  disappear and that is exactly what was happening in our original formula right so this  is just an equivalent way of writing your x i j is equal to zero this term will disappear but  this term will remain that means log q zero will remain this is exactly what was happening  in our original formula right   so that is so now i have given you why a i can replace p zero and p one or rather p one and p zero  by x i j and one minus x i j and now i can make a similar argument for x hat i j also so i  can think of q zero as whatever s predicted at the output right sorry i can treat q one as  whatever is predicted out one output so whatever my sigmoid function predicts i can  think of it as it is predicting the probability of getting a one right so it is just predicting  the heads probability or the probability of getting one so i can instead q i q one i can write it  as x i j hat and similarly instead of q zero i can write as one minus x i hat i g right so did  you get that so these become very messy    so let me just clean this up and i will just go over this again right   so what i was trying to tell you is that in the ideal case you could have just replaced this  by the squared error loss but since you are dealing with binary inputs you can do  something better because you can interpret the outputs as probabilities so when you get  a zerotwo here you can interpret it as it is telling you that the probability of this unit being one is  zerotwo it is very less and that is the same as saying that the probability of this unit being zero  is one right   so you can interpret this as a probability now if you think of it that way then you can  say that at the input you are actually given a probability distribution so which tells you  that in the first case your probability distribution looks like one zero right because all the  mass is focused on value zero because your input is zero at that case and now suppose your  output was zerotwo right and zerotwo is what you are treating as a probability of so this is the  probability of one this is the probably of sorry this is the probability of zero oops and this is  the probability of one  so if your output is predicting zerotwo that means it is predicting zero8 for zero and zerotwo for one  now if you think of it this way then you can capture the loss function between these two  guys using the cross entropy formula which is going to be summation i equal to zero to one p  i log q i is that fine ok and now i just said that the since there are only two terms i can  just write it as p zero log q zero plus p one log q one  then i focused on this relation between p zero p one and your input so whenever your input  is zero ok your p zero is going to be one so then i can just replace p zero by one minus my input  right so if the input is zero then this guy is going to be one and that is exactly what this  expression is also going to be  so i can write it as one minus x j log q zero and similarly for this second guy whenever input  is one this guy is going to be one whenever my input is whenever my input is one this p one is  going to be one whenever my input is zero this p one is going to be zero so i can just replace p one  by log by x i j and now you can see that this expression evaluates to the same as this  expression right you can substitute value of x i j zero or one you will get the corresponding p  zero p one which would be one or zero depending on what your input was and these two  expressions will evaluate to the same thing  so just as i replaced the p’s by x i’s x i j’s i can similarly replace by a the q’s by x i j  hats right because once again q zero is nothing but one minus whatever my output was  predicted because whatever is predicted i am treating as the probability of getting a one  so one minus that is going to be the probability of getting a zero so that is what q zero is and  similarly q one i can replace by x hat i j and so that is exactly what i have done in this  expression here  so now this expression every term in these n terms captures the cross entropy for that  particular random variable right so this is the original distribution p for this random  variable this is the predicted distribution q for this random variable and i have just told  you that this the cross entropy between these two distribution can be written in this  simple form as the function of x i j and x hat i j  so this is the standard thing to do when you are dealing with bernoulli random  variables so you can go back and read up a bit about it ah but for now i guess with this  explanation it should suffice to know why this expression is used and remember that i  am not telling you that this squared error function was bad i am just telling you that  instead of the squared error function cross entropy loss function works better when you  are dealing with binary inputs    so with that let us pursuit and the another we have looking at it is the following you can  now look at this expression and tell me when is this expression going to be minimized  so we have x i j and x hat i j you can see that this expression will be minimized only  when x i j or rather x hat i j is equal to x i j right so now x i j could take value zero or one  ok and now x hat i j could take zero one or zero one  so you can see that for these two combinations the value is going to be minimized only  when x hat i j is actually equal to x i j that means if x hat if x i j was zero then x hat i j  should also be zero and similarly in this case also if x i j was one then the expression will  be minimized only when x hat i j is equal to one so let us see this so suppose x i j was zero  that means this term is going to go to zero but this term is going to remain and now if you  are x hat i j was not equal to zero  then you will get some log of one minus x hat i j as the loss right but if x hat i j was also zero  then you would get log of one which is zero so this whole expression would then evaluate to  zero which is the minimum possible value for this expression right   so that means if x i j is zero then this expression will be minimized only when x hat i j is  also equal to zero similarly if x hat i j sorry if x i j is one then this one minus one will give you zero  so this term is going to disappear but this term will remain so this will just be log of x  hat i j because x i j is equal to one  now if x hat i j is also equal to one then this is become log of one which is zero that means  again this expression will attain it is minimum value when x hat i j is equal to x i j is  equal to one right so this expression now attain it is minimum value in two cases when x  i j is equal to x hat i j is equal to zero or when x i j is equal to x hat i j is equal to one so  compactly i can say that this expression will attain it is minimum when x i j is equal to x  hat i j that is why this loss function makes sense    now again we have this problem that we want to use back propagation to train this  network and once again for back propagation we will need the following gradients the  gradients of the loss function with respect to w and w star ok this is what we are going  to need and i am going to make this same argument again that whatever hard work you  had done in the back propagation lecture you can just reuse all of it     because the only thing your changing is this final loss function so you just need to  compute the gradients with respect to this loss function and everything else is going to  remain the same right   so that is exactly what i am going to do on this slide so whatever is in the boxes here  these two boxes that is something that you have already computed and now what i am  going to compute is the stuff which is outside the boxes so let us look at that so i am  interested in computing this dou l theta by dou h two this is the derivative of a scalar  quantity with respect to a vector say it is going to be a vector and i am going to follow  our usual recipe which is h two is actually equal to h two one h two two up to h two n  so i am going to consider any of these guys which is h two j i am going to compute the  derivative of the loss function with respect to this one entry and since i have that i am  going to construct the entire gradient right so now i will have this dou l theta by dou h  two j right and once i have that expression i am just going to generalize it to all the other  entries in this vector  so let us look at that expression first ok so now if you look at this actually it does not  have an h two j right but we know h two j is the same as x hat j or rather x hat i j right for the  ith input it is going to be x hat i j because h two is equal to x hat i ok you can just see that  the top left corner of the slides say x two is equal to x hat i so this is nothing dou l theta  by dou x hat i j  so now i want to take the derivative of this quantity with respect to one particular x i j and  remember that this quantity has the sum which is indexed over j so j goes from one to n i  am looking at one particular j so that means if i expand this sum of all the js possible the  derivative with respect to all but one is going to be zero because they do not depend on this  particular j so if i am looking at j equal to three then the term which has x hat i one is going  to the derivative of that term is going to be zero  so for all these terms in the expression only that term where a j is equal to the j which i  am considering is going to remain ok so that means only one term in the summation  would remain and for that one term so let me just rid of the summation right so that  means only one term in the summation would remain i am trying to find the derivative  of this quality x which has a lot x i j’s with respect to x i j  so now this is of the form a log x so the derivative would a over x right so that is  exactly what i have written here and similarly for the second guy this is one minus a into  log of one minus x so the derivative is going to be one minus a over one minus x and of  course there is this minus sign here which will then get adjusted appropriately right so  that is how this expression has been completely that is very straight forward and now as  you need the derivative of h two j with respect to a two j  so remember that h two is equal to sigmoid of a two which means it is just an element wise  sigmoid right so i just need to compute the derivative of the j’th entry of h two with  respect to the j’th entry of a two all the other derivatives are going to be zero because they do  not depend on that particular entry of a two so now that is just going to be sigmoid of a two  into one minus sigmoid of a two right   so i have computed these two quantities i can just plug it then back into back  propagation code the rest of the code is going to remain the same and i have the  gradients ready with me     and as i said once i have this one guy i can just extend it i can just generalize it so i  just had these j’s here right for h two j so i can just replace the j by one two up to n and i will  get the same expression  so that is the end of module one where we introduced auto encoders what we showed is  that they are actually just like any other feed forward neural network accept that they  have this special objective that they want to reconstruct the input and the reason they  want to reconstruct the input is they about to first create a bottle neck which is this h  hidden representation and then try to reconstruct from there and just as i gave you that  compression analogy that you have this onezero bits you want to compress it to four bits and then  reconstruct the entire input again  so this will happen only if these four bits capture everything that is required or the most  important characteristics of your original input right and then we could have a loss  function which tries to capture the difference between my original input and my  reconstructed input  now we argued that this loss function will be dependent on the nature of your input so  for the real inputs it was straight forward we just said that we can use the squared error  loss function for the binary inputs we actually did something special we said that we  can actually use the cross entropy and then we had this funny way of writing the cross  entropy which was this x i into log of x hat and one minus x i into log of one minus x hat  and just gave you some intuition that that is the same as writing p log a pi log or rather p  zero log q zero plus p one log q one write and the i just gave you some explanation for doing that  you can go back and check on how do you write the cross entropy for bernoulli random  variables and you will see that this expression makes sense and once we had this  expression computing the gradients was easy so the other thing that we relied on is that  in the back propagation lecture we had taken care of everything up to this point and in  this lecture we have actually changed the loss function  so one loss function was the sum of squared squared loss errors and the other loss  function was the sum of sum of cross entropies whereas in the back propagation lecture  we had only dealt with cross entropy by the case that we made is that sense you have this  chain all you have done is change the last function in the chain right you have changed  this l function all the other functions you have not changed  you can just reuse the computations from these or you can just use the code that you had  written for these in the back propagation assignment and you just need to change this  last guy to adjust for the change in the output layer or the change in the loss layer so  with that we will end the introduction to auto encoders there we have done we have  actually covered how to train an auto encoder using back propagation  
task2/super_cleaned_audios/lesson86.wav,1066.9481,so why let us start so far in the course we have looked at feed forward neural  networks we have seen how to train them and we have seen two special cases of feed  forward neural networks one was the auto encoders for learning representations or  learning latent representations of inputs and the other thing that we had seen was how to  use a feed forward neural network to learn word representations where we saw this word  to wake algorithm and it is different variants it was continuous bag of words skip gram  model graph and so on so those are all since some since applications of the feed  forward neural network  and now we will move on from there though we will look at different type of neural  network today which is convolutional neural networks and we look at some specific  architectures which have become popular over the past few years ok  so with that i will start this lecture on convolutional neural networks so in the first  module we will look at the convolution operation ok    so let us see so suppose we are tracking the position of an aeroplane using a laser  sensor at discrete time intervals right so you have this ok so you have this aeroplane  suppose it is going from say chennai to delhi and at discrete time intervals you are  seeing the tracking the position of the aeroplane right how far it is from chennai at this  point right may be it is fifty kilometers one hundred kilometers and so on   and now your laser you think that it might be noisy it might not be giving you very  accurate measurements so you would be taking these measurements and say intervals  of course it is not in practice you would not do that but just indulge me for the purpose  of illustration that say you are taking these measurements every five seconds or ten seconds  or something like that   now since your sensor is noisy instead of relying on a single measurement you would  probably want to take the average of the past few measurements that you have taken so  that would give you a more accurate representation of what the current position is does  that make sense like you are taking multiple measurements and taking averages of those  right and of course more recent measurements are more important as compared to the  previous measurements right so this is suppose at time step t say this was t minus five  seconds and this was t minus five minutes suppose   so obviously you would not want to take give a very large weightage to the  measurement that you are taken t  five minutes back right because the plane would have  moved by a lot by that time so it rely more on the recent measurements and less on the  previous measurements right so now the mathematical way of lighting this is that you  know the positions or the readings that you have taken at time steps one two three up to time  step t you are interested in the revise estimation of this measurement right so you have  taken some measurement at time step t and you want a revised measurement of that and  the way you are going to compute that is you are going to take a weighted average so  w is the weight of all the previous measurements that you have taken right    so the measurement that you take a t  one t  two t  three all the way up to t minus infinity and  for each of them would have a weight associated with this  so this operation right this thing you can write it as the following operation that you  have a vector of measurements or an array of measurements which is x and you have an  array of weights associated with these measurements the farther the measurement from  the current time step hopefully smaller is the weight assigned to that and this operation is  known as the convolution operation right    so you have x which is the input w is known as the filter and the operation that is  defined as this equation is known as a convolution operation right    look but of course in practice you would not do this from infinity right you would  probably keep a window you will say i will rely on the previous six measurements that  means whatever i took at t  one second t  two second up to t  six seconds right beyond that  it does not really make sense so let us see how this computation happens so this is  weight array so now what would be the dimension of this weight array how many  entries would it have  student seven  seven right zero to six so seven entries ok and this is what my situation looks like right so this is  the x the measurements which i have taken using the laser ok so i have taken some  measurements now i am at a particular time step and i want to make a revised estimate  so i have this xt and from that i want to compute st and the way i am going to do that is  by taking a weighted average of all these previous measurements is the setup clear to  everyone ok   and now this is what my formula is going to be so the revised estimate of ssix is going to  be whatever was xsix into wzero that means the weight assigned to the current time step xfive  into w minus one that means weight assigned to the time step  one xfour into  two and so on  so you get this ok so i have these seven weights and i will multiply with them with the seven  previous readings one is to one multiplication and i will get the weighted average and using  that i get a revised estimate    now i want to get a revised estimate for the next entry how will i get it i will just slide  this weight matrix right  so i will just slide it by one i will again do the same computation and get the revised  measurements again for the next entry i will slide it by one slide it by one slide it by one and  i will keep getting these entries ok so everyone gets the setup how do you do the  convolution operation it is basically a weighted average of the previous entries fine  so here the input as well as the kernel is kind of one dimensional right you so you have it  is so you do not have a twod input here you just have a single dimensional input here    can you use a convolution operation on a twod input also do you know of any twod inputs  images right so we can think of images as twod inputs now again i am trying to do the  same thing the setup is the same the story just changes from laser to a camera now so  i have taken an image maybe the image was captured and i am not very confident about  all the pixels that i have captured ok  so now for any given pixel i want to reestimate it using it is neighborhood that is what  i want to do ok so this is the pixel i am going to look at some neighborhood around it  right so every cell here is one pixel just assume that every cell here is one pixel so now i  am going to reestimate this pixel by taking a weighted average of all its neighborhoods  right so now can you tell me what is my filter going to look like in this particular case  my filter would be just threethree right so whatever neighbors i want to average on for  every neighbor i want a weight so if i am going to average on a neighborhood of three three  then for each of these i will want a weight so my filter would also be of size three three how  many of you get this ok  so we now like to use a twod filter which would be m cross n ok and in general it  would be m m so it would always be a square filter but i am just taking the case  now what this nasty looking formula is doing right so i have a particular pixel so  this is an image so i will refer to this pixel as i ij right so it is the ith ijth entry in the  image i want a revised estimate for that i want an sij for that   so the way i am going to do that is i am going to look at m rows and n columns before it  right so i am going to look at this neighborhood of m cross n ok and for each of  these i would have a weight associated with it so if i am looking at say for example  this was four  four this pixel was four four then i will look at four – one four  one so that would be i three three  so i will look at that neighbor and with that neighbor i would have some weight  associated do you get that how this formula expands  so this formula would have m cross n terms for every term you would have a have a  weight and that weight you can just represented as this filter matrix so you get this  what this formula is doing it looks a bit nasty but it is just the weighted average of all  the neighborhood that you have and the neighborhood is a two dimensional neighborhood  in this case how many if you get this properly ok   now this in this formula actually i am looking at minus a and minus b that means i am  looking at previous neighbors right now you should have these questions right why  previous neighbors why not future neighbors so why am i not looking at this  neighborhood    so there is no correct answer here different convolution operations i mean different  packages use different convolution operations but the most standard one i believe is  when you look at the next neighborhood right that means you at this pixel and you will  look at this neighborhood the neighborhood after it right not the before it ok  and in fact so this is the formula that i am going to look at plus j and plus p that  means i am looking at pixels in the rows after this and in the columns after this pixel all  of you get this instead of before now what is even more natural to do the names  surrounding thing right so i will have this pixel and i will look at it is such a way that  this pixel is the center of the neighborhood right so that is what i am going to go  towards after a couple of slides and that is what i will use for all my convolution  operations but in terms of textbook definitions these are the definitions that you will  find in textbooks ok     so let us let us apply this to a toy example so i have this input which is two dimensional  input i have a kernel which is a twotwo kernel so my m is equal to n is equal to two so i  am going to place this kernel at this location ok and then what will i get as the output    a into w plus b into x plus e into y plus f into z right and i will keep sliding this to get the  other entries do you observe something about the input and the output  student refer time one hundredseven  size the output size has reduced why we will get back to this    so right now i just you to notice it is obvious nothing great about it but i will just get  back to it more formally later on  so for the rest of the discussion we will use the following formula for convolution  which is the centered formula right so  two m  to two n  that means i will be looking at a  neighborhood which is centered on the pixel of interest that is why this   two m to  two m  is  that fine ok     so this is how i am going to look at it so this is how i will place if this is the pixel of  interest which i want to re estimate i will replace the kernel such that it this pixel lies at  the center of the kernel ok    so we will be looking at both preceding and succeeding neighbors ok    so let us see some examples of twod convolutions applied to images    so this is an image i decide to apply the following convolution operation to edge ok tell  me what the resulting image would be  student blurred  blurred why blurred  student we are taking average  we are taking the average right so it would be blurred you get the intuition so this  kernel basically i have fitted at every pixel and i have computed the average around it  and place at pixel by that average value and when we are going to take average things are  going to get blurred right because all the sharpness is gone ok     now let us look at this kernel what will this do sharpen why because one was blurred  the other has to be sharpened what is happening here  student refer time oneonetwo9  it is subtracting the neighbor’s right so you are taking five times the current pixel and  subtracting the neighbors from it right so if the neighbors are similar those would get  subtracted and this would stand out really right does that make sense   this will result in a but this in my on my laptop this looks like a sharpened image i do  not know why it is looking like this here ok it is a sharpened image just trust me you  can so actually are common right so people who have used adobe or any of these photo  shopping software’s right so you have this click button and where you say take an  image sharpen and blur it so this is exactly what the tool is doing in the background it  is applying this convolution operation throughout the image  so when you say blur it is basically placing that convolution operation throughout the  image and computing the blurred image and same for sharpening and all these other  spatial effects that you have most of them come out of some convolution operation ok    so for example the next one what would this do   student refer time onetwotwosix  so i will give you a hint when will this result in a zero output  student refer time onetwothreetwo  when all the neighbors are the same as this right so then when will it result in a nonzero  output  student refer time onetwothreeseven  when there is a difference when there is a difference so looking at this image tell me  one place where you know that it will result in nonzero output  student refer time onetwofourfive  all the boundaries right so this is basically an edge detector in the slides it appears  properly ok so this is basically an edge detector and you get the intuition that these  boundaries whether neighbors are not the same as the current pixel you will not get a zero  value in this case when all the neighbors are the same as the current pixel so you are  taking the sum of the eight neighbors and subtracting the current value eight times so that  would be zero right ok    so enough of examples so now we will see a working example of a twod convolution so  i just want to drill this idea of what happens when you do a twod convolution    so what we are going to do is we have this threethree kernel and assume that everything here  is a pixel ok everything here is a pixel so i am going to slide this three cross three kernel  across this filter now when i place the filter once on the image how many outputs do i  get  student one  one output so if i keep sliding it across the image i will keep getting one one pixel in the  output ok so what the resulting thing that i get is known as a feature map ok because it  is the original input that we have taken for every pixel you have tried to approximate it  or whatever filter weights you have applied and it necessarily does not mean that you  are taking an average it could be some weird average of your neighborhood right so  you have extracted some features from there   so for example in the edge detector case you could think of it that you have extracted  the feature that this pixel does not lie at a boundary right that is why you get the black  pixel do you get that you see this way of interpreting a convolution operation that you  are trying to extract some features from that neighborhood  so in this earlier example whenever you got a black you are basically extracting the  feature that this pixel does not lay at a boundary is that ok fine so now you could get  one such feature map by using a single three cross three convolution operation ok if i use  multiple such convolution operations what would happen i will get multiple feature  maps ok so let us try to understand this what is the dimension of my original image m  cross n into three why is it into three  student rgb channels  rgb channels ok rgb is what we will have right so we will have this threemn so  we will return back to this idea and from now this one image by using a single kernel so  this in fact in for this figure right i am assuming that the input is one cross m cross and i  am not assuming there are three channels although it is a colored image but just bear with  me so it is a one cross m cross an image and when i apply a filter i get a one feature map if  i apply k such filters i will get k feature maps so one feature map could be for the  blurring one one could be the sharpening one one could be the edge detector and so on  right there are various such filters that you could apply    now in the oned case we slide a one dimensional filter over a one dimensional input in the  twod case we slide a two dimensional filter on a two dimensional input what would happen in  the threed case    so now we are going to this rgb images right so we will have three cross m cross n as  the input what would happen in the threeg threed case not threeg    so what would a threed filter look like   student box  look like a cuboid a box basically and we will call it a volume why volume because it  has a width it has a height and it will have a depth so this is what a three d filter would  look like i will assume that it is depth is the same as the depth of your input what is the  depth of your input in this case  student three  three so i will assume that the depth of the filter is the same as a depth of the input and the  width and height could be three three five five seven seven anything right so we will get into that in  more details later on so once again we slide this volume across the entire image ok  what is the output going to be twod or threed  student twod  why so when i was oned i was getting oned output when i was twod i was getting twod  output threed again twod output why because i have assumed that no not width   student refer time onesixfour9  the depth of the filter is the same as the depth of the input so now you just imagine this  if you can suppose the filter was of depth two instead of three then i would slide it  horizontally first vertically and then across the depth also so then what would be  output be in that case  student three dimensional  three dimensional and it would have depth of two  student two  everyone gets that right but for this lecture i am always going to assume that the depth  of the filter is equal to the depth of the input always right and that is how it is for all the  convolution neural networks that we will see the depth of the input is going to be equal  to the depth of the filter the rather the depth of the filter is going to be equal to the depth  of the input so whenever i apply a threed filter i am actually doing a twod convolution  because i am moving only along the width and the height i am not moving along the  depth so the output is going to be twod so now can i have multiple such filters yes  each filter will give me a twod output if i have k such filters i will have a  student refer time onesevenfivefour  k twod output right k twod outputs fine   
task2/super_cleaned_audios/lesson92.wav,1328.3883,googlenet and resnet  so we will go to the next module where you wanted to look at two more architectures  for image classification these are googlenet and resnet   so here is a question right so consider the output at a certain layer of a convolutional  neural network so you have this layer after layer of convolutions and max pooling and  so on and you are at somewhere in the middle and this is what your volume looks like  this is what your output volume looks like now after that you could apply a max  pooling layer you could apply a one  one convolution you could apply a three  three convolution  or you could apply a five  five convolution right   and so far we saw that all these architectures they do one of these things they either do  a max pooling or they do a three  three convolution or they do a five  five convolution or a seven  seven  oneone  oneone right any convolution but they are all uniform they are all either three  three all  either five five or either seven  seven right so why choose between these options why not do all  of this at every layer do you get the question that i am trying to ask right  so far what we were doing is that you have this volume this volume at a certain layer of  the convolutional neural network and after that you are either using all three cross three filters  so you are using twofivesix three  three filters or five into three  three filters or using seven  seven or using five  five  you are never using a mix of all these right so why not use a mix of all these why the  why take a decision on that i only want three  three because it is possible that you want to  capture interactions at different levels so you should have varied size filters at every  layer   so how many of you get the question and the intuition that i am trying to ask ok  so  the idea here in googlenet or in the inception net is that why not apply all of them at the  same time and then concatenate the feature maps right so i will also do max pooling i  will also do three  three feature maps i will also create five  five oneone  oneone and then just  concatenate all of these together so let us see how to do that right     now one problem with this naive idea is that it could result in a large number of  computations so let us see what i mean by that so suppose the padding is zero the stride  is one then if you have a w  h  d input as the volume and if you have an f  f  d  filter then the output would be of this size we all agree with this this is the formula that  we have been looking at so this is the size of the output volume now every entry in  this output volume requires how many computations to get a single entry in this output  volume how many computations do i have to do    student f  f  d  f  f  d so how do i get a single value i apply a convolution at that value and then i  do those many computations and here the number of computations is that i am going  over this block of f  f  d doing a weighted multiplication and then adding them up  right so you need that many computations everyone is clear with this ok  fine   so each element of the output requires these many computations and we have so many  elements in the output right so you are doing really those many number of  computations right so can we do something to reduce the number of computations right  so that is the key idea that we need to focus on so all of us buy the idea that doing this  multi granular or multi sized filters is a good idea because you are capturing interactions  are different layer but i showed you that this is a problem with this you guys just apply  multiple filters so let us see what we do    so we what we do is one  one convolutions what is the one  one convolution do what does  it make sense  student refer time zerofourzerotwo  how does the one  one convolution make sense you have a pixel i fit a one  one convolution  on that what will i get i will get back the pixel   student refer time zerofouronetwo  along the depth right so remember it is not one  one it is one  one  depth  student depth  right so what does a one  one convolution do it actually aggregates along the depth so  this is what my one  one convolution looks like it is one  one  d so i just fit that block on  that pixel and do everything along the depth and get a single value right so from a threed  output using a one  one convolution i can go to a two  twod feature map everyone gets this  ok   now i could use several of these one cross one operations one cross one convolutions in fact i  could use done of these such that done is less than d so what effectively happens the  depth of the output reduces so i take a certain output volume whose original depth was  d now i take done one  one convolutions right so i get an output whose depth is smaller  than the depth of the original output is that fine everyone gets this ok   and you see how this will save computations right because remember that this was f   f  d and now i have reduced d to done ok so it is going to reduce the number of  computations so that is what the idea is you reduce it from f  f  d to f cross of  cross done right so thats this particular network or this paper introduced the idea of this one  cross one actually it did not introduce it used it but it made it popular probably     now once you have done this so this is how i am going to proceed now i have a certain  volume i have applied one  one convolutions to it using that i have reduced the number of  dimensions now i am going to apply three  three convolutions as well as apply five  five  convolutions on top of that right because that is the motivation that i had started with  that i want to apply kernels of multiple granularity   now can you think of some refinement to this you see this branching over here why  use the same one  one convolutions before feeding to three  three as well as five  five i could use a  different set of one  one convolutions and feed it to a five  five and use a different set of one cross  one convolutions and feed it to three  three is that fine what is the problem with this  student again increasing the number of computations  again increasing the number of computations right but they found out that the tradeoff  between this is fine even if you are doing more one cross one operations it still is ok the  number of computations are still manageable ok and then you could also do a max  pooling because we were choosing between these things right five  five three  three seven seven and  max pooling so we will do all of these in parallel and we also do some one  one  convolutions so how many different types of operations we have done we have done  one  one three  three five  five and max pooling followed by one  one convolutions   now all of these outputs that we have got we have got a bunch of feature maps now  this is one set of feature maps this is another feature maps this is another and this is  another all of these four we are going to concat together to get a single output volume do  you see what is happening right it is not very mechanical there is nothing really  profound about what is being done the only two profound ideas are one is apply  multiple kernels of different sizes and the other is to use one  one convolutions to make the  whole computation manageable that these are the only to main ideas the rest of it is not  very different from what we have been doing how many if you get this operation  completely  so this block is called the inception module ok this entire thing is called an inception  module so in subsequent slides when i put an inception module then you know that  these parallel operations are happening right so far whenever we had seen a  convolutional neural network it was all serial right so you started with one operation  then another operation then another operation and so on now you have an output or an  input volume you apply multiple operations in parallel and get one single output right  so it is a parallel serial combination ok  so you will now see the full googlenet architecture so his question was basically three  cross three would result in a different sized feature map right because of an five cross five would  result in a different sized feature map so i will use appropriate padding so that all of  this becomes equal ok     so this is how googlenet looks like so you have the input again rgb and same twotwoseven   twotwoseven or twotwonine  twotwonine then you apply a convolution layer followed by a max pooling layer  convolution max pooling then you have this inception module with a very specific  configuration so they have ninety-six one  one convolutions before feeding to onetwoeight three  three  convolutions onesix one cross one convolutions before feeding to threetwo  fivefive convolutions and so  on and i do not really see much point in going into the details of these numbers there  is hardly any intuition behind them    i again guess that it’s you try a bunch of things and this is the one which probably gave  the best output so the key idea is that of course you have this inception module which  is a parallel module which does a lot of operations in parallel this is again followed by  another inception module which has a different configuration followed by max pooling  then again a few inception modules in fact five of them again max pooling then inception  and this is the other interesting idea that they came up with   so at this point remember in vgg net at the final layer you had an output of size fiveonetwo   seven  seven right and we said that this was a problem how many of you remember this why  was this a problem  student refer time zeroninefivefour  because i need to connect this to a  student fully connected  fully connected layer right and that fully connected layer was of size fourzeroninety-six right so what  they said is that what you could do is instead of taking fiveonetwo cross seven cross seven for each of  these fiveonetwo feature maps that you have take the seven cross seven and just do an average pooling  from there what does what do i mean by that  student average  take these seven cross seven values take an average of that so now instead of fiveonetwo cross seven cross  seven how many values will you end up with  student fiveonetwo  just fiveonetwo and in their case instead of onezerotwofour cross seven cross seven you will just end up with onezerotwofour  values right so instead of looking at these dense connections with every pixel in the  output volume you just take the average of those pixels and then do a dense connection  from there so from this volume you just go to a vector of size onezerotwofour which is exactly  this vector shown here and from there on life becomes easier right because you have  done a fivezero percent sorry fivezero times reduction in the volume so this was onezerotwofour cross fournine  now we just have onezerotwofour cross one so you have a fournine times reduction in the size and that is a  huge parameter reduction   and that actually worked very well in practice they of course add these dropouts and  other things and then you have your fully connected layers and finally the soft max  layer at the output to predict one of the thousand classes right so this is the full  structure of googlenet or inception net or with multiple inception modules right so  just remember that key takeaways are three one is half filters at multiple granularity  applied in parallel the other is use one cross one convolutions to reduce the number of  computations and the final one is to use this average pooling to make sure that you do  not have this blow up of parameters at the output ok so these are the three main ideas  that you need to do right ok    so this is exactly what i explained so instead of having this nasty looking connection  which would have been fivezeroonesevensix cross onezerozerozero you just take the average from this grid and  just get a onezerotwofour dimensional vector which results in a much smaller weight matrix at the  output everyone gets this so ok yeah so this is fine    so this has onetwo times less parameters than alexnet it has two times more computations  right so that is what i meant by the tradeoff so the number of parameters has reduced  significantly of course a large amount of this savings happen in the fully connected  layer its not the ingenuity in the inception module which led to the fewer number of  parameters that actually leads to more number of parameters right   but the reason they could afford more number of parameters in the convolution layers is  because they reduced a lot of parameters in the fully connected layer do you get that  so they did this tradeoff and it has two times more computations then alexnet but it is  still acceptable because you see that there are many many layers as compared to alexnet  right so let us actually count the number of layers that we had here so one two three four five six seven eight nine  onezero oneone onetwo onethree onefour right so it has onefour layers and each of these inception modules is again  like split layer right it has this parallel components there   so having two times more computations was still an acceptable tradeoff and it of course  led to much better accuracy as compared to alexnet or zf net or vgg net right that we  had seen in the original trend graph ok so now we will go on to the last architecture that  we will discuss for image classification which is resnet    so here is the idea behind resnet or here is the motivation right now suppose we have  been able to train a shallow neural network well now again my definitions of shallow  are relative this is by no means shallow there are many layers here right so you have  some eight layers here   now if i have been able to do this properly that means what i mean by that is that using  this network at least i was able to reduce the training error to zero or close to zero some  acceptable value and i was able to get some reasonable generalization performance  that means on the test that i was able to get some reasonable accuracy that is what i  mean by i was able to make this network work well   now suppose i add a few layers to this network and i have carefully added some layers  in between here and in between here or over there right now intuitively i could argue  that if the shallow network was working well right then for the deep network this is exist  at least one solution which can directly come from the shallow network can you tell me  what that solution is  student refer time onefourtwonine  i want all of you to kind of digest that idea what the deep network could have done is i  know that this shallow network works why not i just behave like that and i learn these  parameters in such a way that i just end up copying from here to here how many if you  get this right so there is a case for the deep network to do at least as well as the  shallow network and it could do the same thing at this point all of you get this idea  right  so in other words the solution space of the deep network or rather the solution space of  the shallow network is actually a subset of the solution space of the deep network there  was one solution for the shallow network which could have been used as it is for the  deep network of course for the deep network there are several other solutions because  instead of the identity here you could learn different things there but at least that one  solution exists so i should at least if i do use this in practice ex i should expect that this  would work as well as the shallow network right is that argument fine with everyone   student refer time onefivetwonine  or which has only one yeah yeah of course  student yes  i mean so it those arbitrary things would not work but here what there it is the for the  explanation intuition right you are using some reasonable things and you are just trying  to make it compatible with whatever you have so far so the argument is valid right so i  cannot expect that i had a volume whose depth was onetwoeight and then i suddenly decide to  use only one filter in the next layer right   that means i have compressed everything and now i expect it to be able to recover from  there that is not going to happen right so that is a fair argument but the argument  which i was trying to make or at least for the illustration purpose is that if you do  reasonable things and that is what people were trying out right these this is the exact  network that someone was trying out and this did not work with well i will tell you what  it is so do you get his doubt and my clarification on that is that fine ok     so this is what was happening in practice right so you have a twozero layer network or threetwo  layer network or fourfour layer network and a fivesix layer network and you see that the training  error of deeper networks is much higher than the training error of the shallow networks  that means this argument which i was trying to make that the deep network should at  least do as well as the shallow network was not working well in practice right and it is  if you think about it is not very surprising because this argument hinged on the fact that  it should be able to learn this identity mapping but this identity mapping is one of many  solutions right   so for it to be able to narrow down on that solution it is easy for you and me to think  about it but for the network it does not have this intuition right that i can just copy it  from there to here do you get that the solution space is really really large and like  finding that needle in a haystack right you have these many solutions possible and i  am trying you to arrive at a solution where you end up with the identity solution is that  clear and it is not so easy for the network to do that everyone gets this how many of  you get this idea     so why not explicitly try to do something of this sort where the network can actually  learn some kind of an identity function so now consider any two layers you know by  stack layers i mean this is a convolution layer and followed by a convolution layer right  so these are two convolution layers back to back so from i what do i actually end up  doing here i had a certain input x and i am learning some transformation of x through  these convolution layers right i am trying to learn x and then i sorry i was given x i am  passing it to convolution layer so i will run some transformation of x   and my argument was that if it could learn to directly copy x here the deep network  should at least work as well as the shallow network so why not i explicitly ensure this  so why not i do this that in addition to these connections i also explicitly connect x to  the output do you get this so now what i am trying to do this is hx is equal to f of x  which is the transformation that i learned for x and in addition i also add x so what am  i doing i am explicitly feeding it the identity function right how many if you get the  intuition for this so what i am trying to do is i have a sense that if i could have  transferred this x as it is across layers then there is a chance that i should be able to do as  well as the shallow network right   so now i am going to explicitly ensure that that you learn these transformations but i  will also feed you x at every stage at a reasonable time right so these are known as skip  connections so after every two layers i will feed back the x or you could try after every three  layers i will feed back the x so i am trying to maintain the original copy of x after every  interval ok fine so why would this help so this follows back from our argument and  it is the same thing which i said before    so using this idea of using these skip connections these authors were able to train a  really deep network of onefivetwo layers right and this gave on multiple vision challenges  right one being imagenet it gave onesix percent better results and the best network and  this is the one which reads that near human performance then imagenet localization is  another challenge where you need to localize the object so there they get twoseven percent  better than the best results and there are these bunch of other vision tasks detection  segmentation and all of them and in all of them this significantly outperformed the best  system using a very very deep network of course the downside is that you have a very  very deep network it will take its own time to train and so on but of course if you have  a microsoft or google you can afford to do that     so that is the current theme right i mean the one with the largest computational  resources wins everything right so and they also are there is some other bag of tricks  which is not i mean it is not very difficult to understand so they used batch  normalization every after every conversation layer have you heard of batch  normalization ever in your life ok good they used xavier by two initialization ever  heard of that xavier by two was the same as  student he initialization  he initialization right then they use sgd not any of the fancy adam or adagrad or  anything with a momentum of zeronine learning rate was set to zeroone and divided by onezero  whenever the validation error plateaus the mini batch size was twofivesix they use a weight  decay of one ht what is weight decay weight decay is in the context of which  regularization  student ltwo  ltwo and what does this mean weight decay of one e raised to minus five  student lambda refer time twoonezerofive  the lambda was set to one e raised to minus five all of you remember these things right we  did it in some previous course in some previous life and no drop out was used right so  since i have this here i will just say something more on this so in your reading papers  on deep learning right focus on the experimental section where all these hyper  parameters are described so these are known as the hyper parameters these are not  related to the parameters of the model these are related to hyper parameters which is  what the batch size is whether you used l two regularization what was the learning rate  what was the optimization and so on  so turns out in many cases if you do not stick to this you will not be able to reproduce  the results of the paper right so you might be wondering that this network i understand  this is just onefivetwo layers and i can just keep adding skip connections i can easily code this  up but i am not getting the same results as the original authors of the paper   so this is where you need to dig up them right you need to look at the experimental  section where most good authors provide these details of how they have trained the  network how many epoch that they use what was the patients set and all that if you  follow those the chances of reproducing are much higher still not guaranteed but  definitely much higher ok so that is where we end the lecture on convolutional neural  networks and imagenet classification        
task2/super_cleaned_audios/lesson96.wav,330.461,so now so far what we have done is we have seen the influence of neurons on the or  what image patches cause a neuron to fire then we have visualized the weights so  neurons have been visualized weights have been visualized then we have done some  occlusion experiments on the input image now we will take this further and we are  interested in seeing that what pixels in the image actually help in the output or in any  neuron in the intermediate layers and we will find out some principal way of finding this  influence right and we are going to use back propagation that means we are going to  use gradients ok  so we can think of an image as an m cross n inputs going from xzero x1 all the way up to x  m  n nothing great about that and we are interested in finding the influence of each of  these inputs on a given neuron ok now what is one way of computing influence that  you have learned in this course what is the hero of this course gradients right so  gradients tell you the influence so now can you tell me if i want to compute what is the  influence of this neuron or this input on this neuron what would you do  student refer time one hundred and twenty-seven  hjxi but can you compute that how will you compute that how do you compute the  gradient with respect to the input we have always stopped at the last hidden layer and  the weights before that so how will we do that how will you do this ok this is a trick  question just a hint is there a restriction on the chain rule or can you do it you can just  keep adding links to the chain right so what is so difficult about that you already  know how to compute the gradients till this point and in fact you will also know how to  compute the gradients till this point  and what is stopping you from doing it up to this point what if i just call this h instead  of x then you would not have a problem right and actually we call it h right we call it  hzero we can do it right it is straightforward so let us see      if i want to compute hjxi i can see that if the if hjxi is zero that means this pixel  has zero input on the neuron if it is large then it has a high influence if it is small then it  has a low influence so this is how i will see whether a pixel has an influence of the  certain neurons in the in some of the hidden layers and this is not restricted to  convolutional neural networks as you can see i am just actually treating it like a feed  forward neural network with parse connections ok  so we could just compute these partial derivatives and visualize this gradient itself as an  image so what do i mean by that is i am going to compute hixzero hix1 all the  way up to hix mn right so i am going to compute this m cross n entities and i can  just visualize this as an image now what do you expect this image to look like if zero  represents gray colour what do you expect this image to largely look like what would  you actually hope for this image should be largely gray because most of the input  pixels should not be influencing a given pixel in the hidden layer right that pixel should  influence by only a small number of pixels  so that we can say that this is the patch which causes it to fire and not that every pixel  in the input is causing it to fire because that is meaningless so that does not that is not  something that we care about how many if you get this please raise your hands so i  will just repeat it if a pixel fires for every pixel if a neuron in the hidden layer is  influenced by all the pixels in the input that means it is not really discriminating it is  not really specialized right we want neurons which fire only for certain patches in the  input so that we know that this neuron is responsible for this kind of a pattern ok  so if i plot this as a image i would want most of these entries to be close to zero right  because i want the influence to be zero ok    now the question is how do we compute these gradients so we will just treat them as a  free forward neural network we already know how to do back propagation across these  roots and we just need to add one more term to the chain right so i will just show you  what we will do here so i am interested in h32x2 so i will observe that there are four  paths which go from h32 to x2 or rather from x2 to h32 so i will just sum up the gradients  along these four paths right and if i solve it i will just be left with this ok so that is how i  will visualize so this is very simple we have done a lot of gradients in class so you can  just go back and check this and it should work out well ok  so you can just see this and this way we can just compute the gradients for all the input  pixels    and now i am going to plot it as a image and this is what my image looks like do you  see what is happening here its all very murky right most of it is great that is fine we  expected it but there is nothing really standing out right even in this patch where you  have some non gray pixels it is almost like the entire cat region is appearing as non gray  the influences are not coming out to be very sharp we would have wanted something  like only the eye pixels cause some neuron to fire or only the ear pixels cause some  neuron to fire and that is not really happening ok so it does not produce very sharp  influences so someone proposed something known as guided back propagation which  we are going to see next and that helps you to better understand the influence of the input  pixels  
task2/super_cleaned_audios/lesson82.wav,768.3006,the next one is a bit tricky so the third solution is to use something known as  hierarchical softmax this is a bit counterintuitive in the sense it is a very smart trick  but it is not something which is very obvious so just pay a bit attention on this it is a  neat way of handling this large vocabulary thing and this i think used in various nlp  applications where speed is important not often but wherever speed is important    so this is what our original network was this was the either you take it as a skip gram  model or you take it as a continuous bag of words model right let us take it as a  continuous bag of words model  you had a word as the input and then you had this large prediction and you had this  softmax computation which gives you the probability and you are trying to maximize  this probability for the correct word right where v w is the correct word     now instead of this the hierarchical softmax says that you construct a binary tree such  that your tree has how many nodes v nodes it has one node corresponding to every  word ok and there exist a unique path from the root node to every leaf node every leaf  node corresponds to a word and there is a unique path from the root node to leaf node of  course there will be overlapping things for example for this word the path is these  nodes and for this word also the path is like there is some overlap in the path  but for every word there is a unique path how many if you get that setup now let lw one  lw two up to lw p be the nodes on this path so i am calling this as lwone lwtwo lwthree sorry  sorry sorry sorry yeah actually it is so actually this is l on one l on two l on three that means  the third node on the path of on the second node on the path of on and so on right that is  how it is going to be and let pi w be a binary vector     so what is the size of pi w actually binary tree log of v right so the size of pi w  vector is going to be log of v so if there are eight leaf nodes you will have three nodes as the  size of the vector so for each of these things this vector takes on a value one so here the  value would be one because the path branches to the left if the path branches to right  then the value is going to be zero right so for every node or every word i have this way of  uniquely defining it is path i can say that the path is one zero zero is that fine for the word on  the path is one zero zero if i consider some other word the path would be different is that fine  and of course i have assumed there are only eight words here right that is why this holds if  there are either otherwise i would have a vector whose size is log v right now my v is eight  so it is just three    finally each of these internal nodes is associated with a vector ok so i have u one u two u three  so how many of these would i have if there are v nodes at the leaf how many nonleaf  nodes do you have in the binary tree v you all know this right  so if you have v nodes at the leaf then you will have v nodes internally so for each  internal node i have a vector associated with it so how many vectors do i have in all  u v and my input side is still the same right i have this w word or w context depending  on whether it is a skip gram or by or continuous bag of words model  so how many parameters does this model have is it same as the bag of words model or  less than the bag of words model or more than the bag of words model this is how you  will think you will see how many input parameters do the pool two models have how  many output parameters to the two models are input parameters same output parameters  how many vectors do you have u one to uv each of size k same as the original model  right it is just as an original model i had put everything inside as w context which was k  cross v right so it is the same number of parameters    so the total number of parameters in the network is the same    now for a given pair w comma c which is the correct pair we are interested in the  probability p of w given vc nothing great about this it is the same as i have been saying  always that we want the pa probability of w given c what we are going to model as w  given vc because vc is the representation of c and we model this probability now as the  following thing why does this make sense you just assume this is on and these are on  k’s right so on one on two on three why does this make sense  i will get the word on at the output only if the first element on the path was pi on one and  the second element on the path was pi on two up to the k’th element on the path was pi on  k how many forget that please raise your hands ok right so that is how we are  modeling it is it but what about pi on one pi on two pi on k how do you model that at  least this form is clear to everyone right if it is not let me know because then you not  understand the rest of the stuff yeah ok  so now see that modeling part is always in your hands right you know that you want  you are interested in a certain probability it depends on you how to model it so now  what you have done is you have con constructed a binary tree now i am interested in p  of on given some word vc right or some word vector vc now i can say that but the way  i am thinking about this is that i get the word on only if the first if i started from the root  node the first vector took on the value one or the first branch took on the value one the  second branch took on the value zero and the third branch took on the value zero so that is  exactly what i am saying here  it is a probability that the first turn that i took was a left turn then a right turn then a  right turn yeah the path is you have constructed the binary tree and the path is fixed  now for all the words how to construct the binary tree is a separate thing but the binary  tree has been constructed and every word has a unique path associated with that so that  word will occur only if that path is executed right so i am just trying to find the  probability of that path being executed  now i need to tell you what does each so how many terms are there in this product k  terms right how do i estimate each of these k terms is what i need to tell you ok can  you think of it how would i model each of these probabilities remember that every  node has a vector associated with it how many if you can think of an answer i hope i  are you saying what i think you are saying    so this is what i will do so as i said for the on example this is what you want this is  the path that you want to be executed    and i am going to model it as this  so getting a left turn i model it using this that dot product between the original word  vector which was the input word vector which was vc and the node representation of the  node associated with that particular node does this make sense so i will tell you what  we are trying to do so this path was clear that the probability is going to be a product of  these probabilities   now i want how do i get each of these probabilities so that is again in my hand right i  am going to say that i am going to train my parameters vc and ui where ui is the  parameter corresponding to every node i am going to train it in a such a way that  whenever i want this to take on the value one this should be close to one ok because i will  set up my loss function accordingly we will see the loss function  but i am saying that whenever i want the probability to be equal to one i am going to use  this to computed and alternately when i want the probability to be zero i am going to take  one minus that which is just this is that fine okay let us go ahead a bit and then we will  come back if you are still lost  so what does this actually ensure this ensures that the representation of a context word  vc will have a very high similarity with the node ui if the path takes a left turn there and  it will have a very low similarity with the node ui if the path takes a right turn their how  many if you get this part based on if you assume that this is how we are going to model  it when is this going to be high when the dot product between vc and ui is high when  is this going to be low   when the dot product between these two is low right there is a negative yeah so we ok  sorry i or rather when is this going to be low right so you get that so it is coming so  the word representation which is vc which is this guy would come to the come close to   all these representations or move away from them depending on whether you want to  take a left turn there or a right turn there  now what would happen to words which appear in similar context the same thing that  we have been discussing so far right they will come close to the node representations  which are along the path right is that fine so this is the context representation right  this is actually you are representing every context word by these three representations now  if a word appears in the same context it is representation is going to either come close or  move away from these representations right so words appear in the same context if  you have cat and you had sleep here then cat has to come close to this it has to move  away from this and it has to move away from this is that clear that is how we have set  up the probabilities  now instead if i had dog and again you had the context word as sleep now the  representation of dog also has to go close to this it has to move away from this and it  has to move away from this so in effect again the same thing is happening that the  representation of cat and dog are moving in the same directions so they will eventually  come close to each other how many if you get this intuition    and how many computations do you need now to compute the probability of this so  earlier you acquired that complex softmax computation how many computations do you  need now you definitely need these many computations and each of these  computations requires a sigmoid over or dot product right so that is much much lesser  than so you just need these many dot products as compared to your expensive softmax  computation earlier  so you see how you get the savings using the hierarchical softmax so this is as i said  this is not very intuitive it is like a really smart trick and it takes time to get your head  around it but i am sure if you go back and look at the slides you will get it right if it is  if you have just got 5zero percent of the idea here that is typically how it happens every  time but and i probably not figured out a better way of teaching this but once you go  back i am pretty sure that you will get to understand what is happening  so now the question is how do we construct a binary tree anyone has any thoughts on  that do we need to ensure certain things while constructing the binary tree okay i will  ask this as a quiz question just note that there is some subtlety here ah in practice this  is what is done you just randomly arrange the nodes on the leaf nodes and then you just  construct a binary tree from there right so you have distributed all your leaf nodes  randomly and on top of that you have constructed a binary tree my question is there a  problem in doing that which i will ask you on  
task2/super_cleaned_audios/lesson69.wav,338.9438,initialization methods batch normalization  welcome to lecture nine of cs70onefive today we will talk about greedy layer wise pre  training better activation functions better weight initialization methods and batch  normalization so today’s lecture is more like tips and tricks to make deep learning  work  so when you are actually experimenting with deep learning in practice what are some  of the things that you need to take keep in mind and it is also my way of connecting the  history that we saw to where we are today right so there were certain things which we  saw in the history and now i will try to bring those back and connect to where we are  headed from here right where we have reached today and where we are headed from  here  so that is with that in module one i will do a very quick recap of training neural networks  and not take more than five minutes and i need it for a specific purpose  so we already saw how to train such a very shallow neural network what was the  learning algorithm gradient descent and this was the update rule right in particular i  wanted you to notice that the gradient actually depends on the input  so when you compute the gradient formula you have this multiplication by x so it is  proportional to the input and this is one fact that we will use it at least a couple of cases  in the lecture today so this was a very shallow single neuron network what if we have  a wider network still which algorithm   student gradient descent refer time 0one40  gradient descent ok and we just have these three different formulae and for each of these  formulae note that the gradient or rather this gradient depends on the input that you are  feeding in ok i did not keep this in mind    and what if you have a deeper network so we saw a very shallow network we saw a  wide network and i am showing you a deep network what will you do again gradient  descent  but you will apply the chain rule for computing the gradients and again here in general  you will notice that for any of these weights wone wtwo wthree the gradient formula will have  this h i minus one what is h i minus one   student refer time 0twoone7   input from the previous layer right and h0 is the actual input so the gradient at any  layer is actually proportional to the input from the previous layer and this could either be  the input from the hidden layer or the actual input    and finally we saw this thin so we saw a wide network we saw a thin network now  we will see a wide network and a deep network right sorry we saw earlier we saw a wide  network and a deep network now we see a wide and deep network and here again you  have compute the gradient by applying this chain rule across multiple paths and that is  what we use and we call it back propagation and remember again they are the same  thing holds that the gradients at some point are proportional to the input at that layer  everyone remembers that ok    so this is important so what we have is things to remember from what we have seen  so far is that so training neural networks is basically a game of gradients right so you  compute the gradients and everything depends on those how will you update the weights  and everything from there on is about the gradients  and these gradients actually tell you the responsibility of the parameters towards the  loss and you appropriately update them and we saw a variant way different sorry  various variants of how to use the gradient so we saw the gradient descent we saw nag  momentum and all  but in all of these the underlying core thing was to compute the gradient and then do  some manipulations based on that and the other key thing is that the gradient at a  particular layer depends on the input to that layer ok     so now let us go back and just retrospect a better and see what is it that we have learned  so far so so far what i have taught you gradient descent oh sorry backpropagation is  something which was proposed way back in onenine86 right  so in fact it was existing before that but it was popularized by this work of rumelhart  and others in onenine86 right so but then in the oneninenine0s or early two thousand if back propagation  already existed and we could train deep neural networks then why did not we here so  much about deep learning at that time of course you guys were busy with school and  all at that time but why did the others or older people like me not hear about it  student computational power  computational power is that the only thing  student refer time 04threefive  computation and memory is are the only thing  student convergence  who said convergence ok good so actually what happened right in the late 80s and  early nine0s and even early two thousand when you used back propagation to train really deep  networks it was not very successful and what do i mean by not successful actually  what are the two things that could happen someone gave the answer already  student refer time 0five00  it does not converge right that means you do not reach the optimum solution right in  fact till two006 it was very hard to train very deep networks  and typically even a after a large number of epochs these networks did not converge  that means they were still at a very high loss and although in principle everything is fine  you have a deep neural network you have an algorithm that can train it but you are still  not being able to train it properly and you are not being able to make any practical use of  that  so that was the story till two006 so today is about what happened in two006 what it led to  in the next few years and then where we are currently right so that is the journey that  we need to make ok and that is why we started off with this quick recap of back  propagation because that is what i want to tell you that why did it not work earlier and  where are we today  
task2/super_cleaned_audios/lesson55.wav,509.5017,so in this module we will talk about sparse autoencoders  just some concepts before we jump into the actual way of doing this so hidden neuron  with sigmoid activation will have values between zero to one and you say that the neuron is  activated when this output is close to one and it is not activated when its output is close to zero  ok now a spare encoder tries to ensure the neuron is inactive most of the times what is  that mean  student close  it is close to zero for  student most of the  most of the  student refer time zerozero47  inputs right so i am passing a lot of inputs to it it will try to ensure that it is close to zero  for most of the inputs so in other words what does it trying you ensure i am looking  for the word average the average activation of a neuron is close to zero does that make  sense is that fine     so this is on what you see on the left hand side this is how you would compute the  average activation of a given neuron you have all the m examples you see what the  activation was for each of these and take the average right   now if the neuron is sparse then the average activation would be close to zero is that fine  this is all just different ways of saying the same thing now a sparse encoder uses a  sparsity parameter say rho and it is very close to zero say zerozerozero5   and it tries to enforce the constraint that on average the activation of any neuron in the  hidden layer should be equal to rho which is again close to zero now can you think of a  this is all fine in plain english right you understand what we are trying to do first of all  tell me why does this makes sense what is it that you are trying to ensure over fitting  happens because there is lot of dash  student parameters  parameters slightly abstract it out  student memorization   lot of  student memorization  memorization ok lot of freedom right i mean the weights have a lot of freedom to move  where ever they want to do whatever they want to do such that they can just drive the  training error to zero what have we done to that freedom now  student we are restrict refer time zerotwotwotwo  we are restricting them so any kind of regularization always tries to restrict this  freedom that the parameters or the network have in general right and there are different  ways of restricting this freedom you see that this is one of those ways right you are  trying to ensure that on average the neuron should not fire so it is clear that this some  kind of regularization any one has a doubt with that no   now the second question is taking slightly more on this right it is i can just move ahead  and i have convince you that this is regularization but can you think of bit more and see  what is actually being tried to achieve here what are we trying to do how many of you  get that or at least could here that first of all only the second row ok so yeah how many  of you can think about this like what is it trying to achieve   student refer time zerothreeonethree  right so on average neuron is going to be inactive that means where ever it is active it  is really going to capture some relevant information right so it is going to be active  whenever it is active it is going to adhere to certain patterns so we are ensuring that  each of these neurons are just a very few patterns and it has discriminative power in that  sense do you get that   so now if that means if i show it a three if i show it a two if i show it a one every time if the  neuron fires when there is no discriminative power in that but now if i ensure that the  neuron fires only a few times it will try to fire for meaning full patterns so it will try to  fire for a curve or a curve in the between as you have it in the case of three right you have  this cusp in the between in the middle so it will fire for some kinds of pattern  so that is what the hope is it is not just like adding some math and adding some  regularization but at least there is some intuition behind that how many of you get that  intuition ok good and now can tell me a way of putting this everything english is  fine intuition is fine but how do convert this to a mathematically equation   you want to ensure that rho hat l is equal to rho there will of course be different ways  of doing this the way these guys do it by adding this term to the loss function so  remember your loss function is always going to be l dash theta plus omega theta right  where omega theta does the regularization and l dash theta is your regular loss which  would be the squared error loss or the cross entropy loss or whatever loss you are dealing  with right   so remember this term is always there but the reason i do not bring it up so often is  because we have already dealt with it we know how to compute the gradients we know  how to do the back propagation and all that and now since your final loss is just a sum  of these two terms i know how to deal with this and i know that gradients are additive so  i just need to deal with the second term that is why i am only focusing on omega theta  l theta has been dealt with is that fine   now this is what omega theta is why does this make sense when would this take its  minimum value when rho is equal to  student watt  watt everyone sees that how many of you sees that please raise your hands ok fine let  us plot it and check actually right      so this is how that function looks like so i have plotted the function which i have  written here for a of course a single k right and my rho that i have taken is zerotwo and if i  plot that function for different values of rho hat l it will reach the value zero only when rho  hat l is equal to point so again go back and plot this and check and it is actually clear  from the equations itself that it will be minimized only when rho hat is equal to rho l  right  so that means this is a genuine i mean this is a reasonable thing to do we would think  of other ways of doing that and i am sure you can but this is also a reasonable way of  doing this    so now our last function is as i said it is going to be a combination of two values l theta  is a normal squared error loss that we have been dealing with and omega theta is this  sparsity constraint that you have added   now you already how to calculate the first term what are we interested in now so you  see that this pattern will keep repeating right so now you can do whatever you want your  loss function you have this generic frame of called the back propagation algorithm and  you know that a last part of that back propagation algorithm is going to remain the same  right only thing you are changing is the output layer or the loss function   just need to compute something there and the rest of it will remain the same how many  of you get this general idea and also appreciate it right that is why this is a very  powerful frame right you can just make minor tweaks at the top and you are rest of the  code has to remain the same   so you can actually go back and try out these regularization terms in mnist  assignments right if you really want to see what happens ok so now this is what  omega theta is and now what i am going to do it can be rewritten as this that is obvious  just expanding out the law of function   and by chain rule this is what i get now unfortunately the rest of the slide there is an  error the ta’s please note this i can kind of overlooked this ah but i will just convey the  idea right so you would want to do something of this sort everyone agrees with that  remember what is rho hat it depends on sorry rho hat l it depends on  student refer time zero7three6   h of l right it is the average activation of the l’th neuron and this depends on some of the  weights so that is why this chain rule makes sense and now how to compute this there  is an error on this slide but you have done enough gradients in the class for me to have  confidence that you can do it on your own everyone is confident that they can work it  out on your own     so i will skip this we will fix these errors there are some summation and other terms  missing here and the second part is actually correct which has been derived on the next  slide    but i would not go over this this is there are the slides again go back and look at it how  many of you are confident that you can do this on your own please raise your hands  yeah because we have done enough of this in class right   so you can you should be able to it no if you are not able to do it then i am not doing a  good job at teaching you right so you should be able to do it now fine and we will fix  these errors so ta’s just remind me after the class so everyone gets the general idea  you find a loss you find a constraint you define it with omega theta find out the  derivative of that with respect to your parameters and just change your gradient descent  upgrade tool accordingly   
task2/super_cleaned_audios/lesson41.wav,569.707,so in this video we will try to look at an explanation for why we need bias correction in  adam or in other words i want to explain why do i do this particular step why did i take  m t and v t as it is but why did i do this particular step which i called as the bias  correction step    so note that in the case of adam if you look at this equation for m t we are actually  taking a running average of the gradients and storing it as m t right so this is the  gradient and we are taking a running average or exponential running average of these  gradients exponentially decaying running average  so the reason we are doing that is that we do not want to rely on a single estimate so we  do not want to rely only on gradient of w t we want to look at the overall behaviour of  the gradients over multiple time steps and then take a decision so that means in one  particular gradient at time t is actually pushing us in some direction we do not want to be  very hasty and start moving there we want to accumulate the history and appropriately  weigh everything in the history that is the idea behind taking this running average of  radiance  and the other way of looking at is that we are interested in the expected value of the  gradients and not the point estimate at time w t right at time t rather so gradient of wt  which is this quantity which is the point estimate at time t we are not interested in that  were interested in the expected value and our behaviour should be according to the  expected value that is what we desire  so however instead of computing the expected value of this quantity which should have  been ideal we are computing mt as the exponentially moving average so in the ideal  case we would want that these two quantities are the same that the expected value of mt  the way i am computing it and the expected value of the gradient of w t should be the  same if that is the same then i am fine because then that means i am just taking the  expected value or the of the gradient instead of relying on the point estimate ok so let  us see if that is indeed the case    so for convenience we are going to just denote this gradient w t as g t because it is  cumbersome to write this grad symbol and we will just not make it so readable the  derivation that we are going to do so i am just going to replace that as g t so what i have  written is g t here instead of grad w t right so from now on i will just use g t for grad w t  is that fine ok so we have this expression for m t  so now let us just try to expand it and see what happens right so m zero it is going to be zero  because that is my starting points i have no history nothings so i will just going to keep  it as zero m one is my first time step at which it is going to be beta into m zero so i am just  substituted t minus one and t here and in the original expression i have just substituted  appropriate quantities for m of t minus one and g of t so m of t minus one is zero m zero and g of t  is g one and of course b zero m zero itself was zero so what will be left it is one minus beta g one  now let us look at what happens is m two m two is going to be beta m one plus one minus beta g  two but i already have an expression for m one so i am just going to substitute that here and  this is what i get now let us look at m three m three is again going to be beta times m two plus one  minus beta times g three and i have an expression for m two so i am going to substitute that  here and see if that leads to something interesting  so i have just substituted the value of m two here right and i already had the m three part here  the this term here as it is ok and now let us see so this already starts looking something  interesting you see some pattern here in particular we could take these one minus beta  terms outside they can be taken common and then you will be left with beta square g one  plus beta square g one plus beta g two plus g three so let us try to write this more compactly  right so i have taken one minus beta common and then i have written the remaining  terms as this particular summation and you can verify  so when i is equal to one this is going to be beta three minus one which is beta square into g one  when i is equal to two this is going to be beta three minus two which is going to be beta into g two  and when i is going to be three this is going to be beta raise to three minus three which is beta raise  to zero which is just one into g three right so we get back the same expression that we had here  of course there is a one minus beta outside so this is a more compact way of writing it and  this was for the threeth entry right this was for m three the third entry  now what if we want to write it for the t’th entry in general what if we want to write the  expression for m t    so in general m t we can write it as one minus beta as i equal to one to t b beta t beta raised  to t minus i into g i right so this three is here i have just replaced them by t s right you can  just verify that this is from you can just generalize from the third entry to the t’th entry    so now let us see we have the following expression we have simplified the expression  for m t and written it more compactly but what we were eventually interested in the  expected value of m t right we wanted to show that certain things holds for the expected  value of m t    so you just take expectation on both sides so this is what we will get ok now one minus  beta is of course a constant so i can move it outside the expectation so then i get an  expectation of a sum  now the expectation of a sum is the same as the sum of expectations so i can write it as  a sum of expectations ok now again beta is a constant so i can take it outside the expect  expectation so what i will be left with is beta raise to t minus i outside and expectation  of g i right so this is actually expectation of g one when i equal to one then expectation of g  two expectation of g three and so on  now we will make an assumption that all these gi’s that means the gradient at time  step one the gradient at time step two the gradient as time step three and so on they all come  from the same distribution ok we are going to make that assumption so let us try to  understand the implication of that right so let us say this was a distribution from which  g one came right suppose i am dealing with a scalar quantity and maybe this was the  distribution from which g one came now g two could have come from a different  distribution g three could have come from a different distribution and if that was the case  then expectation of g one would be different from the expectation of g two and so on  so what we have assumed to it will make things simple for us is that g one g two g three any g i  comes from the same distribution and hence you can say that the expectation of all these  gi’s is going to be just the expectation of g that is this one single distribution from these  which these entries come this of course a very strong assumption but we are going to  live with this assumption    so then this expectation of g i just becomes expectation of g so i have gotten rid of the  index i that means i can move it outside the summation right so this is what i will get  now these two have come out of the summation and inside i have this quantity now let  me just expand this quantity this is nothing but beta raise to t minus one plus beta raise to t  minus two plus so on at last you will reach t minus t which is just going to be beta raise to  zero  so this is nothing but a sum of a g p with common ratio beta and i can replace that sum  by this formula you know this is the formula for the sum of a g p with common ratio  beta so i have just replaced that and now what happens is this one minus beta and one minus  beta cancel out so i get this particular expression that the expected value of m t is equal  to the expected value of g into one minus beta t  so i will just take one minus beta t on the other side and i can move it inside the  expectation because it is a constant it does not matter so i will get as oh actually yeah i  can just move it inside so i will get it as expectation of m t over one minus beta is equal to  expectation of g t right and this quantity the one which i have circled is nothing but m  hat t right this was exactly the bias correction that i was applying if i go back to the  previous slide or the slide before that so this was exactly the bias correction that i was  applying  so what i have inside is this so what i have shown is that if i apply the bias correction  then the expected value of the bias corrected m t is equal to the expected value of the  gradient and that is actually what i wanted i wanted that whatever m t i am computing  if i look at its expected value it should be the same as the expected value of my gradients  and that is what i have arrived it  hence this bias correction makes sense and hence we apply this bias correction for  adam so this we have shown for m t we had a similar expression for v t right so for m  t we had this bias correction as m hat t and similarly for v t also we had this bias  correction as v hat t so you can derive the same kind of derivation for v t also and show  that that bias correction makes sense right so this is an explanation for why you do bias  correction in the case of adam  thank you  
task2/super_cleaned_audios/lesson40.wav,2405.666,in this module we look at gradient descent with adaptive learning rate so first we  will see motivation or intuition for why we need this and once you get the motivation i  believe the rest should be straightforward  so far what we have been doing is please pay attention on this slide i need to define  some notations and you should not get confused with that so far we have been dealing  with the situation where we had just one feature which was x and one weight  corresponding to it which was w and one bias which corresponded always on input  right now we are going to look at the situation where we have more than one inputs  that means earlier we were basing our predictions only based on the director and now  we are the director actor genre imdb ratings and so on  so here x one x two x three x four these are four different features or four different inputs that i  have and this is not x square just i know it is obvious but i am just making it clear  right so this is x one x two x three x four ok it is not probably the best choice of notation but i  will just stick to that so now each of these has a corresponding w one w two w three w four ok and  this is how your decision looks like it is the dot product between the weight vector and  the input vector ok this is how i am going to decide and that is a single sigmoid neuron  again  now given a single point xy do i need to again go through this computation sorry w p  oh sorry ok i will just erase this so this w is actually the vector w so it includes w one  w two w three w four and i am trying to take the derivative with one element of that vector do i  need to show you how to compute this have you seen this before can you tell me i  will show you the derivative with respect to w one can you tell me it will be a product of  some terms can you tell me what is the last term going to be    everyone gets this you remember this form so only thing which is changing is this  guy right so this part is exactly what we have derived and when we had one input we  just call it x and now we have multiple inputs so it will depend on that particular input  right which ever w one corresponds to  now make an interesting observation there so sorry before that yeah this is obvious if  there are n points we will just take the sum of the gradients with respect to the n points  ok now what happens if the feature x two is sparse what do i mean by that it is mostly  zero ok what does that mean so i am looking at lot of movie data ok amir khan acts in  a very few movies so if i have a feature which says actor amir khan then that is going  to be zero for most of the movies in my data scene right that is what i mean by sparse  so if i have onezerozerozerozero movies then probably only 5zero of them would have this feature as  one ok does that make sense so it is going to be very sparse now if the feature is  sparse why do we care about it what will happen what do we really care about when  we are talking about optimization in this course the gradients right that decides how  well we move in the plane that we are considering w b plane or the other in the end  dimensional region that we care about  so now if x two is sparse what would happen to this it will be zero lot of times because x two  is zero lot of times right so now just take a minute to understand this right so now  remember let us talk about stochastic gradient descent or mini batch gradient descent or  even batch descent you are going over all the onezerozerozerozero points that you have you are  computing the gradient with respect to all the parameters   one of those parameters happens to be w two right you have gone over onezerozerozerozero points  but in how many of those you will actually get the gradient for this only in the one5 which  x two was present right everywhere else the gradient would be zero so that means your  sum of the gradients overall the endpoints is going to be small or big  student small  small for this particular feature or for this particular weight it is going to be small right  because you do not have enough samples where you are seeing this so now what would  happen to the update you started with a random value for w two after one epoch or  making one entire pass of the data what would happen to the updates for w two very  small very few updates compare this to a feature which is dense do you get a lot of  updates so you see there is something unfair happening here if a feature is sparse it is  not getting updated enough   now that was ok in one situation if this feature was not really important but now  consider the exact example which i gave you which is this an amir khan movie or not  but suppose i am doing a classification whether this movie is going to be hit or not i  would believe this feature is very important because almost always when he is the actor  the movie is a hit right so you really cannot ignore this feature you want to learn the  parameters correctly for this feature do you get the setup right there could be cases  where your feature is very sparse but at the day at the same time very predictive of the  output that you are trying to learn right and in this case the output is whether the movie  would be a hit or not  the other example could be is christopher nolan the director so yes probably directed  less than onezero movies but all of them have been at some point in the imdb top two5zero or  something right so that is a very important feature but you will not get it very  frequently in your data right so you cannot really ignore these features that means  you still want to learn these features properly so you have sparse features you have  dense features we understand that for the sparse features the updates would be slower  and for the dense features the update would be faster the sparse would be zero in most  cases no no so you will do this zero mean thing  no but if it is a same value and you are going to zero mean the data right so the value  even if it is one it is going to be very close to zero right so you always assume zero means  otherwise all this does not make sense right because if your features are not in the same  range then anyways you are in trouble right fine so this is what i was trying to say  that the gradient with respect to w t is going to be zero for most inputs and hence w t will  not get enough updates and as i said if this is an important feature we cannot really  ignore it we have to make sure that it learns better  so what is the case that i am making for what do we actually need can you relate it to  the discussion on learning rate that we have been having so if the feature is sparse you  know it is going to get very fewer updates so can we change its learning rate so that  feature gets updates a bit faster as compared to the other features so you get the  motivation right how to do this is a separate story but at least we need to do this     so the intuition is decay the learning rate for parameters in proportion to their update  history so you have been recording the update history you have been looking at the  parameter you know all the gradient w two that you had calculated so far right how  many times you had computed the gradients and what those values were actually now for  these sparse features those are going to be zero  so your cumulated history is going to be small right for a dense feature it is going to  be high so why not make the learning rate inversely proportional to this history that  means if the feature has been updated fewer times give it a larger learning rate if it is  not updated if it is updated many times give it to a smaller learning rate can you give  me a mathematical formula for doing this this is the intuition just think about it for a  minute learning rate inversely proportional to update history ok good how many of you  get that but most of you will get it once i show you the answer  this is my gradient which i had computed so far i mean at this time step i will keep  accumulating it in a history vector so at time step zero i will take the magnitude of this  again i am taking the magnitude right because it does not matter whether you made an  update in the positive direction or the negative direction you just matters that whether  how much by how much it move so i will just square this quantity so that i can get rid  of the sign so i am taking the magnitudes and i am storing all that so at time step t  what would vt contain it is grad w zero square plus w one square grad w one square and so on  up till time step t  now this was my if i ignore this quantity this was my normal gradient descent update  rule now do you see what i have done i have divided the learning rate by whatever  history i had accumulated so for the dense features what would happen is the learning  rate will increase or decrease with time the learning rate will decrease right and for the  sparse features relatively less in fact if you have written gotten zero updates so far so  when you have to update the first few times you will have a very high learning rate  does that make sense right because this quantity would be zero so our eta would actually  be very large so you see how that intuition got converted into some reasonable formula    now can you tell me a way of actually realising this i want to show you that what  happens when you have sparse data and i want to do this with the toy example that we  had where we had only one feature and other feature was always on right so how do i  create this sparse data so you should think about these because these are things you  will have to do when you are practising machine learning and if you are working with  the problem and you want to create some simulated data so that you can verify some  hypothesis that you have so how would you do this  see i am going to create thousand data points right which is x y points and of course i  have this x zero which is always on right so x zero is always on i cannot make that sparse  what about the other feature if i am creating thousand data points what should i ensure  is that 8zero percent of them or some 9zero percent of them is always zero right just as the amir  khan case and most of the data it is going to be zero so what we will do is as i said we just  have two parameters w and b b cannot make sparse is always going to be on so what  we will do is we will make x sparse we just create random x y pairs and then for 8zero  percent of those we will set x to zero right so now this x feature is going to be very  sparse   so now i have created some data which is sparse one of the features is sparse and now  i want to see what happens when i run gradient descent momentum and nesterov  accelerated gradient descent and how does the algorithm behave and now if i apply this  algorithm which i did not name it is called adagrad ok this algorithm is called  adagrad if i apply this algorithm then what how does the situation change    so this is what gradient descent momentum and nag do now at least the difference  between momentum and nag should be clear nag blue curve is inside the red curve  right so oscillations are slightly smaller this is how they behave  now there is something very interesting that these algorithms are doing for this  particular data set that i have created can you spot it what is the interesting thing  happening here i want you to take some time and think about and relate it to the  discussion that we just had how many of you see what is happening here very few i will  give a hint ok it is almost as if these algorithms went to a school where they did not  teach pythagoras theorem now related to the discussion that we just had what is  happening initially so initially what is happening is you started from here ok and this is  the w b planes so you have w on the horizontal axis and b on the vertical axis  what is happening to all your updates initially where are you moving you are moving  along the b direction are you making any movements along the w direction no why  w was sparse its gradients are mostly zero it was not being able to make any updates in  the w direction or it was able to do make updates in the b direction it did as much as it  could do after reaching here it realizes that there is no point in going to be further right  it actually took uturn because it realise that there is nothing i cannot really go ahead i  have to now start working in a direction of w  so now in practice although in this toy example it does not it still converges fast but in  practice what will happen is you have just moved in one direction reached a point and  now from there again you are going to take right turn and reach to your destination  right so you are taking you are doing something which is not fast this is not how you  would go from this point to this point there has to be a better way right and this is  happening because w is not getting updated frequently all the updates are initially done  for b  now when it is no longer possible to change b because you reached the optimum value  for b then only you start changing w and that to very slowly because it will have to wait  for many updates to happen for that to happen how many of you get this so this is  exactly what is written on the slides because in our data the feature corresponding to w is  sparse and hence w undergoes very few updates and b is very dense and it undergoes a  lot of a updates  now such sparsity is very common in large neural networks which have thousands of  features right so you can imagine this now if i have thousands of features now  suppose i am doing credit card fraud detection ok now say one of my features is  corresponding to some education that the person had and suppose he has done some very  less sort after degree or less sort after curriculum  so that feature is going to be sparse where most of the cases but i cannot ignore it may  be this is the most predictive feature that i might have right so you could think of  various cases where you have thousands of features out of which many are going to be  off for a given example right everyone sees that this is the real world scenario where  lot of your features are going to be sparse and in many cases you cannot ignore the  sparse features ok fine now let see what adagrad does any guesses    so i am running this we should start seeing something a green curve starting from here  do you see what is happening expected now try to guess if you are going to run into a  problem i have deliberately halted the algorithm i just want you to think if you are  going to run into a problem ok all of you think you have something which makes  sense so now i have run it for in this case again this is the toy example hence you do  not see a lot of difference between these algorithms in terms of number of steps taken to  converge but in real world application it would be very different but now what has  happened is i have run the algorithm for as much i can and i am then stuck here i am not  being able to move forward why is this happening  well i am the histories accumulating it is growing now what am i doing to the learning  rate i am just killing it right it is eta by a very large constant now that is going to be  very small so no matter how big my gradient is it is going to get multiplied by a very  small learning rate and i cannot just move any forward anymore right so see that will  happen that is why in this case this is some point here which i do not want to go over  now and it is this  in fact i do not have an explanation for that but this one observation which people have  made that remember we have the square root in the denominator if you remove the  square root in principle you are still doing the same thing right you are still making it  inversely proportional to a cumulated history but it does not work well when you do  that that i do not know why it happens and i just read these comments at several places  that it does not work when you remove the square root from the denominator but that is  not important for this discussion that is just point for reference later on  so right now what i am trying to say is that it did the right thing it started making  updates for w also and started making larger updates hence we see this simultaneous  moment in both w and b direction but the flip side is over a period of time the effective  learning rate for b will decrease so much that we no longer be able to move in the  vertical direction right and if i am not being able to move in the vertical direction we  will not reach the minima in this particular example not always but in this particular  example you need to move further in the direction of b but a learning rate is not  allowing you to do that so that is what is happening  so now can you avoid this yes how multiply by so first divide it so that the  decreases then multiply it so that does not decrease all of these are interesting ideas i  am not i mean it is very hard to say upfront whether this is wrong or right but yeah these  are you get the idea basically something is happening which is you are aggressively  killing the learning rate    now i just want to make sure that you are not so aggressive so what happens because  of the aggressive killing is the frequent parameters they start receiving fewer updates  now this is what rmsprop does i want you to stare at this for a minute assume that  beta is going to be something which is greater than zero9zero or zero95 or something and try to  make sense of what is happening try to imagine what is vt is going to look like in terms  of grad w zero grad w one and so on to start from v one and see what happens what was v one  earlier and what it is going to be now ok but it still grows my magnitude when i am still  adding stuff so how does it help me in not blowing of the denominators  so yeah i think you most of you get so again this is the trick is basically you are using  this exponentially exponential moving average so even at the first step earlier i was  doing grad w t square now actually doing zerozero5 into grad w t square oh sorry grad w one  square right so that is what my v one is going to be now what is my v two going to be it  is going to be zero95 into zerozero5 grad w one square plus grad w two square right so this  quantity is even shrinking further and at each step this is going to keep a zerozero5 ok and  you see now at each step this is going to get multiplied by this quantity and shrink  further  so now i am not aggressively growing the denominator i am not considering the full  gradient but only a fraction of it and in fact a very small multiple of it so i am still  accumulating the history but i am not being very aggressive while doing that right so  you understand this everyone gets this    so now let us see if we run what would happen any guesses ok so initially now this  is i think a brown curve it is already there but you can see it so i will keep running it  and at some point it will diverge from the green curve yeah do you see that now i  have reached its destination right so at the point where the b learning rate the learning  rate for b was getting killed in this case that does not happen because you have  prevented the denominator from growing very large actually multiplied by its small  values so that it does not grow very fast   so adagrad got stuck when it was close to convergence because the learning rate was  killed and it was no longer able to move in a direction of b but for rmsprop it  overcomes this problem by not growing the denominator very aggressively ok now  can you think of any further modifications there is everything that you learned so far  and my everything yeah   yeah i am not very sure why that i agree that i am also bit surprised that it completely  overlaps with it i checked it and that is how it turns out to be and guessing it is an  artifact of the artificial data that i have created so it is trying to say is actually making  sense that it should not overlap so much right initially it should slightly be biased  towards b and then probably that is what you are trying to say right but i told it just an  artifact of this data that i have but what matters is from as going to say illusion but from  the illustration is that it actually does not kill the learning rate    what is the one idea that now think of everything that you learned in starting from  gradient descent then you tried to improve it using something then you tried to further  improve it and so on and now we have taken a slide d two from there you are now  focusing on the learning rates but there were other things which you are doing earlier  can you bring those back add momentum how many of you say add momentum as if i  can just added you are right actually  so let us see what we can do so it does everything that rmsprop does that means it  tries to make the learning rate inversely proportional to a sane cumulated history by  sane mean it does not allow the history to blow up and it also will use the cumulative  history of the gradients so let us see the update tool for adam so what is this term  doing actually it is taking a moving average of there is the same as the momentum  base role right just taking a moving average of your gradients ok the same analogy  that i am going to phoenix market city i am just taking all my history into account ok  and vt is again a cumulative history this is the same as what was happening in  rmsprop right where you get lost   now what would be the next step be can you give me the final update rule at least  think about it mt into vt no ok just try to think about it and it is very hard to say it out  there are too many grads and suffixes and so on so just think about what you did in the  momentum case ok now there is one more step which i am going to ignore i will just  say what that step is and then i will come back to that later on  so this is something known as bias correction ok just ignore it for the time being i will  come back to this discussion just for the time being just assume that i am taking mt and  dividing it by some quantity right so for all practical purposes i am just using mt just  dividing it by a quantity ok just for now that should suffice and then my final update  rule is going to be this   so let me go over this what did you expect here in a normal gradient descent they  should have been grad w t that means the derivative with respect to current w ok  instead of that i am using a cumulated history instead of using just this quantity i am  using a cumulated history does it make sense this is same as momentum base gradient  descent how many of you get that ok and now this quantity there is nothing new this  is the same as what rmsprop suggested that you divide the learning rate by a cumulated  history of gradients right so just a combination of these two one is take care of the  learning rate and the other is use a cumulative history does it make sense now ok fine  now this part is something that i need to tell you about so i will tell it to you after i  run the algorithm and then i will come back to that but is the update rule clear that it is  a combination of momentum plus killing the learning rate ok fine    it is a similar set of equations for bt    now let us see what happens to this algorithm is actually call at adam it stands for  adaptive moments right yeah what is can you tell me why that name  why moments  student sir mean is  good where is the mean here this is a mean this is a moving exponentially weighted  average right this is an exponentially weighted mean what about this what is this  quantity if you take the average of this is the second moment right exponentially  weighted second moment right so using the first moment and the second moment we  come up with an adaptive learning rate  so now i will run this algorithm are you able to see this see a coloured curve ok so  it is here you see that now ok do you see what happen do you see this curve everyone  sees that ok so what is happening it is taking uturns right so again whatever  happens because of momentum it is happening in this case also and then finally it will  converge again let me be clear that in this case now it should be very clear we need to  change who is ta for the slide so this colour needs to be changed or it should be  bright right from the first so what is happening is it is getting overlaid and then it  becomes bright when we need to have a brighter colour right from the beginning ok  so this again in this toy example right you do not really see the speed as such because  all of them are converging you know almost the same number of steps but this again i  repeat for the toy example but at least you see that the behaviour is very different and  behaviour is consistent with whatever you have put into the update rule right in one  case the learning rate gets killed in the second case it does not decay and in third case  when you using this moments sorry this momentum term you again have this behaviour  similar to the momentum gradient descent where you actually overshoot and then you  come back ok so is that clear all these algorithms ok now here is the million dollar  question    which of these two you use in practice so what are the options that you have for your  back propagation assignment even if you have not read the assignment you should just  tell me based on whatever you have learned you have gradient descent  student momentum  momentum  nag rmsprop  student adagrad  adagrad adam ok so which of these would you choose and if there is one or which is  called eve but it did not really gain much momentum but adam so in practice adam  seems to be more or less the default choice i should tell you that recently there was a  paper or called couple of papers which actually show that there is a slight error i mean  there is you could showcase where adam will not actually converge as expected with  but still then after that as is the case in whole of deep learning resources that one person  says this work and immediately the next is someone else this does not work or vice  versa right  so someone show that this does not work adam does not work in some cases but then  someone else did detailed study showing that in most practical applications ok you have  taken a toy data set where you can show something under some conditions adam will  not converge but if i look at real world data sets like mnist image data or something  those conditions do not hold there so adam really works well so in practice adam is  more or less the standard choice nowadays at least all the image classification work  which deals with convolutional neural networks and convolutional neural networks and  so on that uses adam as the optimization algorithm  we have used it largely for a lot of sequence to sequenced learning problems and it  works well although it is supposed to be robust to the initial learning rate right because  you are tampering with the learning rate as you go along right you are not sticking to  eta but you are conveniently blowing it up or shrinking it based on your requirement  so it should not be sensitive to the initial learning rate but we have observed that at  least for the sequence generation problems if you use one of these learning rates as a  starting point they work best of course of course these are heuristic right we also  depends on how much data you have and so on  if you are going to train but only thousand samples and first of all of course you should  question why are you using deep learning but you have gone pass that question already  has everyone else has then you are still be using a deep neural network and in that case  may be these learning rates are going to be very small but in general for a large number  of data sets out there which lot of academic research happens which are of reasonable  size these learning rates happen to be well in practice  now having said that many papers report that sgd with momentum either the nesterov  momentum or the vanilla momentum with a simple annealing learning rate we  remember we did this learning rate decay either a constant decay or that heuristic decay  that after you look at the validation loss and then decide whether to decay or not that  also seems to work at par with adam right so my advice would be that if you really  know what you are doing with sgd and momentum right that means if you really know  how to look at the loss how to track it how to adjust the learning rates and so on  with a little bit of manual tampering it should work as well as adam there are people  which show that it works well as adam but if you are just a practitioner who does not  really want to bother too much about setting the learning rate setting the momentum  setting the schedules on both of them remember for momentum also we had a schedule  and was just given by one of these papers and it might differ for your application you  might want to tweak that a bit so if you are not really bothered about doing all these  things then adam would just be over all the best choice right with very minimum  tempering of the initial learning rate  as i said some recent work suggested there is a problem with adam and we will not  converge in some cases but then it still i mean i would say that juries not out on that yet  because there is of course theoretical angle to it and also the practical angle again  practice has been used widely for the last three to four years at least and it works well in a  large number of applications right so that is why adam would typically be the overall  best choice  now there is this one thing which i need to do which is i need to tell you why do we use  this bias correction so now what do you actually want to you are taking a mean ok  you do not want to rely on the current estimate of the gradient but you want to take an  exponentially moving average of the gradients  now what would you actually would be doing all this what is the intuition behind this  since you are talking about moments and so on can you think in terms of probability  distributions so let me just try to say this we write that your gradients your values of  grad wt right and i will just i think alternately use gt instead of grad wt just needs to  gradient in that form it actually comes from some distribution depending on the point at  which you are right the gradient would change but it comes from a certain distribution  and now what you actually want at any time step when you are making this update this  particular update ok is it clear yeah when you are making this update what would you  actually want it should not move too much away from sorry  so now your gradients how you are computing say if you are doing the stochastic  version you are computing it for every point that you have right with respect to that  point you would have some loss function and some derivative with respect to your  parameters if you move on to different point you will have some different parameters  so there is some randomness in this ok so i am saying that these gradients would be  treated as random variables which can take on values according to a certain distribution  and now what do i mean so what would i actually want when i am making an  update so i have to one basic choices i could have just use grad wt which is the  derivative with respect to the current time step add the current time step ok instead of  that i know why i am not happy with that because it has this problem that it could pull  me to the extreme so at this point is actually saying change it change your w value in a  particular way which is more suited to me some other point would say something else  so what we want is that whatever update we make should be very close to the dash of  the distribution mean of the distribution right and instead of computing the mean we are  computing a moving average and exponentially moving average  so now what do we actually want to say i said that gt is the random variable for  denoting the gradient what do i actually want i want the expected value of mt should  be equal to what the true expected value of gt this is what i want because i want to i do  not want my updates to move in the extreme it should be closer to the average to the  mean of the distribution do you agree that this is my wish list this is what something  that i should desire for ok now let see what is mt actually if i want to write it as a  formula    so i have mt is equal to one minus beta i will call this as gt right so remember the gt is  grad of w t ok so now let us try to write formula for this so m zero i will set it to zero so  m one is going to be one minus beta in to gone ok mtwo is going to be beta into one minus beta into  g one plus one minus beta into g two and m three is going to be beta into one minus beta square g one  plus beta into one minus  student beta square  sorry beta square  student minus beta   minus beta wait is the first term correct  student yes  no beta ok so wait what am i oh beta is getting multiplied to g two plus one minus beta  into g three so what is the general formula going to be it is mt is equal to so one minus beta  can come out ok summation i is equal to one to t one minus beta  student beta square  beta raise to t minus i and gi right ok so this is what my mt is ok now let me take the  expectation of this this fine now ok this is b one minus beta now this is going to be is  that fine so what is this this is an ap gp what is the sum going to be  so it is going to be one over one minus oh it is actually sorry one minus beta raise to t over one  minus beta is that fine so what will happen is this will get cancelled and what you are  left with this one minus beta raise to t into e of gt ok so what is the relation that you have  e of mt ok e of mt is equal to one minus beta raise to t into e of gt what did you actually  want  student e of gt  right so now how will you ensure that divide by divide mt by one minus beta raise to t and  that exactly the bias correction that we have done ok sorry about this messy derivation  but i guess most of you get it if not we will just type it properly and upload it in the  slides how many of you got this most of you got fine so that is the similar derivation  for vt also fine so that is why we need the bias correction  
task2/super_cleaned_audios/lesson54.wav,1520.9931, in this module we will learn about denoising autoencoders  so the idea behind the denoising autoencoder is very simple what you do is you have  your original x i now for the minute for a minute just consider the discussion when your  x is are binary inputs ok so each of these red guys can be between can be zero or one now  what i do is before feeding it this input to my autoencoder the box is the autoencoder  what i do is i do a corruption  so the corruption is as follows with probability q i will set x ij that means one of these  guys to zero right and with probability one minus q i will keep it as it is ok so with some  probability q i am actually corrupting the data otherwise i am retaining the data as it is  and then feeding that data to the autoencoder why would this work binary input case  as i said just assume that the inputs are binary   we will also see the other case why would this work what was our problem earlier  that was completely able to reconstruct the training data right but at test time i had  issues now what i have done to the training data corrupted it just think for a minute  what will happen now i want someone to ask me a question in return oh that is the  corruption that i am choosing or you could flip it is what you are saying yeah if it is zero  change it to one so that is also fine that is the question i was expecting what is the loss  function now what is the loss function x hat my i minus x tilde i or x hat i minus x i  which choice makes sense  student first tilde  first let us the case take the case when i do x tilde i what happens in that case from this  networks perspective it is still learning to memorize the training data right it just this is  what it thinks as the training data and just trying to learn that transformation right so it  is not really helping my case do you understand that i just corrupted the training data  that is fine but from the networks point of view it still gets away by memorizing this  data and that is not what i want so what should i do can anyone tell me the i mean  can everyone tell me the answers  student minimize  minimize the error between  student an x i  an x i how many if you understand why that should help all of you gave the answer  but only few of your raised your hands why so hard to deal with this inconsistency    besides because i am still going to minimize my original objective function ok now can  the network get away by copying the input to the output so input remember the input to  the network is this and what i am trying to minimize this if i just copy x tilde i to the  output will my objective function be minimized no right so it does not have incentive  to copy now so what will it have to rely on say a reasonable probably twozero percent is  the standard right so even if i reconstruct i will not get zero error i will at least get some  twozero percentage  so let us let me give you an example and then let me know if you can figure out what  happens this example will contradict something else that we have done before but just  play along suppose my input features were height weight and bmi and we all know that  bmi depends on height and weight i hope all of us know  now can you think what is happening i am corrupting one of these inputs and i still  want everything to be reconstructed back so what will the network now have to rely  on it will have a now rely on this relations between these inputs also so again if i take  my example of digit three i have corrupted some of these pixels right but i still want to be  able to reconstruct three so it will have to be smart enough to learn that if i have seen this  and i have seen this then it has to be something in between which gives me a three do you  get the intuition  so now i am making it is job harder so that it is robust to changes at test time that  means a test time if my digit looked something like this it should still be able to predict  it as a three or it will still be able to learn the same representation as three do you get the  intuition right so that is what i am trying to do i am trying to somehow bring in the  corruptions that i would expect a test case and trying to make the model more robust  it can no longer get away by memorizing the training data because i am not feeding it the  correct training data it has to do something smarter than that everyone gets this i will  come back to your question everyone gets this please raise your hands yes yes this is all  under regularization no this is regularization no so at that case i have already made that  overfitting can happen in an over complete as well as under complete autoencoder  everyone gets that right i show that example where it could happen in both the cases  so my figure maybe over compete but it can just happen in any of these is that fine    it no longer makes sense for the network to just start copying the input data  different kinds of noises means yeah so let me try to answer that right so what  probably you are trying to say is that all my input images were three vertically written i  added some noise and managed it but now at test time suddenly you show me a three of this  kind like that will not work also that is what were your question was a different types  mean different values of the noise twozero percent twenty-five percent and so on    so we will first see practical application in which autoencoders are used and then  compare it to denoising at autoencoders so this the next few slides for those of you may  care is also a small answer to the difference between machine learning and dp    so suppose you are given this task which is handwritten digit recognition i see  everyone paying attention now i should say this before every slide ok so this is the  task handwritten digit recognition you are given some data where you want to classify  the digits into one of these onezero glasses the traditional machine learning approach to this  is we just construct a feature vector this is a twenty-eight cross twenty-eight image so i guess twenty-eight cross twenty-eight  pixels which is seven hundred and eighty-four i treat this as a feature vector and feed it to any of my machine  learning algorithms say svm or multi class svm or logistic regression or any of these  right and do a classification based on them this is what you would have done in your  machine learning course if i had given you this assignment right    now the autoencoder approach or in general the deep learning approach would be you  take this data which is the original feature representation that you had there is no  engineering feature engineering happening here right ideally i want to have features of  the form that if pixel twenty-five comma threezero was black and if pixel threezero comma twozero was also black  then probably i am drawing a curve somewhere so it could be one of these curvy digits  and not one or any of these seven or any of these things right so you want to do some  feature engineering  so typically in machine learning what you do is you start with these seven hundred and eighty-four features you  observe a few things and you have these handcrafted features added on top of these  right so you will add some more features to the data now the deep learning approach  is that you let you also learn the features on their own so how did we learn these  features we took this original input we passed it through the an autoencoder which  captured some of these relevant characteristics  the differences we do not really know what these relevant characteristics are that  means you and i cannot read them and make sense of them i cannot say that this pixel is  actually capturing the interaction oh sorry this neuron is actually capturing the  interaction between my sevenzerozero pixel and sevenonezero pixel i cannot do that i could have  handcrafted those features if i believe that all my data is around the center i could have  handcrafted some features which say that capture the interactions between those that is  what you do in machine learning  here you are trying to learn the features also on their own right what would happen if i  add one more layer to this autoencoder i would learn even more complex interactions  between these features so this neuron is actually learning interactions between all the  input neuronsok i add one more layer here again this neuron will learn all the  interactions between these abstract representations right so i could learn more and  more abstract representations of the input so i am not doing feature engineering i am  just throwing data at the network and i am assuming that it will learn better and better  representations  now i am doing this in the autoencoder setup where actually i am trying to optimize the  objective function of minimizing this loss and of course the squared of this loss is just  fine so first what i will do is i am not happy with my original seven hundred and eighty-four dimensions so i  train a autoencoder to learn some k dimensions which are good i know these are good  because they are able to reconstruct the data perfectly to a certain extent right of course  because you add regularization it may not be perfect but it captures the essence of the  data you get that  so i have better dash representations now feature representations right my original  feature representation was seven hundred and eighty-four i have come up with some better representations now  what will i do was my task to learn feature representations what was it classification  right so what will i do now is i will i have learned this much from the autoencoder    i will throw away the last layer i do not care about the last layer what i care at the last  layer is a classification problem right so i will construct a new neural network where  the first two layers of the network are the same as what i learned from the autoencoder and  on top of that i will add an output layer and now i will try to train this network how  many of you get what is happening here those of you do not get it can you ask me  some questions let me just try to answer on my own it is like playing chess with yourself  so this is my original input seven hundred and eighty-four dimensions what i have learned with autoencoders is a  smarter representation of this data ok now one simple solution that i have is i have this  onezerozero dimensional data suppose this is the representation so for all the training examples  instead of using that seven hundred and eighty-fourdimension data and feeding it to a multi class svm what i can  do is i can first compute this onezerozero dimensional representation and feed that to a multi  class svm is that fine and you see that should work better in practice because i have  reduced the dimensions i have reduced the dimensions smartly  and now i can train this network is this fine all i am saying is instead of a multi class  svm i could also have a neural network right i could feed that representation to a  neural network so what would that neural network look like onezerozero what are the  parameters here w belonging to onezerozero cross onezero how many if get it now ok so this is  what i could have done  so i have learned a better feature representation and now i am using that representation  to learn my classifier if i do this in an end to end manner that means my feature  representation is also came out of a neural network and my classifier is also a neural  network then i have a complete end to end solution for this you get this    now we will see a way of visualizing this and then we will make some observations  from the visualizations so first let me tell you what the visualizations is    so i am returning to the autoencoder setup so i had this input and i had this h  dimensional or k dimensional hidden layer now i can think of each of these neurons as  something which gets activated for a particular type of input is that fine what do i mean  by activated it is output would be  student one  remember this is a logistic neurons that we are talking about or even tanh neurons the  output would be one so it is the maximum output that you could gain fine now so for  example h one is equal to sigmoid of this when would this fire when where w one transpose  x i is very high right when you are in that regime where the sigmoid flattens right this  regime ok when it is very high   so i want to be able to maximize w one transpose x i do you get this i want to be able to  maximize this i want to find my w one transpose is fixed now because i have trained the  autoencoder i have got these weights this is all post mortem right i have trained the  autoencoder i have got these weights now i want to find an input which will cause this  particular neuron to fire  so what is my max what is my optimization problem maximize just help me out  maximize w one transpose x let me just call it x and the optimization is with respect to x  right because i want to find the x which maximizes this quantity my training is done i do  not no longer care about changing ws my training has been done i am interested in  finding x’s which will maximally fire this    so and i am going to assume that all my inputs are normalized this just makes some  analysis easier and remember that normalization is always ok you always do that so  this is the optimization problem that i am interested in solving what is the solution to  this how many if you can solve this no i want to find the x i  student refer time onefourtwonine  now i have trained the autoencoder now i have known all these the one i am  considering one column of the matrix w one i want to see what is the input that i should  give so that i am sure that this neuron will get activated and i know that this neuron  will get activated if i maximize this quantity right   so i want to maximize that quantity and find an x such that it will get maximized i was  just hoping that no one brings in eigenvectors w one is a column it is not a matrix just try  to work it out what is this this is a dash between w and transpose and x i dot product  when would the dot product be maximized when they are both in the same direction  right that means you know the direction is going to be x i is equal to and what did i  want the norm to be now do you get it fine    so the solution is going to be this is fine w one by the norm of w one so just remember  that this quantity is going to get maximized when the dot product is maximized the dot  product is maximized when both x i and w one transpose are in the same direction right so  that means x i should be in the same direction as w one and i also wanted this constraint  that x i should be the norm of x i should be one so i am just dividing w one by the norm of  w one  so i know now what is the input i should feed to the network so that one of these  neurons fires now what i am going to do is i am going to plot the xi’s which maximize  each of these neurons i am going to consider some onezerozero neurons in the hidden layer and  i am trying to find out the input image which is going to maximize or which is going to  cause each of these neurons to fire do you get what i am trying to do even though you  do not get why i am doing it but do you get what i am trying to do ok  so what am i going to do is this is a vector right so i am just going to try to plot this  as an image of the appropriate dimension    and this is what i get with a vanilla autoencoder there was no noise this is what i get and  this is for the mnist digit data set right so my data is two three one and so on digits this is  what happens when i get twenty-five percent nice and this is what happens when i get 5zero percent  what do you understand from these figures remember that each of this is the figure  which caused one particular neuron to fire is that clear each of these is a trigger which  caused one neuron to fire   one image yeah one box corresponds to one column yeah so it is just that the  dimension of the column is again twenty-eight cross twenty-eight so i am just plotting it as a twenty-eight by twenty-eight  image so i will just let me just clarify that is i think that is what i said yet so what is  the dimension of this in fact you just know this right this dimension of this is twenty-eight cross  twenty-eight  so i can just take that vector and again plotted as a twenty-eight cross twenty-eight image so what i mean  is this is seven hundred and eighty-four right so x i is a seven hundred and eighty-four dimensional vector i am just taking it as a twenty-eight cross  twenty-eight image and plotting it because my inputs were actually images so i am just plotting  those images fine so at least you see what i am doing here and what i am telling you is  that each of these boxes that you see corresponds to one of these images so i had  images x one x two up to x k such that each of these caused the k’th neuron to fire ok now  what are you seeing here i mean what how do you make sense of what you are seeing  and remember in the mnist ok sorry so let us try to forget all this neural network and  everything and let us just try to see yes the weights would be  student more distinct  no why do you say the weights are more distinct yeah but on average you would be still  reducing it right ok so let me just explain what is happening then we can come back to  this so now we have this set up we had some input we had a certain number of  neurons here and then we had the output ok this is what our neural network was trying  to do  student refer time one85three  now let us take this task of recognizing a digit now how do i actually recognize a digit  if i want to distinguish between a nine and a three i would try to see if there is a curve in these  positions and it is not there in this hence this is a three this is a nine that is something roughly  like that right so in other words i am now i have given delegated so that means what i  do is i think of three as a combination of you get the idea as a combination of these images  with these strokes right so this is actually this stroke this is actually this stroke this is  roughly this stroke and so on you get the idea  so i think of three as a combination of many of these strokes right now what i would like  is if this guy could detect one of these strokes right the other guy could detect one of  these other strokes right now you see that some of these strokes are shared across digits  for example all these strokes here look at the digit nine these strokes gives common to three  and nine both right but some strokes would be missing for three some strokes would be missing  for nine and you would have extra strokes in both of these so now each of these neurons  could actually recognize these strokes then a combination of the information that each of  these neurons is capturing could help me decide whether it is a three or a nine how many of  you get that intuition  student refer time twozerotwothree  so i would like each of these neurons to detect certain strokes ok that means i would  like this neuron the first neuron to fire for an input like this where there is a stroke at the  bottom i would like some other neuron to fire for a different input whether there is  stroke here now can you relate this to what you are seeing in the picture in the second  and third picture this neuron is firing for inputs which would have a stroke at the corner  right and you see different neurons are firing four different strokes so each neuron is  trying to capture something relevant and together now i could combine them to get the  final output how many of you see this how many of you do not get this  so to ask questions otherwise i cannot really help it how many of you want me to go  over this again which part yeah so let me just repeat what each of these boxes is right  so each of these boxes is the image which causes the k’th neuron to fire right so  remember i decide i came up with this that this is the input which causes the second  neuron to fire what was the dimension of this input twenty-eight cross twenty-eight   so i am just plotting that twenty-eight cross twenty-eight input right and i am realizing that this input seems  to be something which has a dark spot here right so now just related to the analogy that  i am trying to give at the bottom that this neuron fires for inputs which have a stroke  here that is that is capturing and there are other neurons which are trying to fire for other  strokes  and i would want these neurons to capture different strokes so that together they  captured all the information in the image and helped me decide that a combination of  these strokes gives me a nine a combination of these other strokes gives me a three is that clear  now you also  student yes sir  so yeah so now the thing is this right the again the same thing you could learn to  reconstruct the output but you may not capture the important characteristics in the input  right so now as you keep making it is job harder it has to rely on capturing these  important characteristics in the input right and actually if you look at the difference  between the second figure and the third figure right let us look at the same guy here  so you see that this is actually thicker and wider the stroke that you see here is thicker  and wider so now it is actually relying on more neighborhood information to fire it is  not firing just for this stroke but it is fighting for a larger stroke it is also requires more  neighborhood information because you are corrupting the pitch  so it has to rely on information from the other guys the same example that i gave for  height weight and body mass index right the same thing holds here i have corrupted a lot  of inputs so now it will fire only if it gets a lot of information from the neighboring  inputs also is that fine ok and i now coming back to your question yeah i do realize  now what you are saying that the weights are actually becoming larger yeah it makes it  more robust but again  so regularization just does not always mean that your weights have to be small right that  is one way of constraining or regularizing but this is another way of regularizing where  you are making it more robust but it does not necessarily need to lead to the same  solution where your smaller weights does that make sense it is ok for most of you any  please raise your hands if this     and this is same thing that i have written here    now we saw one form of this function ok which was just flip the input if the output is  just corrupt the input right you could also add a gaussian noise so you could take the  input add a gaussian noise to it with zero mean and then again try to reconstruct the original  input back is that fine so you could just use different noise functions to do this so we  will now see such a denoising autoencoder where we have actually added a gaussian  noise instead of the zero one noise or the corruption that we were doing    yeah so the purpose of this particular example that i am giving is to compare an  autoencoder which is regularized by adding this gaussian noise with an autoencoder  which is regularize by using weight decaying right the ltwo regularization so ltwo  regularization is also known as weight decaying because you kind of decay the weights  right you force the weights to be small   so what they showed is that with denoising autoencoder using a gaussian noise you  actually learn something known as edge detectors right so you see all of these are  trying to detect edge again the same thing is happening i am plotting the images which  will maximally cause a particular neuron to fire and it looks like all these neurons fire for  different edge patterns in your original data  so now they are capturing all the edges in the data and the combination of these edges  should tell you what your final class is ok and this seems to work much better than the  weight decay filter which is not really capturing any regular pattern ok so this is just an  empirical evidence that an autoencoder with a gaussian noise seems to do better than  autoencoder with the ltwo regularization   
task2/super_cleaned_audios/lesson68.wav,954.5556,so in this module we will look at dropout now  so the intuition that we have developed in the previous module which was about  ensemble methods is what that is that ensemble makes sense in most cases because  you do not expect the errors of these k models that you are using to be perfectly  correlated and we saw that whenever they are not perfectly correlated you are going to  get some advantage  now how do you do this in the context of neural networks so remember what was  bagging multiple instances of the same network trained on different subsets of the data  what is the problem with this in the context of neural networks each of these neural  networks is very complex training each of these is going to take time and i going to train  k of them is that fine right  so you decide ok sorry so one option that you have is you train several different neural  networks having different architectures right but this is going to be expensive because  you have to train k of them the other option that you have is you train the same  network but on different subsets of the data this is also going to be expensive  so whatever ensambling sampling techniques you can think if in the think of in the  context of neural networks which are essentially these two techniques different  architectures and take an ensemble or train the same architecture on different subsets of  the data both of them are going to be expensive right  so now how do you go about it and it is not just training time expensive it even if we  manage to train it at test time again when you are given a test instance you have to pass  it through all of these complex neural networks each of which is going to take some  computation and then take the ensemble of the outputs right so even at test time it is  expensive it is not just that that training time it is expense    so now dropout is a technique which addresses both these issues which issues train  time computation as well as test time computation so it effectively allows training  several neural network architectures without any significant computational overhead so  we will see how that works and it just not training time as i said it also allows us to do  this quickly at test time    so again let us see so again here ok i will get to it when i know so drop out actually  refers to dropping out units from the neural network  so this is my original neural network and i am just talking about one neural network  forget about ensembles just one neural network is what i have now what dropout says  this you dropout some units from this neural network that means dropout some  neurons and when i dropout some neurons i am also going to drop out the incoming and  the outgoing edges otherwise where are they headed right so i am just dropping out  so basically what is effectively happening here i am getting a new network architecture  right at least that is clear that is what dropout effectively does but i have already made  a case that i do not want so many architectures that because it is a headache to train all  of them and again a test time i have to pass it through all of them right  so i need to still fill that gap but drop out says that drop some units and you will get a  new architecture but how does that simplify life we will see that and now each node is  actually retained with a fixed probability for the hidden nodes and even further input  nodes  so then we were not wrong in actually dropping out the visible node because you can  do dropout at the visible nodes also ok anyways yeah so for the hidden units you would  drop them with a probability fifty percent and the input units you will drop them with a  probability of twenty percent typically it again is some hyper parameter that you will have to  tune but typically this is what you will do and i hope you see that dropping nodes from  the hidden unit from the input unit is same as corrupting the input data right it is same  as adding noise to the input data is that fine     so this is the idea now let us see how to actually implement this idea okso suppose a  neural network has n nodes using the dropout idea each node can be retained or dropped  an example in the above case i have dropped some five nodes to get a thinned network  so if there are n nodes what are the total number of thin networks that i can get from it  and so that means i can get two raise to n different neural networks am i happy about  this or sad about this sad there is just too many neural networks how can i train them  actually right  so how do i do this i am just creating a lot of suspense without giving you the answer  ok so first trick is share the weights across all these networks ok we will see what  that means and the second trick is sample a different network for each training instance  ok none of which is clear at this point i can see i can read your faces i am good at it ok  so let us see how to do that    so we initialize all the parameters of the network randomly or whatever may be used  and start training when i start training i will pick up the first training instance or the  mini batch or whatever i am doing we apply dropout resulting in this network  what will i do and they forward prop forward propagation right ok now ok we  compute the loss and back propagate how some weights are missing right how do i do  back propagation now i have deliberately dropped up some of these connections they  did not participate in the forward propagation this back propagate which are the  parameters which will update now only the ones which actually participated right  so i will just do back propagation just look at the red arrows i will just do it over the  paths which are actually present in my network fair enough right that is what you meant  by normally ok that is normal ok so i will just do it over the weights which actually  participated that is fair enough that is the only thing you could obviously do    now i take the second instance again i apply dropout and quite naturally i will get a  different thinned network as you see the figure three in this slide ok what would i will do  now  student forward propagation  forward propagation then compute the loss back propagate to compute the loss ok and  then  student back propagate  back propagate again back propagate only to the  student active nodes  active nodes so these other nodes which will get activated so what is happening here  is now trying to relate it to what we were doing in bagging right where we are trying to  train these different networks on different subsets of the training data right do you see  something similar happening here there are many such thin networks each time i am  sampling a different network and updating it right  so it is equivalent to training these large number of networks on different subsets of the  data right but then the problem is that some of these networks may never even get  sampled there are two raised to n of those my amount of data is definitely to be less than  two raised to n  so some of these networks might just not even get sampled then what is happening  or they would get sampled very rarely right for example what is the probability that  again i will end up with the same network we are computing it good it is very less ok  i am fine with that at 7threezero right  so it is a very less right so it is quite likely that this network will never be sampled  again that means for that network the parameters are getting updated very few times  am i fine with it yes i am why because the same weights will get updated for a  different network i am just using the same weight matrix throughout remember that my  w matrix or w one w two is the same throughout  it is just that at different depth subsets different instances i am just touching some  portions of this w one and i am not touching the other portions of w one so now what  would happen so i have shown you two training instances right what would happen to the  weights which were active for the first training instance as well as the second training  instance it will get updated twice and which are active only once  student refer time zero7five9   only once right so over a period of time many of these weights are shared across all  these networks that i am sampling right so even though a particular network is  sampled only a few times its weights will get updated many times via these other  networks which are similar to it do you get that how many of you get this ok good  so what is happening i will just repeat that i have just one weight matrix i am sampling  a thinned out network which only uses some of these weights  so for that training instance i will update those weights now i know that the likelihood  of the same network getting sampled again is very less but i do not care about it  because i could sample a different network but i am sure that some of these weights  will again repeat in that right and in that i told they will get updated so even though  each of these networks is seemingly getting very few updates overall all the weights  shared by these networks are getting updated as much as they should be is that fine   everyone gets this idea ok fine and while i am also taking care that similar things like  early stopping or weight regularization ltwo regularization where i am not allowing a  single weight to continuously grow or something otherwise because these weights will  be off for some networks is that fine you see the connection between early stopping l  two regularization and this is that ok    and so each thinned network gets trained rarely or sometimes even never but i am not  worried about it because it is weights will get updated through some of these other thin  networks     this is all finite training time at training time what is happening is this is one of these  blue guys introduce on with the probability p that means the weights going out of it  who are available with a probability p right and other times they were not available  now what do i do it test time i cannot let me finish this ok i cannot take an ensemble  of d ok the answer would have been that at test time instantiate all these two raised to n  networks pass the training passed the test example through all of them and then take an  ensemble right but of course that is probablitivly expensive so what will i do at test  time what is the simple trick that i will do so he says that just use this network  and just use the final net matrix that you had no but then you have guessing out of the two  raised in the sample some small number of those and do it actually dropout uses  something very simple than this what it says is that each of my nodes was present only  p fraction of the times in the training data ok that means one way of looking at it is that  so imagine that you could think of this as the analogy is that all these nodes are  participating in a discussion right where they trying to see how to do this job properly  but with probability p they all sleep off right   so at the end of the meeting you will trust each of them only with probability p so that  is the simple trick with dropout uses it says that just scale their weights by p because  that is how much i trust this node it only participated in p faction of the decisions so  that is the confidence that i have in it  so if it is saying that with wone weight do this i will only do it with p into w one weight  does that make sense ok and there is again a squared egg with vacuum kind of  explanation for this ok which was there in the quiz last year which is very convoluted  it does not really give you the true picture because you can derive some math and so  that this is mathematically proper but that again works in very specific conditions but at  least if you get the intuition that is fine that what we are saying is that these nodes will  leave an active a few number of times so i will only trust them that much and i will just  scale their weights by that factor  so at test time i will just pass my test instance through one network which is the full  network with the weights scaled according to the rule which i just said that is exactly  what dropout does    so what dropout actually does is we will apply some kind of masking noise to the  hidden units right since the same as seeing that you are computing the hidden unit but  then you are masking it off ok  so what is the effect of this i will give you the answer and i like i like you to think  about it the answer is that it prevents the neurons from becoming lazy what do lazy  people do they depend on others yeah actually yeah they depend on others now so let  me answer that give the answer for this and then tell me whether that is still contradict  ok  so let us see right consider this layer of neurons all of these are collectively responsible  for what happens to this guy right now you see what i mean by neurons becoming  lazy i could just see ok i will not give my input these other neurons will take care of it  they will adjust their weights  so that they eventually it will fire or not fire or whatever right you see that could  happen but now these neurons cannot rely on their neighbors because they do not know  when their neighbors are going to ditch them right they will suddenly drop off ok and  now i was waiting for my neighbor to actually do something and he is not going to do it  so i have to be alert always do you get the analogy  so these guys are collectively responsible for something and they know that some  people in the collection are going to betray them so each of them has to be more  careful so the more technical term for this is that does not allow the neurons to co  adapted  so it does not allow them to get into this mutual agreement that you take care of certain  things i will take care of certain things and together we will do the job right you do  question one i will do question two i am ok it does not allow them to do this  so let us just concretize that intuition a bit for so essentially a hidden unit cannot rely  too much on other units as they may get dropped out at any time each hidden neuron  has to learn to be more robust right it has to do the job as if it is the only guy responsible  for the job ok and let us consider one of these neurons h i    and let us see that a h i learns to detect faces sorry it learns to detect a nose so i am  trying to do face detection whether an image is about a face or not and h i is the feature  which fires if there is a face somewhere if there is a nose somewhere in the image is  that fine  now if all these guys start acting lazily ok this guy is going to detect a nose that  means definitely face will be there so i do not need to do anything right what would  happen now suddenly this guy is going to go away dropped out so then these other  guys need to do one of two things either add redundancy that means one of them should  also take responsibility for detecting a nose or do it in a different way take  responsibility for detecting the lips or the eyes or some other part do you get that right  because you know that i cannot co adopted with my other neurons i cannot say that ok  in these front facing faces you just detect the nose and will be done and we will all keep  quiet right  i do not know whether you will do your job properly so i will have to add more  redundancy you detect a nose i will also detect a nose or you detect a nose and i will  detect something else which helps detecting the feature right so that is why these  networks become more and more robust as you add this dropouts    so that is all that i had to say i still do not know whether i have answered your question  or not all of them try to detect nose see as long as that helps reducing the final loss it is  fine it is just the case that you would have some training images where the nose is not  visible maybe that person is drinking something right  so for at least for those training instances someone else has to take care that you detect  from the other images right otherwise a loss would not be zero for that training instance  so as long as you have some training instances see if all your training instances can be  detected just by detecting the nose then there is nothing wrong in all of them trying to  detect the nose so if the training it is like that it will happen but the hope is the training  data is not like that right is that fine so we will end here  
task2/super_cleaned_audios/lesson83.wav,436.6606,so now from here we will move on to yet another way of learning word representations  which is known as the glove representations  so the count based methods rely on global co occurrence counts from the corpus for  computing word representations that is what we saw in svd they look at these co  occurrence counts and from there they build the word representations  the predict based models set up a learning problem where you have this feed forward  net network and it tries to predict certain things from the given words and then you learn  the parameters of that network and you set up the task in such a way that the parameters  actually correspond to word representations so this was the difference between count  and predict based methods now what is the obvious next thing to do like hear the  answer from a few of you but i want to hear it from everyone  what is the obvious next thing to do you have count based methods you have predict  based methods combine the two right so come up with some kind of a hybrid so that  is exactly what glove does which is known as global vectors    so i will go back to the co occurrence matrix so remember x ij encodes the important  global information about the word i and j and whether you replace it by pmi or ppmi or  just keep the counts it just gives you some information about how many times these two  words actually appeared together  so x ij encodes this global information and i call it global because it is computed from  the entire corpus fine why not learn word vectors which are faithful to this information  so what do i mean by that suppose v i is the representation of the i’th word and v j as a  representative the j’th word which i want to learn i do not have these representations i  wanted to learn now this gives me the dot product between them which gives me the  similarity between them why not i set up my task in such a way that this similarity is  actually proportional to this probability  so what does a similarly tell us how well these two go together what does p of j given i  tell us how likely j is given i right so does that make sense to have this analogy that  the dot product tells me the similarity the other notion of similarity is that how likely j is  to appear in context of i which is given by p of j given i so why not set up my task such  that whatever vector as i learn are actually faithful to this global similarity that i have  computed from the entire corpus how many if you get this intuition  how many if you see the difference between this and the predict based models in the  prediction based models you are operating at one word pair at a time here you are  looking at these global counts ok we are trying to directly learn vectors which are  faithful to your global similarity as given by your co occurrence counts you get the  merger between the two methods you should not get it yet because we still have to do  something or at least you get the intuition now what is p of j given i it is actually this  ok  so i can write it as this ok and similarly i can write the other guy v j transpose v i and  that is going to be different because that is going to have p i given j instead of p j given i  so i will have log x ij is fine but instead of x i i will have x j here    now if i add these two equations so i am going to add this equation and this equation  so the left hand side i just get two times v i transpose v j because v i transpose v j is the  same as v j transpose v i and on the right hand side i get certain quantities so this is  what i would actually want my word vectors to look at look like i would want my word  vectors to be such that when i take their dot product they give me the quantity on the  right hand side and this quantity has come based on counts learned from the corpus  so i have counts on the right hand side and i have learnable parameters on the left hand  side so you see how we are merging these two but how do you learn this problem  now it is ok to say what i like what i have said now is that this is what i desire i desire  that my word vectors should be learned in such a way that they are faithful to the global  counts through the following equation this is what i desire desiring something is one  thing but now how do i set this up as a learning problem  so when i ask you what is the learning problem what do you need to think about  objective function good that is a good start so what is the objective function for this  what are the parameters of the optimization you are optimizing with respect to what the  v i is and the v j’s right all the word representations how many of those do you have v  each of size k so those are your parameters for optimization now what is the loss  function if i give you the loss function it will look very very obvious but i do not want  to do that  so just continue thinking about that while i will make some more simplifications to  what we have here now what is this count this is the co occurrence count how many  times these two occur together what is this count the number of times the word i  appear so this depends only on i what is this count the number of times the word g  appears so to make the model more flexible that means give it some more freedom  what i am going to do is instead of log x i and log x j i am going to introduce parameters  b i and b j ok    i am saying that these parameters can also be learned so effectively using all these three  i should be able to get this this is what i desire now set up the loss function using these  two things come on that should not be so hard what is this this is what you are trying  to predict what is this this is what you know is true because you have computed from  the corpus now can you come say the loss function the difference between these two  right so you could have this as the loss function this is the predicted value using  models parameters this is the actual value computed from the corpus  so think of this that you are trying to learn the parameters in such a way that you end up  predicting this and if you predicted this you know you have done the right thing ok and  this you know already because you have computed it from the corpus so this is the true  value and this is the predicted value so as in any loss function predicted minus true the  whole square does that make sense how many if you are fine with this     so now how will you train in this network gradient descent so i will use gradient  descent and you will get these parameters  so there is a bit more on this which i will not cover actually so i will just skip this  slide you can go back and take a look at it it is a some slight modifications to this yes  so again the same idea that cat will go close to all the so here again you will have the v  i and the uc s right so you will have cat will come close to all the words that it co  occurs with feline will also come close to the same words so maybe i have not used to  right notation here if you need to change it again so we should have v i‘s and u j’s  right so again you have one word matrix word representations and the other is the  context representation then it is fine right that is the problem here how many if you get  that right again we have to have these two things let us change that everywhere  
task2/super_cleaned_audios/lesson97.wav,249.9512,so we will see what guided backpropagation is so idea here is a bit hacky a bist  heuristically but it still works very well so let us see what it is right  so suppose you feed an input image to a convolutional neural network that image will  go through all the convolution layers and say it one convolution layer this is what your  feature map looks like i am operating at a very small scale i am just considering a two  two  feature map ok   now we consider one neuron in some feature map at some layer ok so we will consider  this particular neuron and we are finding interested in finding the influence of the input  on this neutron so this is what i will do is i will set all the other neurons in this layer to  zero because i do not care about them i only care about this particular neurons i just focus  on that     and we now back propagate all the way back to the image right that means i will  compute if i call this as htwo then i will compute htwoioneitwoi3 and so on ok   now recall that during forward pass what happens is because you have relu neurons any  output which was negative that was clamped to zero in the forward pass any output which  was negative was clamped to zero so what would happen to the gradients when they flow  back through those neurons you already did this if an relu neuron is dead the gradients  do not flow back right so the gradients will not flow back through these neurons that  means that only the so only these gradients will actually flow back which correspond to  non negative entries in the image before it or in the matrix above it right is that fine   so now these guys use this interesting idea that in the forward pass you dont allow  negative things to go forward so the backward pass also do something similar dont  allow the negative influences to go back that means any gradient which is negative just  clamp it to zero ok so what i am going to do is all these negative elements in the gradient  i am going to set them to zero you see that so this is just taking the same idea which you  apply that forward propagation that relu clamps the output to zero if the influence was  negative and the backward pass also do the same any gradients which are negative just  clammed them to zero   so the intuition here was that maybe there was a pixel which is really influencing the  particular neuron and it stands out but because there are some positive and negative  gradients flowing back they seem to cancel each other and all these influences tend to  be zero because thats what we observe that image was largely gray with very few non gray  pixels   so this is very heuristically because the reason i call it a heuristic is because you are  messing with the math right the math tells you that the correct gradient has to go back  irrespective of whether its positive or negative but they give this justification that on  based on two things and the forward pass you are not passing the negative gradients a  negative outputs so in the backward pass also kill them and this should avoid this  canceling of positive and negative output  so this is known as guided back propagation because you are meddling with the actual  back propagation you are doing something different     and so the idea was to neglect all the negative influences and when they apply this  guided back propagation this is what the influence looks like so you see that it is much  sharper now it is actually very nice its focusing completely on the eyes and you can see  the layout of the cat much more clearly as in the earlier picture earlier image right   so this is a popular technique to use to for various things it is also among other things  for in for understanding what your convolutional neural network is doing right so this  lecture is entirely about understanding what are the neurons learning what are the weight  matrices learning what are the kernels learning and so on so these are all again tricks  that you need to have in your repository to be able to do something more than just  zero4one5 right so this guided back propagation is one algorithm that you will implement  as a part of the assignment so   
task2/super_cleaned_audios/lesson81.wav,382.7798,so we will move on to the next way of dealing with the expensive softmax so  remember that so this is known as contrastive estimation   so remember that this is where we are in the story that we saw the bag of words model  we saw the skip gram model and we saw that both of them have this expensive softmax  computation at the end and that is the problem we are trying to deal with so we saw  one way of dealing with which was negative sampling so you i hope you saw that there  was no expensive computation there  the only computation there was the dot product between the two words which appear  together or which do not appear together now let us see what happens in contrastive  estimation    so here again you use a same idea so you have a positive sentence or a positive  example he sat on a chair you create a negative sentence which you replace the word by  some random word now you construct a feed forward network like this which takes  these two one hot representations basically uses your word context matrix to give you the  summation of these two representations right that is exactly what we have done in the  skip gram model now you have this hidden representation which is the sum of the two  word representations  now from here on instead of doing this softmax computation which we had earlier we  just predict a single score ok we just predict the score for this word pair being of correct  word pair we do the same thing with the random pair so we take sat we take  abracadabra and the add up there word representations you get this hidden  representations and you get a score sr fine so what is the output computation right  now what is the is it a matrix operation is it a scalar operation is it a vector  operation what is this h is equal to we need to change this to k on the slide please  note so what is this product w into h just a dot product between two vector  w is just k cross one that means it is a vector so as compared to k cross v earlier we  just have k cross one you get that how many of you get this we have a very simple  computation at the end ok but now how we set up by loss function earlier i could set  up the loss function as maximizing the log like it of the correct word but now i just  predicting two scores so what is the loss function what should i try to intuitively do  and today we are going to see a new loss function which we have not seen earlier so  try to think about this what would you do forget about the math forget about the  machine learning all that what would you actually want what is your wish list that  should be easy to characterize  score s score sr do you want this or this first one right you want s to be greater than sr  can you think of making an objective function out of this you want to maximize  student refer time zero3one8    s minus sr  fine that is a good starting point so would you be happy with this what  would you want this or this both cases s is greater than sr right what would you  want  student a big margin  a big margin fine    so we would like sr to be greater than s and not just so we could try to maximize s  minus srok but we would also like this difference to be a certain margin that means i  would want s to be greater than sr by at least a margin of m and that m is something i  will decide so i could say that it should be at least onezero points greater than sr or one point  greater than sr depending on the scores that i have so all my scores are between zero to one  then probably a margin of zero3 or zero4 is ok sounds reasonable right that means s could  be zero6 and sr could be zerotwo does that make sense  so what i am saying is what i am trying to say is that this is my sr i want s to be  greater than sr i am not just happy with that i am saying that even if i add a margin to sr  even then this condition should hold right  and that is the same as saying that s sr and there should be at least a margin of m  between that that is the difference that i accept i am not if you tell me that s is zero99  and sr is zero98 where then you are not really distinguishing much i want at least s to be  zero9 and sr to be at least less than zero5 or somewhere    so there should at least some gap between that and that gap is m so instead of  maximizing s minus sr i am going to maximize s minus sr plus m is that fine ok  now suppose you are at some point of training i will have some need some parameter  configuration that means you have learned some values for vc and vw and you do  this forward propagation compute s and sr and we actually find that this condition  holds right so right now my loss function is this at some point you are doing this and  you observe that this condition holds that means s is actually greater than sr plus m in  that case what do you want a loss to be how many of you get the question  i want that s and sr should be separated from a margin of m in the favor of s i am  doing my training i am at certain configuration for uc s and vw s and so on i pass it  through the feed forward network and i get s and sr and i observe that this condition  already holds  is my network doing anything wrong at this point it is doing it is job properly what  should be the loss that i back propagate zero again gets that there is nothing to correct  here i do not need to back propagate any loss    so then can you give me the full objective function maximize this but at this condition  already holds then do not do anything is that fine so that is about this so and again  observe that we have gotten rid of the expensive softmax computation  
task2/super_cleaned_audios/lesson42.wav,1027.1829,analysis singular value decomposition  so this lecture actually is a bit of a digression and it is supposed to cover some of the  basics that we need for various sections of the course so it is very important that you  understand some concepts for linear algebra specifically eigenvalues eigenvectors and in  particular today we will do principal component analysis and the reason that i do it is  there is an very neat relation of pca and to autoencoders an autoencoder is something  that well cover in the course it is a part of any deep neural network course  and singular value decomposition is something that we using when we learn word  vectors the word vector is again something very important i can just i can do the non  svd version of it where i just talk about what word to wick is but that will not give you  the same probably not the same interpretation as if you start from svd and then reach  word vectors right so that is why i am covering these basics  so how many of you know eigenvalues and eigenvectors very embarrassing question  how many of you absolutely hate eigenvalues and eigenvectors so let us see if we can  change that today i mean on the positive side     so what happens when a matrix hits a vector so most of you a lot of people that i talk  to right actually think that eigenvectors are the villains of linear algebra it is very hard to  understand them and so on but today i am going to make a case for they are not the  villains they are actually the superheroes of linear algebra so that is what the lecture is  about so what happens when a matrix hits a vector  student transforms it  transforms it right so actually what happens is that it strays from it is path so this is  the original refer time one hundred and fifty-eight this is the original vector x ok and now once i multiply  it by a that means if i do the transformation a x then i get a new vector and two  things happen right one is the direction changes which is obvious and in many cases the  scale also changes that means the vector might get elongated it is magnitude would  increase or it would decrease   so if you really think about it actually right so matrices are the real villains of linear  algebra right and we just look at this vector was minding it is own business going along  it is own direction a metric comes and hits it and completely changes it is world right i  mean it just throws it off path increases a dimension or slows it down or whatever it so  that is they are the bad guys now for every villain what do you have a super hero right  so what is a super hero corresponding to orbit what does a super hero do know that  is a very linear algebra i am talking about comic books that this is very linear algebraic  answer he stands up to the villain right    and that is exactly what eigen vectors do it right they refused to change their part they  tell the matrix you can hit me as many times as you want probably you can increase my  you could probably slow me down a bit or push me ahead or something but i am not  going to stray off from your path right so that is what eigenvalue eigenvectors do  so here is a matrix which is a villain and here is an eigenvector which is our hero and  now when this matrix hits this eigenvector it refuses to stray from it is part right it says  i will move forward i will move back whatever but i will not change my direction ok i  will just stay honest to what i am and these vectors are called the eigenvectors i am  more formally you can write it as ax is equal to lambda x right so that means the  direction remains the same only the scale changes it will either get slowed down or it  will get boosted up right so the magnitude would change but the direction remains the  same    now what is so special about eigenvectors like why are why is it that they are always  in the lime light i know the any course that you do invariably touch eigenvectors or  eigenvalues at some point in that course right where be it machine learning image  processing whatever you do you always speech everything that you do you will always  have eigenvectors and eigenvalues why is it so well it is turns out that several properties  of matrices can actually be explained away by looking at their eigenvalues so if i look  at a matrix i would probably not be able to comment much on it but if you tell me  something about the eigenvalues  i can see a lot of things about of it and there is an entire field on this way this entire  spectral graph theory which looks at properties of laplacian matrices and come in  something on the properties of the graph and so on right and that is just an example  which we do not care about but what we care about in this course there are a few things  that we care about with respect to eigenvalues and eigenvector and that is what i am  going to focus on right so that is what this lecture is going to be out and i will take two  specific cases which are very important for us to understand certain concepts later on  so i will start with the first one    and i will start with a very simple example to motivate this problem and eventually  will lead to a result which will help us understand a very important concept in deep  neural network training which is exploding and vanishing vanishing gradient we will  not touch that concept today but we will use these ideas when we are looking at that  later on  so let us take this example of two restaurants so there is a chinese restaurant and a  mexican restaurant and on day one k one students eat in the chinese restaurant and k two  students eat in the mexican restaurant so this is what my situation is on day zero k one for  chinese and k two for mexican now what happens as is obvious people get bored or they  have different want to try out different things so on day two or other each subsequent  day what happens is that a fraction p of the students who ate chinese today will opt for  max mexican on day on the next day and a fraction q of the students who ate ma  mexican today are going to opt for chinese  so you get this situation right so i started with k one k two so what i am saying is on day  one that is the next day only a fraction p of the k one students will remain for chinese and a  fraction one minus q would be transferred from mexican to chinese ok and similarly only  a fraction q of the students would again stick to the mexican food and a fraction one minus  p into k one would shift from chinese to mexican is this setup clear ok can you write this  as a matrix operation it would be a matrix multiplied by a vector right can you tell me the  vector  student refer time zerosixtwo9  k one k two k one k two and the matrix is in all this ok this is what it is and i am saying that this  happens on each subsequent day it is every day now this keeps happening so on day one  i started with say one8zero and now day two it change to something again day three it will change  something by the same fraction  now let me call this as matrix m and this is of course v zero right by definition as we  decided now what would happen on day two what would v two be m applied to v one right and  which would be m square applied to v zero i am just substituting the value of v one which is  m into v zero in general on the nth day what would happen m raised to n into v zero ok so  you see that the number of customers in the two restaurants is given by this series you had  v zero then m into v zero then m square v zero and so on up to m raised to n vn ok you see how  the number of customer is changing  now and this is how i represent it as a state transition diagram right so i had certain  numbers on day one and it changed with the trans with the probability p they will stay back  with a probability one minus p they will move to the next or the different restaurant and so  on right    and now this though a very toyish example can you relate it to many things in real life or  many things that you will take in decision making right that you are so even if you are  playing a game for example and even if you are playing atari games or something you  are in a certain state based on some action that will take will move to a different state and  so on right so these things happen in various real world applications right there is a  certain state for example even in stock market prediction you are at a certain value of  fish stock it might change to a different value right and these values you could just say  them as high low or neutral that i am not going into the actual numbers  today the stock value is high it does it possibility that it will transition to something low  and so on right so these kind of state transition diagrams occur in various real world  examples now this is a problem for the two restaurant owners right why is this a  problem for the two restaurant owners they do not know how much food to make but  every day the number of customers is changing right but is the number of customers  actually changing will the system eventually reach a steady state will it is it obvious  that it will reach a steady state or maybe it will not even reaches steady but the way i  describe it i do not see why it should reach a steady state right you have some people  here they go there come back go there and so on  the only thing which i have assumed is that the transition matrix which was the matrix  m is constant across all the time steps right so every day it is at the same priorities by  which things are changed right so what is your guess if i were to ask you to take a  guess ok let us see how many of you think and it is there is no correct answer here at  this point so just tell me how many of you think it will reach a steady state how many  of you think it will keep changing and why is the sum never equal to one ok so fine so it  turns out that they will right and let us see how     so we will define some things and some of these are just definitions some of them have  accompanying proofs which i am not going to do here you can the proofs have been  linked from the slides so you can take a look at them if you are interested  so suppose there is a matrix a n cross n matrix which has eigenvalues are lambda one  lambda two up to lambda n now what this definition is saying is that assume that there is  one eigenvalue which is greater there is no assumption actually the eigenvalue which is  greater than all the other eigenvalues is called the dominant eigenvalue and when i am  looking at a dominant eigenvalue i am only concerned with the magnitude not the sign  so it could be that an eigenvalue is minus onezero and all the other eigenvalues are one two three four five  so the dominant eigenvalue would be minus onezero right and i will just take it as step is that  clear the definition of a dominant eigenvalue  now how many of you know what is the stochastic matrix so matrix m is called a  stochastic matrix if all the entries are positive and the sum of the elements in each  column is equal to one so now this definition is again slightly misstated so there is a row  stochastic matrix the column stochastic matrix and also doubly stochastic matrix right  so what i am talking about here is a column stochastic matrix like our matrix have you  seen such a stochastic matrix any time in your life in the last five minutes the m matrix  right so the m matrix is a stochastic matrix because the sum of the columns was one  right you had p one minus p q one minus q ok or was it some of the rows was one rows was one is  it the columns  so this is a stochastic matrix just a definition now i combine these two definitions  which is dominant eigenvalue and stochastic matrix and give you a theorem right so  the largest dominant or the dominant eigenvalue of a stochastic matrix is equal to one ok  so to prove this what do i have to prove so i need to prove two things one that one is an  eigenvalue of this matrix of any stochastic matrix and second all the other eigenvalues  are less than one so that is exactly what this proof does here you can take a look at it and  just to give you a heads up so last year i use to do this that please see the proof go back  and look at the proof people never look at the proofs  so i used to ask them in the quiz where i should be sure that people not going to answer  right so please when i say go back and look at the proof do that ok so and lastly if a  is an n cross n square matrix and you have this series a v zero a square v zero up to an vn  then this series will converge to the dominant eigenvector of a what does a statement  mean let us not get into the proof right what does it actually mean ok so let us start  with very basic stuff right what is the series actually what is each element in this series  it is a vector it is a vector everyone gets that every element in the series is a vector  now what do i mean that a series of vectors converges to the dominant eigen vector  what is convergence mean if i keep finding the next element next element next element  of this series and i keep doing this as long as i can i will reach a value n right where n is  the nth element in the series which will just be a multiple of the dominant eigen vector is  that clear you not seem to be clear everyone gets that  so what do you mean by if you take a series of numbers and if i say that the series  converges to zero what does that mean if you keep finding the next element in the series  you will hit a point n where you find the nth element of the series and it will be zero refer  time onethreetwozero that ok so we will just i will leave it at that for now now so stochastic  matrix dominant eigenvalues the connection between two and the convergence theorem for  a series of vectors which is a v zero a square v zero and so on    now let ed be the dominant eigen vector of m where m is a dash matrix in our case it is  a stochastic matrix so what with the corresponding dominant eigenvalue be  student one  one ok so given the previous definitions and theorems what can you say about the  sequence it converges to a dash of ed  student refer time onethreefive9  a multiple of ed right so there exists an n such that the a length nth element of the  series which is given by this is going to be equal to some multiple of the dominant  eigenvector no no k is some multiple no this is not related to eigenvalues yet just wait  for the next statement then you will see the difference that this is not the do eigenvalue  yet  now my question is what happens from here onwards what would be the next element  in the series how many of you say some k dash into ed what is the other pause i do not  have the other option what is the other option  student k into ed  k into ed how many of you say k into ed a large number of ok so you see that now just  notice the eigenvalue will come up right so at step n plus one you would have m into vn  which is m into k into ed and this quantity is actually one so the theorem says it will  converge to some multiple of k and now if it is a stochastic matrix what will happen  after that time step it will just remain the same vector  so what would happen to the number of customers in the two restaurants it will remain  the same right you get that ok fine now this was all for what kind of matrices  stochastic matrices square stochastic matrices    but we generally care about any square matrix in fact we should care about any matrix  not discriminate but any square matrix will do for now so for a square matrix let p be  the time step at which this series approaches a multiple of the dominant eigenvector  the theorem was for any square matrix remember it was not for stochastic square  matrices we just use this value that for a stochastic square matrix the dominant  eigenvalue is one which it need which leads to that neat result that the num then the  number of customers just becomes constant but for any square matrix i could write it as  this that there exist some step p at which the element of the p’th element of the series  would just be a multiple of the dominant eigenvector  now what would happen at step p plus one is this fine what about step p plus two and in  general at p plus k or p plus n everyone gets this so now can you tell me what does this  knowing this dominant eigen value tell us about this series when will it stabilize  actually  student refer time onesixtwofive  when lambda is equal to one that is the case we already saw if the dominant eigen value is  greater than one what would happen  student refer time onesixthreethree  series will explode the series will explode and if it is less than one what would happen  the series will vanish ok so this is an important result that we will use when we are  discussing exploding and vanishing gradients  so we will see that in the case of something one as a recurrent neural networks you end  up with something of this sort and then i will make some comments on that right so  that is why we will be using this will come probably six seven or maybe more lectures down  the line ok but we will be using it at this point so the main result from here is that if the  dominant eigenvalue this should be lambda d is greater than one then it will explode less  than one it will vanish and equal to one it will stabilize so that is one result one important  property of eigenvalues and eigenvectors that well be needing at a later point in the  course   
task2/super_cleaned_audios/lesson56.wav,477.0217,so with that we will move on to something known as contractive autoencoder so this  is yet another type of auto encoders again with the same aim that you want to do some  kind of a regularization  so it again tries to prevent and over complete auto encoder or even an under complete  auto encoder for that point from learning the identity function  so it does not allow you to simply copy the inputs to the outputs that is what it is trying  to learn and it does so by adding the following the regularization term to last function  and the way it does this is by defining the following regularization term ok what is this  term ok let us see some things which we already know what is this frobenius norm  of some matrix what is this matrix  student jacobean  jacobean what is the jacobean  student refer time one hundred  what are the two variables here that you see  student h  h and  student x  h is a scalar matrix vector  student vector  vector x  student vector  vector right so it is some function between two vectors ok and it is a matrix so take a  guess how many entries would not you have if x is r n and h is r k   student n cross k  n cross k even if you do not know what the entries are you are able to guess that it is  going to be a n cross k matrix right     now let us see what this n cross k matrix looks like ok     so it has the input has n dimensions and the hidden layer has k dimensions so this is  what the jacobean looks like  what is the first column if the partial derivative of every neuron in the first hidden layer  with respect to the first input right and now you can see what the other columns would  be this is what the jacobean is this basically the derivative of h with respect to the  vector x answer is just you are taking a derivative of a vector with respect to another  vector you will get a matrix as the output ok now what does the jl’th entry here capture  actually   student refer time two hundred and twelve  what does a derivative capture  student refer time two hundred and fourteen  how much does h l change with a small change in  student x k  x k right that is what a derivative captures is that fine and then what does the frobenius  norm capture it is just the square of sum of the square of all the elements of the matrix  right so it is basically how much each of these elements vary with respect to the input  and we are just taking the square of that so you see what is the term that we have  added    now tell me what is intuition behind this ok so when would this term so remember  this term is added to the loss function and you are trying to minimize the loss function  so that means you want this term to go to  student refer time three hundred and one  you want the frobenius norm to be   student zero refer time zero3zero3  zero right ideally of course that will not happen because there is always a tradeoff between  l theta and omega theta if you make it zero then l theta would be very high right     so now what would happen if one of these guys say dou h one by dou x one actually goes to  zero what does that mean h one is not sensitive to variations in x one right fine but was our  original mandate what did we want these neurons to capture we wanted the neurons to  capture these important characteristics right   so if x one changes we want h one to change do you get that how many of you get that  we wanted the neurons to capture the important characteristics of the data right but  now we have added a contradictory condition which says that we do not want the neuron  to capture a variations in the data do you see this so what is happening here l theta  says that i should be able to capture these variations right otherwise i will not be able to  reconstruct   if all my h i’s are not sensitive to variances x one that means i give it any x one it will  produces the same h i is that clear is that with ok everyone right that means so see this  is this so i have these training examples occurs all these training examples my bold x  which is vector x is going to change that means xi’s which are the elements of this  vectors are going to change   now what this condition is saying is that if i change xi i do not want the h l’s to change  i do not want the values of the hidden representations to change so that means it is  changing the respective of what is the input fed to it try to produce the same output do  you get this argument ok that means it is not capturing any important characteristics  of the data is that fine is that valid argument but that is not what we wanted we wanted  it to capture the important characteristic of the data so what are we trying to do now     so just i it is hard for me to do evaluate what you have said but just pay attention and  see if that is correct you can judge it on your own right so that is the actually the idea  right we have put these two contradictory conditions with each other right l theta says  capture the important variances of the data omega theta says do not capture variations in  the data watch the tradeoff capture only very important variations in the data do not  capture the variations which are not important can you relate this to something that you  have seen before  student bias variance  no the other answer there are only two answers bias variance and pca when i say the  other answer  student pca  what am i trying to force it to do capture only the important variation it is if it is not  clear right now we will come back to this ok     so let us try to understand with this with the help of an illustration right how many of  you get the argument which i made on this slide ok most of all     now this is the situation i have u one and u two as my dimensions fine which of this is  important u one the variations in the data across u one is something that i should care about  because i can see that brings in some difference what about the variations in u two  student not important  not important they seem like noises because these variations are there they are not all  lying on the central line they are slightly away from the line here are some variations  but should i go out of my way to capture these variations does it make sense to do that  no right so it makes sense to maximize a neuron to be sensitive to variations along u one   but it does not make sense to make the neuron sensitive to variations along this other  dimension which is u two ok by doing so we can balance the two conditions so one  condition was trying to capture all the important variations ok do this but do it only for  the dimensions which really matter the other conditions says that do not capture  important variations ok do this but do it only for those dimensions which do not matter  what is this remind you of at least the diagram should have it away right  student refer time zerosevenoneseven   it is same as principle component analysis right so that is exactly what you try to do in  pca you try to capture the variations across the important dimensions but not across  the non important dimensions how many of you get the concept of contractive auto  encoders ok good so i think that is a where we will end lecture seven     and just a quick summary so we showed that under certain conditions auto encoders  are equivalent to pca  and we use this result very crucially there that svd theorem i will not state it     and then we looked at different types of regularizations for auto encoders where we  looked at weight decaying that means the standard ltwo norm we looked at the sparse  auto encoder the contractive auto encoder and we also looked at these denoising auto  encoders right so that is the summary of this lecture   
task2/super_cleaned_audios/lesson57.wav,610.9444,augmentation parameter sharing and tying injecting noise at input  ensemble methods dropout  so in this lecture we are going to talk about a bunch of regularization techniques for  deep neural networks you might find some very familiar terms here for example ltwo  regularization perhaps something else also but i promise you that we will see a very  different interpretation of this from what you have done in your earlier courses right  so again as is the trend in this course i will start with some basic concepts i will take  today’s lecture to finish off the basic part which is the bias variance tradeoff and i will  try to make it more informative then what you have done in your earlier courses and in  the rest of the lecture which will happen on friday we will build upon these basics and  then try to look at these as the regularization forms  so let us start so these are the sources which i have looked at so one of them is the  chapter seven from deep learning book other is this very good lecture by ali ghodsis on  regularization and of course this paper on dropout so let us start with bias and  variance again some five hundred and ten minutes would be similar to what you have seen in the  middle class but then i will go on to something different    so we will begin with a quick overview of bias variance and the tradeoff between them    so let us consider the problem of fitting a curve through a given set of points ok now  remember i have always been telling you that there is always this true relation between x  and y which is f of x right and which we never know  so we do not know what this is in the movie example we do not know what this is in  the credit card fraud detection or in the oil mining example in this particular example i  know it right so what i have done is i know that the true relation between x and y is  the sinusoidal curve i know this but instead of giving you every point on this sinusoidal  curve what i have done is i have such sampled some points from it i have taken some  points and given to you    so from now on we will behave as if we do not know that this is how it came it is a big  secret and we now want to fit a curve to this that means i want to learn the function f  hat of x ok which of course will have some parameters and what will be my goal is that  now let us look at this again my goal would be if i feed at this point after the model is  train the output should be as close to this point as possible that is our training criteria  everyone gets this    so we consider two models the first model is a simple model how many parameters  does it have  student two  two parameters right the other model and this is what happens when i train the simple  model of course i will get a line but do you see something special about this line why  did i get this as a line or this as a line so on average it is trying to minimize the  distance from all the points if i have this as the line then i will have a very high error  for these points right so just something which goes along the average and hence the  sum of the squared errors would be minimized right  so it is important that when you see these figures you should make these connections to  the math behind it so this is the geometry you have to make connections to the math  behind it right and i hope all of you make that connection now i take a complex model  which is a degree two5 polynomial ok so this is w one x w two x square w three x cube and so on  it is a degree two5 polynomial that i have used and i again learn the parameters of this  using how will you learn the parameters you have a quiz two days from now on gradient  descent  what else do you know if you know any other algorithm of course you know but  getting this end right what else will you use you can use gradient descent for learning  these parameters the same idea right you will define a loss you will compute the  gradients with respect to all these parameters how many of them are there here two6 and  just update those parameters till a fixed number of iterations or any convergence criteria  ok and this is the curve which i get for the complex model note this in both these cases  we are making an assumption about how y is related to x right in this case i made a  simple assumption in this case i made a slightly complex assumption but in both the  cases we do not know what is true relation is  the true relation is actually the sine curve but we do not know that we are just making  an assumption so you remember the five things in machine learning you have a data  you make an assumption about how the input is related to the output so these are my  two assumptions then i have some parameters you know the number of parameters in  these cases i use a learning algorithm which happens to be gradient descent and then i  minimize an objective function which would be squared error loss in this case fine  now the training data actually consists of one00 points ok but you do not see one00 points  here    so what i have done is i have sampled some two5 points from here and use that as the  training data so i have learned my parameters w one and w naught or w two5 up to w  naught using these two5 points now i will repeat this experiment k times what i do is  every time i will get a different sample of two5 points and i will try to learn the parameters  of the model will i get the same curve every time will i get the same function every  time no my parameters would change slightly right because my training data is  different  so i am trying to learn it differently to adjust to that training data so my function is  going to be different it is the same form it is either the linear function or the polynomial  function but the parameters the coefficients are going to be different    i will actually draw these different functions and we will make some observations from  that so this is the black curve that you see is the true sinusoidal curve from which the  data has come the blue line is one of these functions which i have trained from one  random sample of the data right  now i train different functions from different random samples of the data and see what  happens i get different lines this obvious can you relate to this every time i am  basically learning a different value of w one and w naught is that ok and i have done this  two5 times and plotted these lines what do you observe with respect to each of these if  you compare any line to any other line  so if you compare one of these lines to the remaining two4 lines what do you observe  they are very close to each other they are not very different from each other however  there is a problem they are very far from dash the actual function that means we are  under fitting we have very few parameters in fact only two that is why we are under  fitting let us look at the other case fine this is the function the polynomial the blue  curve that you see is the polynomial that i learned from one random sample of the data  now i am going to learn this from a different sample of the data you see what happens  you see that the green curve is actually very different from the blue curve you see that  here actually this was peaking whereas this is going down similarly this was peaking  but this is going down and so on so you see that there are clear differences between the  two curves and if i draw the next curve you see it is even more different the same  function learnt from different data point is turning out to be very different why because  it is over fitting on those two5 points that i have given the simple model did not even have  the capacity to do or fit because it is just two parameters  how much can i over fit i will just end up drawing the average line right but here it is  really able to overt fit and you see that these two5 curves or i do not know how many  curves that i will draw all of these are going to be very different from each other you  see that and everyone agrees that this would happen if you actually try to do this ok so  complex models train on different samples of the data are very different from each other  what is happening there is over fitting ok    now let me define two concepts from statistics one is bias bias is very simple it tells us  that this is the true function if you are trying to learn the approximate function and you  do it many times then you will get an expected value of the function so it tells you how  much does this expected value differ from the true function ok you get the definition  the definition is straight forward ok now for the simple line or the simple model the  green line that you see is actually the average of all those two5 lines that you had seen ok  what can you say about the bias very high right because this difference is very high  this green line is very different from the red curve which is my true function right  predicted and true function now what about complex model the blue curve that you  see is actually the average of all those two5 different curves that i had drawn so what is  the bias it is very low does that make sense this means that the simple model has a  high bias and the complex model has a low bias is it clear to everyone    now let us define another quantity which is variance everyone knows what variance is  so this is one of the functions that i have learned this is the average of that function  and the variance tells me the spread now based on the figures that you have seen can  you tell me what would happen for the simple model low variance or high variance  student low variance  low variance because all these models were very close to each other there was not  much spread in the models what about the complex model  high variance all these models were very far from each other the spread was very high  ok so roughly speaking it tells us how much the different f f act f x is that you are  learning how different are they from the average f of x    so informally i can say the following simple model has a high bias low variance  complex model has a low bias high variance and as always going to be a tradeoff  between the bias and variance  so why is there always a tradeoff between the bias and variance people have done ml  course why is there a tradeoff how many of you know the mathematical answer to  that you have not done this in the ml course no so it turns out that both bias and  variance contribute to the mean square error and let us see how  
task2/super_cleaned_audios/lesson43.wav,635.9409,now from here on we will go on to something even more basic we will start defining  some basic definitions from linear algebra and these are again important for something  that i need in the next lecture so let us start with this   i mean in the process we all just see why the eigenvectors are important for us in this  course     so how many of you know what a basis is so a set of vectors belonging to r n is called  a basis if they are linearly independent right and every vector in r n can be expressed  as a linear combination of these vectors so a set of n vectors vone to vn is linearly  independent if no vector in the set can be expressed as a linear combination of the  remaining n minus one vector so a more weird we are stating it that so that everyone get  confuse is that if you take this linear combination  the only solution to this is all the ci’s is equal to zero and that make perfect sense right that  is that same as that linear combination linear independence and all that thus is make  sense to everyone ok so what does linear independence mean that any vector from this  set cannot be expressed as the linear combination of the other set other vectors in the set  and a more formal way of saying that is this everyone gets this what is linear  independence     now let us consider some very stupid examples again the space r two and we consider  these two vectors one zero and zero one are they linearly independent yes ok they cannot be  expressed as a multiple of each other right now any vector ab belonging to r square  can be expressed as a linear combination of these two vectors ok and x and y are linearly  independent the only solution is c one x plus oh sorry c one x plus c two y is c one and y equal to  zero what about if i move to r three one zero zero zero one zero and zero zero one so x y and z axis right are the unit  vector     so in that x and y turns to be unit vectors in the direction of the coordinate axis and we  are used to representing every point in r two as a linear combination of these two vector is  that exactly what i what we do so when we say that i have a point two comma three i am  actually telling you that the point is two one zero plus three zero one right i am expressing at are the  linear combination of the coordinate axis     but now this nothing sacrosanct about x and y right i could have chosen just about any  other axis so in particular we could have chosen this as our basis are these two vectors  linearly independent can any vector and r two be expressed as a linear combination of  these two vectors sure so i give you a vector a b how do you going to express it as a  linear combination of these two vectors so you will do it this way right how will you  find that values are the x one and x two so other linear system of linear equations right    so this is what you will do i know all are good in doing this and what do we actually do  when we do this what is the algorithm that we use how do we solve this what is the  algorithm that you use solving this  student gaussian elimination  gaussian elimination right in two variables of course we do not call it an algorithm that is  what we did in eight standard or something but when we come to engineering we call it  gaussian elimination right so the same algorithm    so in general given a set of linearly independent vectors we can express any vector that  belonging to rn as a linear combination of these vectors right i can say z is equal to  alpha one u one plus alpha two u two and so on given alpha one to alpha n are linearly independent  ok so that means any vector in rn can be expressed using these vectors which form the  basis of rn does that make sense that is why call the basis vector because anything else  these are the fundamental vectors using these anything else can be expressed in that  space it is that clear  so this is how it will be how do i write this in matrix notation a there are lot of these  and these thing i do not really understand what you mean by that yeah good so this is  what you mean so that we writing same in matrix notation and now this is again a dash  a system of linear equation there was a lot of space to fill and one dash good so system  of linear equation and again you can solve them using  student gaussian elimination  gaussian elimination what is the complexity of gaussian elimination let us see options  right n n square n cube fl n cube right the gaussian elimination the complexity is o n  cube right and i am not doing all this just to the sake of time pass i have a point of  make which i will make on the next slide right so now this was for any basis that  means if you have any n linear independent vectors  now i will consider a special basis where instead of n linearly independent vectors in  addition these vectors are also orthogonal ok orthogonal vectors are linearly  independent ok so a set of orthogonal vectors are linearly independent but the converse  is not all this right    so now let us see what if we have an orthonormal basis that means a basis consisting  of orthonormal vectors so orthonormal is combination of two words ortho means the two  vectors are orthogonal and normal means all the vectors are unit vectors that means i  am normalized them by their magnitude  so what is the condition that holds ui transpose uj is equal to zero if i is not equal to j and  ui transpose ui is equal to one ok now what happens in this special case so we have this  again we can express any vectors z as a linear combination of these now let me try to  do this i am just pre multiplying this equation by u one transpose what happens on the  right hand side everything disappears all of the this terms will disappear because they  are of the form ui transpose uj where i’s not equal to j and the first term is  student one  one so what remains alpha one so you can directly find alpha one using a dot product of two  vectors what is the complexity of this operation n th is just n products ok now how  many such alphas do we need to find  student refer time 6zero9  n of those so what is the complexity n square so that is now you see why an  orthonormal basis is a very convenient basis you can get all these coefficients just by  doing a dot product between two vectors and later on i will show you that you might not  need all of these you might just need some subset k of these right so that means you  just do k of these dot products and get these values so do you now understand the  meaning of what is why why do i say it is orthonormal basis is the most convenient  basis that you can hope for right  so the another way of looking it right at it is again just to make few more comfortable  with vectors and projections and so on right so this was your original point z one z  which is a comma b right and how do you actually draw that vector that this is a and  this is b ok so how do you find the coordinates actually you projects on to your basis  vectors which were these x and y vectors that is how you found the components along  those the coefficient along those  now instead of this x and y i have any other set of vectors which is u one and u two and i  will do the same thing i will project this on to uone ok i will project this on to u two and that  projection will give me alpha one and alpha two right so now what is alpha one and that sense  this is z this is alpha one and this is theta right so alpha one is equal to z into cos theta ok  and what this cos theta so again you arrive at the same thing fine  so essentially taking a projection of a vector on to your basis is this fine to everyone  there is just to difference arriving at the same formula that alpha is are given by a dot  product between the basis vector and your original vector    so an orthogonal basis is the more convenient basis that you can hope for that is the  point which i wanted to have you are convinced about that    now but what does any of this have to do with eigenvectors i started off with  eigenvectors i proved one property there and then i came to this linear algebra basic  definitions and what a basis is set of linear independent vectors and i eventually showed  you that an orthonormal basis is the most convenient basis that you can hope so what  does any of this have to do with eigenvector   student refer time eight5zero  always for us square symmetric matrix right why do you care about square symmetric  matrix not sure yet so we get to that so first of all it is turns out the eigenvectors can  form a basis and this is for any matrix so the eigenvectors of a matrix having distinct  eigenvalues are linearly independent  so does every matrix if i have an n cross n matrix will it have n eigenvectors no it  can have less than or equal to eigenvector depending on the refer time 9one5 so what  is this saying is that if these eigenvectors are having distinct eigenvalues ok then these  eigenvectors would be linearly independent fine ok and turns out that for a square  symmetric matrix that is the even more special the eigenvector of a square symmetric  matrix are  student orthogonal  orthogonal right and we already know that orthogonal is good right so remember  when we have orthogonal we do not really care about orthonormal because that is it is a  simple operation if you have a set of vectors u one u two u three which are orthogonal you can  just divide them by the magnitudes and just get a set of orthonormal vectors right so  orthogonal and orthonormal i will use it interchangeably ok and whatever i done thus  they form a very convenient basis so the eigenvectors of a square symmetric matrix  form a very convenient basis    so that is how i connect the parts which was about the eigenvectors to the second part  which was about basis and why would we want to do this and we already we had a  coordinate axis that is the very good basis one zero zero zero zero one zero zero one and n dimension similarly  so why should i want to use the different basis i have said that eigenvectors is a very  convenient basis but why do i care about it i already have a very very convenient basis  which is just these one or two vectors are along these directions right so why do i care  about a different basis i understand that i that is there somewhere but something more  than that that is one advantage which i will talk about what else more interesting ok  in what sense i love the power which comes with my job right that you give a right  answer and still i can embarrass you know so that is correct actually   
task2/super_cleaned_audios/lesson94.wav,348.9006,so now this was visualizing the neurons inside the convolutional neural network so  neurons remember are the outputs right these are not these are the feature maps what  about the weights itself what are the weights in a convolutional neural network  student refer time twenty-five  the filters the filters themselves are weight have you ever tried to visualize weights  before when  student auto encoders  auto encoders and what was a trick there how did we  student refer time thirty-six  visualize what was the optimization problem that we solved  student refer time forty  how many of you went and looked at the prerequisites how many of you looked at the  prerequisites ok    so we had done something similar while discussing auto encoders so because that we  had done something similar while discussing auto encoders right so we were interested  in knowing that there is a particular hidden neuron inside the auto encoder and we  wanted to see that what does this neuron capture so if i give it emnist digits then what  kind of patterns does it fire for and if you remember we had solved this optimization  problem and realize that this neuron will fire for an input which looks like this where  wone or all the weights which are connecting to this neuron ok what was the dimension of  the input if you are dealing with emnist digits  student refer time one hundred and twenty-two  seven hundred and eighty-four what is the dimension of this a one thing which i have circled here   student refer time one hundred and twenty-seven  seven hundred and eighty-four right it is written x equal to so it has to be seven hundred and eighty-four why is it seven hundred and eighty-four because there are seven hundred and eighty-four  weights connecting each of the input pixels to that neuron right so that means this  weight matrix itself we can visualize it as an image and thats exactly what we had done if  you remember we had this grid of images that we were analyzing and in some images we  saw that some dark element fires here and each we were arguing that this is the curve  which exists in two or nine or eight and that is the one which is capturing   and in some cases there was a cusp here which was firing and we were arguing that this  could be for the three or for a nine or for a eight or something like that right so we were trying to  visualize these things and the way we had plotted it was just treating this weight matrix  or weight vector as an image and seeing what causes the neuron to fire right     so we can do something similar for convolutional neural networks i want you to think  how would you do that i will give you some hints  the answer is there on the next slide  but i just want you to think about it right so remember here you have dense connections  ok that means your weight vector was the same dimension as the input vector what  about filters in the case of cnn they are smaller they are three  three five  five or seven  seven much  smaller than your original image  so then what do these filters correspond to just think of the animation that we had seen  right we had this image and we were taking a filter and applying it at different places so  what does the filter correspond to what is the filter overlap  with patches in the image  right so now what kind of analysis can you do  student dense  what kind of patches does this filter fire for or what kind of patches does the neuron  connected to this filter fire for does that make sense everyone gets the intuition how  many if you get the intuition please raise your hands thank you     so now recall that we can think of a cnn as a feed forward neural network and in  particular when you have a filter it actually interacts only with few pixels right so  interacts with say pixel one two five and six so that is the patch that it interacts with  and now i want to see when does this neuron fire so that is the same as asking what do  i put in one two five six for this neuron to fire or similarly what do i put in three i do not know this  was one two five six i guess so three four seven eight for the same different neuron to fire right but all  these neurons fire because they are connected to the same filter   so that means i am interested in these patches which will cause the neuron to fire and  those patches can appear anywhere in their image is that fine that is the whole point of  convolution neural networks wherever there is a nose whether it is at the top corner of  the image or the center or the bottom it should be able to detect right that is the whole  point of weight sharing and sparse connectivity ok    so we are going to do exactly the same thing we will have a three  three filters or five  five  filters or seven  seven filters were just going to visualize as them as images but unlike the  earlier case where the image actually correspond to the full mnist image here these  images are just corresponding to those three cross three or five cross five patches and you want to see  what kind of patches causes the neurons to fire ok and the solution is still the same we  will have this w by w the normalized weight filter weight which is causing the input to  fire how many if you are fine with everything at this point please raise your hands high  up    so this is what we get right and we observe certain things which like we had earlier  made a case for that these filters try to detect certain types of patterns or textures or  edges so you can see that right this is capturing these slanting edges this is trying to  capture some horizontal sorry vertical edges then some edges oriented differently and  also some colored patterns some texture right so you see something like a checkbox  here or chess box here and so on  so these filters are actually firing for different kinds of patches so they are trying to  detect different things from the images so you could visualize this and unless you see a  lot of variety in this that means something is wrong right because your filters are not  being trained to be discriminative with terms of different patterns that they can detect  and so on right so you want these variety of patterns to occur ok and i am going to  make a claim that this is only interpretable for the first layers in the convolutional neural  network why is it so i am seeing some half complete answers so i will ask this as a  quiz question   
task2/super_cleaned_audios/lesson80.wav,472.8994,so what i will do is i will quickly go over what we were doing yesterday and then by the time people come in we can start with the new stuff right so we were looking at so that is so this needs to be corrected someone who pointed out yesterday same as bag of words it should be same problems as the bag of words model so we are trying to fix this problem where we have this large softmax computation which is very inefficient and you wanted ways of getting rid of that so the first thing that we were looking at is using negative sampling and here the key idea was to con construct this d and d prime where d prime was the random corpus and d was a true corpus and how do you create this random corpus is something that was left at the end and which i need to go over today so i will go over that and then we realize that this actually could be modeled using such a network where you take the dot product between the word representations and try to maximize this to dot product for all the correct pairs by setting up your loss function accordingly and try to maximize or rather minimize this dot product minimize this dot product for all the incorrect pairs by again setting the objective function appropriately so we had this objective function where we want to maximize the probability that the pair is correct for the correct pairs and maximize the probability that the pair is incorrect for the incorrect pairs and both these probabilities we had modeled using a sigmoid function and inside the sigmoid function we had the dot product between the corresponding representations so the net effect is you either maximize the dot product of the correct pairs or minimize the dot product of the or rather and in minimize the dot product of the incorrect pairs fine and then so now today the part which was remaining about the comparison between d and d prime so what i was saying last time is that d prime is actually k times d that means in sample more negative examples than positive examples so if you think about it actually the number of negative examples in the language is much much more than a number of positive examples let us say if you have fifty k words in your vocabulary most of them do not appear together right so that number is actually very very large as compared to the number of words which can occur together so how do you account for this natural imbalance so they said that if you keep it same then we are saying that the size of d prime and d is going to be same that means the words which appear together and not to appear together we are keeping those two corpora as the same so that does not sound reasonable so they decided that we will keep it k times ok now this k was a hyper parameter which was tuned based on the data that they had and can you guess how they would have tuned it no what do you tune your parameters on what did how did you tune your parameters for the back propagation of the word no using what student validation set a validation set is it too early in the morning it fine validation set so they might have had some validation set and if you look at the original word to word code which someone had posted yesterday which allows you to compute the distance matrix right so you could what you could do is you could learn these representations take a few pairs of words and take a few pairs of good words right say cat and dog or cat and feline and so on and also bad words like cat and truck bad combinations rather and see if the distance between cat and truck is much higher than the distance between cat and feline or cat and dog so you select that k which gives you the best performance on your validation set and the validation set here would essentially be to find if you get good representations for word pairs that you care about and for word pairs that you do not care about ok now the other thing was how do you create this r so you have v words in the vocabulary you are looking at one of those w you know that some of those have appeared with w in some context but there is this large set which has not appeared with w in any context right so you are going to draw r from this set and the simplest thing to do would be to just draw the uniform distribution that means all words and let us call this suppose there are capital r words here all of these words could be drawn from using the probability one by r where r is less than v is that fine that is one way of doing it just randomly pick any word from the remaining words and put it a pair it with w but you would also want to account for the individual frequencies of those words right if the word is actually very frequent pair it up more with w if it is not frequent do not pair it up enough does that make sense so i could actually use the frequencies of each of these words and sample according to that frequency right instead of using a unigram distribution so they did something similar but they had this hyper parameter again so basically i was sampling using the probability of r which is equal to count of r divided by the number of times number of all the words in the corpus that is actually the frequency of r divided by the total number of words in the corpus so instead of just taking that they had this wearied factor of three by four do you we realize that if you take this three by four you get the best performance so let me just make a few comments on that so the original code of or rather the original skip gram or the bag of words model actually worked very well and it kind of hard a lot of seminal effect or a lot of revolutionary effect on the field of nlp right so now everyone started talking about word vectors and how you can use this meaningful representations of words as features for various down steep nlp thus right so at the end in nlp what you are doing is you are collecting of a bunch of words a document or a sentence or something and trying to do some processing on that now earlier used to construct features out of these sentences using some handcrafted features but now someone said that there is this automatic way of constructing word features right which is using this method so people really bought onto that idea and a lot of work started happening and then later on at the end of the course we will see something that what it eventually led to but later on when people started analyzing this more carefully right they realized that the original wordtwovec implementation had a lot of these heuristics or lot of these parameters which need to be really tuned to the core for it to be able to compete with svd right so that is what we look at the end so svd was already one way of computing word representations ah which while popular was not so popular it was used for various reasons but it was not like every npl application is using svd representations right but now it is almost like every npl application is using word representations so later on we will see that some of these things like three by four or k the value of k the value of learning rate and some other hyper parameters if you really tuned them very very well it is only then that as this wordtwovec algorithm can beat the world representations learned by svd or rather the other thing that if you introduce some parameters in svd and tune them because remember for svd there was no tuning right we just got a solution we just had the closed form solution which is the eigen vectors but you could do some things for creating the cooccurrence matrix if you introduce some factor there which is also looks like this three by four or something like that or if you also introduced something which looks like a k then you will be able to get the same kind of representations or equally powerful representations from svd as what you get from wordtwovec so that is why i am stressing on these hyper parameters there is some significance of those 
